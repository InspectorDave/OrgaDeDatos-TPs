{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://i.ytimg.com/vi/Wm8ftqDZUVk/maxresdefault.jpg\" alt=\"FIUBA\" width=\"33%\"/>\n",
    "  </p>\n",
    "  \n",
    "# **Trabajo Práctico 1: Reservas de Hotel**\n",
    "### **Checkpoint**: 3\n",
    "### **Grupo**: 11 - Los Pandas\n",
    "### **Cuatrimestre**: 2ºC 2023\n",
    "### **Corrector**: Mateo Suster\n",
    "### **Integrantes**:\n",
    "### 103456 - Labollita, Francisco\n",
    "### 102312 - Mundani Vegega, Ezequiel\n",
    "###  97263 - Otegui, Matías Iñaki"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensambles: Stacking\n",
    "\n",
    "Se busca entrenar diferentes modelos (modelos base) y un modelo más, que decide, dada una instancia nueva, qué modelo usar. Se utiliza el concepto de meta-aprendizaje para reemplazar el mecanismo de voto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score,f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', 'is_categorical_dtype is deprecated')\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza del dataset\n",
    "\n",
    "Al momento de entrenar los modelos previos, no siempre se realizaron las mismas modificaciones sobre el dataset original. Por una cuestión de comodidad, se va a hacer que todos los modelos tengan el mismo dataset, por lo que se utilizará aquel en el que se transformaron algunos atributos en booleanos y se normalizaron las columnas numéricas cuantitativas. Esto es porque únicamente KNN no tuvo esta modificación y la normalización no afecta a los árboles pero sí a KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels_df = pd.read_csv('hotels_train.csv')\n",
    "hotels_df_backup = hotels_df.copy()\n",
    "\n",
    "#Eliminación de columnas irrelevantes\n",
    "hotels_df_mod = hotels_df.drop(['arrival_date_year', 'arrival_date_day_of_month', 'stays_in_weekend_nights', 'stays_in_week_nights', 'children', 'company', 'adr', 'id'], axis=1)\n",
    "\n",
    "#Eliminación de filas con valores nulos\n",
    "hotels_df_mod = hotels_df_mod.dropna(subset=['country', 'distribution_channel', 'market_segment'])\n",
    "\n",
    "#Transformación de variables numéricas a booleanas\n",
    "hotels_df_mod['required_car_parking_spaces'] = hotels_df_mod['required_car_parking_spaces'].apply(lambda x: True if x >= 1 else False)\n",
    "hotels_df_mod['required_car_parking_spaces'] = hotels_df_mod['required_car_parking_spaces'].astype(bool)\n",
    "\n",
    "hotels_df_mod['days_in_waiting_list'] = hotels_df_mod['days_in_waiting_list'].apply(lambda x: True if x >= 1 else False)\n",
    "hotels_df_mod['days_in_waiting_list'] = hotels_df_mod['days_in_waiting_list'].astype(bool)\n",
    "\n",
    "hotels_df_mod['babies'] = hotels_df_mod['babies'].apply(lambda x: True if x >= 1 else False)\n",
    "hotels_df_mod['babies'] = hotels_df_mod['babies'].astype(bool)\n",
    "\n",
    "hotels_df_mod['previous_cancellations'] = hotels_df_mod['previous_cancellations'].apply(lambda x: True if x >= 1 else False)\n",
    "hotels_df_mod['previous_cancellations'] = hotels_df_mod['previous_cancellations'].astype(bool)\n",
    "\n",
    "hotels_df_mod['total_of_special_requests'] = hotels_df_mod['total_of_special_requests'].apply(lambda x: True if x >= 1 else False)\n",
    "hotels_df_mod['total_of_special_requests'] = hotels_df_mod['total_of_special_requests'].astype(bool)\n",
    "\n",
    "hotels_df_mod['previous_bookings_not_canceled'] = hotels_df_mod['previous_bookings_not_canceled'].apply(lambda x: True if x >= 1 else False)\n",
    "hotels_df_mod['previous_bookings_not_canceled'] = hotels_df_mod['previous_bookings_not_canceled'].astype(bool)\n",
    "\n",
    "hotels_df_mod['booking_changes'] = hotels_df_mod['booking_changes'].apply(lambda x: True if x >= 1 else False)\n",
    "hotels_df_mod['booking_changes'] = hotels_df_mod['booking_changes'].astype(bool)\n",
    "\n",
    "#Eliminación de filas con outliers\n",
    "hotels_df_mod = hotels_df_mod.drop(hotels_df_mod[hotels_df_mod['adults'] > 4].index)\n",
    "\n",
    "#Agent sin definir es un valor válido, por lo que se reemplaza por Undefined\n",
    "hotels_df_mod['agent'] = hotels_df_mod['agent'].astype(str)\n",
    "hotels_df_mod['agent'] = hotels_df_mod['agent'].replace('nan', 'Undefined')\n",
    "\n",
    "#Se cambia la columna reserved_room_type por room_type_match\n",
    "hotels_df_mod = hotels_df_mod.rename(columns={'reserved_room_type': 'room_type_match'})\n",
    "\n",
    "hotels_df_mod.loc[hotels_df_mod['room_type_match'] == hotels_df_mod['assigned_room_type'], 'room_type_match'] = True\n",
    "hotels_df_mod.loc[hotels_df_mod['room_type_match'] != hotels_df_mod['assigned_room_type'], 'room_type_match'] = False\n",
    "hotels_df_mod['room_type_match'] = hotels_df_mod['room_type_match'].astype(bool)\n",
    "\n",
    "#Se normalizan los valores de las columnas numéricas cuantitativas\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "for col in hotels_df_mod.select_dtypes(include=[np.number, \"int64\", \"float64\"]).columns:\n",
    "    hotels_df_mod[col] = scaler.fit_transform(hotels_df_mod[[col]])\n",
    "\n",
    "#One-hot encoding para las columnas categóricas\n",
    "hotels_df_mod = pd.get_dummies(hotels_df_mod, columns=[\"hotel\", \"arrival_date_month\", \"meal\",\n",
    "    \"country\", \"market_segment\", \"distribution_channel\", \"assigned_room_type\", \"deposit_type\",\n",
    "    \"customer_type\", \"agent\" ], drop_first=True)\n",
    "\n",
    "#Se ordenan las columnas\n",
    "hotels_df_mod = hotels_df_mod.reindex(sorted(hotels_df_mod.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = hotels_df_mod['is_canceled'].copy()\n",
    "df_x = hotels_df_mod.drop(['is_canceled'], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.30, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos del ensamble\n",
    "\n",
    "### Modelo Árbol de Decisión (DT)\n",
    "\n",
    "Se tendrá al árbol de decisión entrenado, con los mejores hiperparámetros obtenidos anteriormente. A pesar que los atributos cuantitativos numéricos del dataset fueron normalizados, esta modificación no debería generar ningún cambio en el F1-Score obtenido anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score sobre el set de entrenamiento: 0.872\n",
      "F1-Score sobre el set de prueba: 0.842\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = tree.DecisionTreeClassifier(ccp_alpha=3.8018939632056124e-05, max_depth=16, random_state=2, criterion='gini')\n",
    "\n",
    "model = dt_classifier.fit(x_train,y_train)\n",
    "\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "print(\"F1-Score sobre el set de entrenamiento:\", round(f1_score(y_train, y_train_pred), 3))\n",
    "print(\"F1-Score sobre el set de prueba:\", round(f1_score(y_test, y_test_pred), 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo K-Nearest Neighbors (KNN)\n",
    "\n",
    "Se tendrá al modelo entrenado con KNN, con los mejores hiperparámetros obtenidos anteriormente. Tener en cuenta que el F1-Score obtenido no será el mismo porque el dataset tuvo una limpieza diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m knn_classifier \u001b[39m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[39m=\u001b[39m\u001b[39m77\u001b[39m, metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39meuclidean\u001b[39m\u001b[39m'\u001b[39m, weights\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdistance\u001b[39m\u001b[39m'\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m model \u001b[39m=\u001b[39m knn_classifier\u001b[39m.\u001b[39mfit(x_train,y_train)\n\u001b[1;32m----> 5\u001b[0m y_train_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(x_train)\n\u001b[0;32m      6\u001b[0m y_test_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(x_test)\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mF1-Score sobre el set de entrenamiento:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mround\u001b[39m(f1_score(y_train, y_train_pred), \u001b[39m3\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\xaki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py:269\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    267\u001b[0m     neigh_dist \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m     neigh_dist, neigh_ind \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkneighbors(X)\n\u001b[0;32m    271\u001b[0m classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\n\u001b[0;32m    272\u001b[0m _y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_y\n",
      "File \u001b[1;32mc:\\Users\\xaki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:822\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    815\u001b[0m use_pairwise_distances_reductions \u001b[39m=\u001b[39m (\n\u001b[0;32m    816\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbrute\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m     \u001b[39mand\u001b[39;00m ArgKmin\u001b[39m.\u001b[39mis_usable_for(\n\u001b[0;32m    818\u001b[0m         X \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meffective_metric_\n\u001b[0;32m    819\u001b[0m     )\n\u001b[0;32m    820\u001b[0m )\n\u001b[0;32m    821\u001b[0m \u001b[39mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[1;32m--> 822\u001b[0m     results \u001b[39m=\u001b[39m ArgKmin\u001b[39m.\u001b[39;49mcompute(\n\u001b[0;32m    823\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    824\u001b[0m         Y\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_X,\n\u001b[0;32m    825\u001b[0m         k\u001b[39m=\u001b[39;49mn_neighbors,\n\u001b[0;32m    826\u001b[0m         metric\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meffective_metric_,\n\u001b[0;32m    827\u001b[0m         metric_kwargs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meffective_metric_params_,\n\u001b[0;32m    828\u001b[0m         strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    829\u001b[0m         return_distance\u001b[39m=\u001b[39;49mreturn_distance,\n\u001b[0;32m    830\u001b[0m     )\n\u001b[0;32m    832\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[0;32m    833\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbrute\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mprecomputed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m issparse(X)\n\u001b[0;32m    834\u001b[0m ):\n\u001b[0;32m    835\u001b[0m     results \u001b[39m=\u001b[39m _kneighbors_from_graph(\n\u001b[0;32m    836\u001b[0m         X, n_neighbors\u001b[39m=\u001b[39mn_neighbors, return_distance\u001b[39m=\u001b[39mreturn_distance\n\u001b[0;32m    837\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\xaki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:259\u001b[0m, in \u001b[0;36mArgKmin.compute\u001b[1;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute the argkmin reduction.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \n\u001b[0;32m    180\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39mreturns.\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m Y\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mfloat64:\n\u001b[1;32m--> 259\u001b[0m     \u001b[39mreturn\u001b[39;00m ArgKmin64\u001b[39m.\u001b[39;49mcompute(\n\u001b[0;32m    260\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    261\u001b[0m         Y\u001b[39m=\u001b[39;49mY,\n\u001b[0;32m    262\u001b[0m         k\u001b[39m=\u001b[39;49mk,\n\u001b[0;32m    263\u001b[0m         metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m    264\u001b[0m         chunk_size\u001b[39m=\u001b[39;49mchunk_size,\n\u001b[0;32m    265\u001b[0m         metric_kwargs\u001b[39m=\u001b[39;49mmetric_kwargs,\n\u001b[0;32m    266\u001b[0m         strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[0;32m    267\u001b[0m         return_distance\u001b[39m=\u001b[39;49mreturn_distance,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m Y\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mfloat32:\n\u001b[0;32m    271\u001b[0m     \u001b[39mreturn\u001b[39;00m ArgKmin32\u001b[39m.\u001b[39mcompute(\n\u001b[0;32m    272\u001b[0m         X\u001b[39m=\u001b[39mX,\n\u001b[0;32m    273\u001b[0m         Y\u001b[39m=\u001b[39mY,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m         return_distance\u001b[39m=\u001b[39mreturn_distance,\n\u001b[0;32m    280\u001b[0m     )\n",
      "File \u001b[1;32msklearn\\metrics\\_pairwise_distances_reduction\\_argkmin.pyx:90\u001b[0m, in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\xaki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\threadpoolctl.py:440\u001b[0m, in \u001b[0;36m_ThreadpoolLimiter.__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__enter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m--> 440\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mtype\u001b[39m, value, traceback):\n\u001b[0;32m    441\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestore_original_limits()\n\u001b[0;32m    443\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    444\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap\u001b[39m(\u001b[39mcls\u001b[39m, controller, \u001b[39m*\u001b[39m, limits\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, user_api\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "knn_classifier = KNeighborsClassifier(n_neighbors=77, metric='euclidean', weights='distance', n_jobs=-1)\n",
    "\n",
    "model = knn_classifier.fit(x_train,y_train)\n",
    "\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "print(\"F1-Score sobre el set de entrenamiento:\", round(f1_score(y_train, y_train_pred), 3))\n",
    "print(\"F1-Score sobre el set de prueba:\", round(f1_score(y_test, y_test_pred), 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Support Vector Machine (SVM)\n",
    "\n",
    "Se tendrá al SVM entrenado con sus hiperparámetros. Tener en cuenta que el F1-Score obtenido no será el mismo porque el dataset tuvo una limpieza diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score sobre el set de entrenamiento: 0.86\n",
      "F1-Score sobre el set de prueba: 0.841\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = SVC(kernel='poly', random_state = 2)\n",
    "model = svm_classifier.fit(x_train,y_train)\n",
    "\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "print(\"F1-Score sobre el set de entrenamiento:\", round(f1_score(y_train, y_train_pred), 3))\n",
    "print(\"F1-Score sobre el set de prueba:\", round(f1_score(y_test, y_test_pred), 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Random Forest (RF)\n",
    "\n",
    "Análogamente al decision tree, se tendrá al mejor bosque entrenado, con los mejores hiperparámetros obtenidos anteriormente. A pesar que los atributos cuantitativos numéricos del dataset fueron normalizados, esta modificación no debería generar ningún cambio en el F1-Score obtenido anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score sobre el set de entrenamiento: 0.973\n",
      "F1-Score sobre el set de prueba: 0.861\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(criterion = 'gini', \n",
    "                                        min_samples_leaf= 1, \n",
    "                                        min_samples_split= 4, \n",
    "                                        n_estimators=50, \n",
    "                                        oob_score=True,\n",
    "                                        random_state=1,\n",
    "                                        n_jobs=-1)\n",
    "\n",
    "model = rf_classifier.fit(x_train,y_train)\n",
    "\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "print(\"F1-Score sobre el set de entrenamiento:\", round(f1_score(y_train, y_train_pred), 3))\n",
    "print(\"F1-Score sobre el set de prueba:\", round(f1_score(y_test, y_test_pred), 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo XGBoost\n",
    "\n",
    "Análogamente al decision tree, se tendrá al XGBoost, con los mejores hiperparámetros obtenidos anteriormente. A pesar que la limpieza del dataset fue distinta, los atributos cuantitativos numéricos del dataset fueron normalizados, esta modificación no debería generar ningún cambio en el F1-Score obtenido anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score sobre el set de entrenamiento: 0.926\n",
      "F1-Score sobre el set de prueba: 0.866\n"
     ]
    }
   ],
   "source": [
    "xgb_classifier = xgb.XGBClassifier(colsample_bytree = 0.6, learning_rate = 0.1, max_depth = 10,\n",
    "                                   n_estimators = 400, subsample = 0.7, random_state = 0, n_jobs = -1)\n",
    "\n",
    "model = xgb_classifier.fit(x_train,y_train)\n",
    "\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "print(\"F1-Score sobre el set de entrenamiento:\", round(f1_score(y_train, y_train_pred), 3))\n",
    "print(\"F1-Score sobre el set de prueba:\", round(f1_score(y_test, y_test_pred), 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armado del ensamble por stacking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función make_classification de Scikit-learn es útil para generar conjuntos de datos sintéticos que se pueden usar para probar diferentes algoritmos. El conjunto de datos que generaremos está diseñado para representar un problema de clasificación binaria basado en los siguientes parámetros:\n",
    "\n",
    "* n_features: la cantidad de features en el conjunto de datos\n",
    "* n_informative y n_redundant: la cantidad de características informativas y redundantes en el conjunto de datos.\n",
    "* n_clusters_per_class: el número de clústeres incluidos en cada clase. Los valores más altos hacen que el problema sea más difícil.\n",
    "* class_sep: controla la separación entre grupos/clases. Los valores más grandes facilitan la tarea.\n",
    "* flip_y: especifica el porcentaje de etiquetas de clase que se asignarán al azar.Útil para agregar algo de ruido al conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo un conjunto de datos para entrenar los modelos\n",
    "x, y = make_classification(n_samples=3000, \n",
    "                           n_features=20, \n",
    "                           n_informative=15, \n",
    "                           n_redundant=5,\n",
    "                           n_clusters_per_class=5,\n",
    "                           class_sep=0.7,\n",
    "                           flip_y=0.03,\n",
    "                           n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para evaluar los modelos 5fold -CV (repite 2 veces)\n",
    "def evaluate_model(model, x, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=1)\n",
    "    scores = cross_val_score(model, x, y, scoring='accuracy', cv=cv, verbose=1, n_jobs=3, error_score='raise')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para plotear a performance\n",
    "def plot_results(model_scores, name):\n",
    "    \n",
    "    model_names = list(model_scores.keys())\n",
    "    results = [model_scores[model] for model in model_names]\n",
    "    fig = go.Figure()\n",
    "    for model, result in zip(model_names, results):\n",
    "        fig.add_trace(go.Box(\n",
    "            y=result,\n",
    "            name=model,\n",
    "            boxpoints='all',\n",
    "            jitter=0.5,\n",
    "            whiskerwidth=0.2,\n",
    "            marker_size=2,\n",
    "            line_width=1)\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "    title='Performance of Different Models Using 5-Fold Cross-Validation',\n",
    "    paper_bgcolor='rgb(243, 243, 243)',\n",
    "    plot_bgcolor='rgb(243, 243, 243)',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Accuracy',\n",
    "    showlegend=False)\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamiento y evaluación de modelos individuales**\n",
    "\n",
    "Para obtener un nivel de referencia de performance y poder compararlo con el ensamble, entrenaremos y evaluaremos individualmente los siguientes modelos base:\n",
    "\n",
    "* Bosque aleatorio con 50 árboles de decisión (RF).\n",
    "* Máquina de vectores de soporte (SVM)\n",
    "* Clasificador K-vecinos más cercanos (KNN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada uno de los modelos se evaluará utilizando una estrategia de validación cruzada de cinco folds repetida dos veces. En cada iteracion cada modelo se entrenó en el 80 % de los datos y se validó en el 20 % restante.\n",
    "\n",
    "Este método da como resultado 10 puntajes de performance diferentes para cada modelo que se almacenarán en un diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Armo conjunto entrenamiento y test 80-20\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 42, train_size = 0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelos Base\n",
    "base_models = {'random_forest':RandomForestClassifier(n_estimators=50),\n",
    "               'svm': SVC(),\n",
    "               'knn': KNeighborsClassifier(n_neighbors=11)}\n",
    "\n",
    "#Guarda los scores de cada modelo\n",
    "model_scores = defaultdict()\n",
    "\n",
    "#Entreno los clasificadores\n",
    "for name, model in base_models.items():\n",
    "    print('Evaluating {}'.format(name))\n",
    "    scores = evaluate_model(model, x_train, y_train)\n",
    "    model_scores[name] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelos Base\n",
    "base_models = [('random_forest', RandomForestClassifier(n_estimators=50)),\n",
    "               ('svm', SVC()),\n",
    "               ('knn', KNeighborsClassifier(n_neighbors=11))]\n",
    "\n",
    "#Meta Modelo\n",
    "meta_model = LogisticRegressionCV()\n",
    "\n",
    "#Ensemble Stacking\n",
    "stacking_model = StackingClassifier(estimators=base_models, \n",
    "                                    final_estimator=meta_model, \n",
    "                                    passthrough=True, \n",
    "                                    cv=5,\n",
    "                                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance el modelo en entrenamiento\n",
    "stacking_scores = evaluate_model(stacking_model, x_train, y_train)\n",
    "model_scores['stacking'] = stacking_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grafico performance\n",
    "plot_results(model_scores, name='stacking_model_cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluo en conjunto de test\n",
    "#stacking_model.fit(x_train,y_train).score(x_test,y_test)\n",
    "stacking_model.fit(x_train,y_train)\n",
    "y_pred_st = stacking_model.predict(x_test)\n",
    "accuracy_score(y_test, y_pred_st)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción del test armado a partir de hotels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tabla\u001b[39m=\u001b[39mconfusion_matrix(y_test, y_test_pred)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMatriz de confusión de los datos de prueba\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m sns\u001b[39m.\u001b[39mheatmap(tabla,cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mGnBu\u001b[39m\u001b[39m'\u001b[39m,annot\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,fmt\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mg\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "tabla=confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Matriz de confusión de los datos de prueba\")\n",
    "sns.heatmap(tabla,cmap='GnBu',annot=True,fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=accuracy_score(y_train, y_train_pred)\n",
    "recall=recall_score(y_train, y_train_pred)\n",
    "f1=f1_score(y_train, y_train_pred)\n",
    "precision=precision_score(y_train, y_train_pred)\n",
    "\n",
    "print(\"Cálculo de las métricas en el conjunto de entrenamiento\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))\n",
    "\n",
    "print(classification_report(y_test,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8623615239124561\n",
      "Recall: 0.8642995065436602\n",
      "Precision: 0.8627262019488168\n",
      "f1 score: 0.863512137613204\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.86      0.86      9183\n",
      "         1.0       0.86      0.86      0.86      9322\n",
      "\n",
      "    accuracy                           0.86     18505\n",
      "   macro avg       0.86      0.86      0.86     18505\n",
      "weighted avg       0.86      0.86      0.86     18505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba= xgb_model.predict_proba(x_test)[:, 1]\n",
    "print('AUC:',roc_auc_score(y_test, y_pred_proba))\n",
    "\n",
    "y_pred=xgb_model.predict(x_test)\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(\"Matriz de confusión de los datos de prueba\")\n",
    "sns.heatmap(cm, cmap='Blues',annot=True,fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción de hotels_test\n",
    "\n",
    "Se modifica el dataset de test de manera similar al de train, para que el modelo obtenido pueda ser aplicado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('hotels_test.csv')\n",
    "\n",
    "test_df_mod = test_df.copy()\n",
    "\n",
    "#Se guarda el id de cada fila para luego poder identificarla\n",
    "id_backup = test_df_mod[['id']].copy()\n",
    "\n",
    "#Se cambia la columna reserved_room_type por room_type_match\n",
    "test_df_mod = test_df_mod.rename(columns={'reserved_room_type': 'room_type_match'})\n",
    "\n",
    "test_df_mod.loc[test_df_mod['room_type_match'] == test_df_mod['assigned_room_type'], 'room_type_match'] = True\n",
    "test_df_mod.loc[test_df_mod['room_type_match'] != test_df_mod['assigned_room_type'], 'room_type_match'] = False\n",
    "test_df_mod['room_type_match'] = test_df_mod['room_type_match'].astype(bool)\n",
    "\n",
    "#Agent sin definir es un valor válido, por lo que se reemplaza por Undefined\n",
    "test_df_mod['agent'] = test_df_mod['agent'].astype(str)\n",
    "test_df_mod['agent'] = test_df_mod['agent'].replace('nan', 'Undefined')\n",
    "\n",
    "#Transformación de variables numéricas a booleanas \n",
    "test_df_mod['required_car_parking_spaces'] = test_df_mod['required_car_parking_spaces'].apply(lambda x: True if x >= 1 else False)\n",
    "test_df_mod['required_car_parking_spaces'] = test_df_mod['required_car_parking_spaces'].astype(bool)\n",
    "\n",
    "test_df_mod['days_in_waiting_list'] = test_df_mod['days_in_waiting_list'].apply(lambda x: True if x >= 1 else False)\n",
    "test_df_mod['days_in_waiting_list'] = test_df_mod['days_in_waiting_list'].astype(bool)\n",
    "\n",
    "test_df_mod['babies'] = test_df_mod['babies'].apply(lambda x: True if x >= 1 else False)\n",
    "test_df_mod['babies'] = test_df_mod['babies'].astype(bool)\n",
    "\n",
    "test_df_mod['previous_cancellations'] = test_df_mod['previous_cancellations'].apply(lambda x: True if x >= 1 else False)\n",
    "test_df_mod['previous_cancellations'] = test_df_mod['previous_cancellations'].astype(bool)\n",
    "\n",
    "test_df_mod['total_of_special_requests'] = test_df_mod['total_of_special_requests'].apply(lambda x: True if x >= 1 else False)\n",
    "test_df_mod['total_of_special_requests'] = test_df_mod['total_of_special_requests'].astype(bool)\n",
    "\n",
    "test_df_mod['previous_bookings_not_canceled'] = test_df_mod['previous_bookings_not_canceled'].apply(lambda x: True if x >= 1 else False)\n",
    "test_df_mod['previous_bookings_not_canceled'] = test_df_mod['previous_bookings_not_canceled'].astype(bool)\n",
    "\n",
    "test_df_mod['booking_changes'] = test_df_mod['booking_changes'].apply(lambda x: True if x >= 1 else False)\n",
    "test_df_mod['booking_changes'] = test_df_mod['booking_changes'].astype(bool)\n",
    "\n",
    "test_df_mod = test_df_mod.drop(['arrival_date_year', 'arrival_date_day_of_month', 'stays_in_weekend_nights', 'stays_in_week_nights', 'children', 'company', 'adr', 'id'], axis=1)\n",
    "test_df_mod = test_df_mod.drop(['reservation_status_date'], axis='columns') #Esta es la columna que no debería estar en el dataset de test\n",
    "\n",
    "#Se normalizan los valores de las columnas numéricas cuantitativas\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "for col in test_df_mod.select_dtypes(include=[np.number, \"int64\", \"float64\"]).columns:\n",
    "    test_df_mod[col] = scaler.fit_transform(test_df_mod[[col]])\n",
    "\n",
    "#One-hot encoding para las columnas categóricas\n",
    "test_df_mod = pd.get_dummies(test_df_mod, columns=[\"hotel\", \"arrival_date_month\", \"meal\", \"country\", \"market_segment\", \"distribution_channel\", \"assigned_room_type\", \"deposit_type\", \"customer_type\", \"agent\" ], drop_first=True)\n",
    "\n",
    "#Se crean las columnas que están en el df para entrenar pero no en el df a predecir\n",
    "for col in df_x.columns:\n",
    "    if col not in test_df_mod.columns:\n",
    "        test_df_mod[col] = False\n",
    "\n",
    "#Se eliminan las columnas que están en el df para predecir pero no en el df para entrenar\n",
    "for col in test_df_mod.columns:\n",
    "    if col not in df_x.columns:\n",
    "        test_df_mod = test_df_mod.drop(columns=[col])\n",
    "\n",
    "#Se ordenan las columnas\n",
    "test_df_mod = test_df_mod.reindex(sorted(test_df_mod.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se realiza una predicción sobre test utilizando el modelo\n",
    "y_pred = model.predict(test_df_mod)\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "predictions['id'] = id_backup['id'].values\n",
    "predictions['is_canceled'] = y_pred.astype(int)\n",
    "\n",
    "predictions.to_csv('sample_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

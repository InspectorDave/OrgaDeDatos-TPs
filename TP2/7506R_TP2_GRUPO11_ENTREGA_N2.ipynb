{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://i.ytimg.com/vi/Wm8ftqDZUVk/maxresdefault.jpg\" alt=\"FIUBA\" width=\"25%\"/>\n",
    "  </p>\n",
    "  \n",
    "# **Trabajo Pr√°ctico 2: Cr√≠ticas Cinematogr√°ficas**\n",
    "### **Grupo**: 11 - Los Pandas üêº\n",
    "### **Cuatrimestre**: 2¬∫C 2023\n",
    "### **Corrector**: Mateo Suster\n",
    "### **Integrantes**:\n",
    "- ### 106861 - Labollita, Francisco\n",
    "- ### 102312 - Mundani Vegega, Ezequiel\n",
    "- ###  97263 - Otegui, Mat√≠as I√±aki"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>review_es</th>\n",
       "      <th>sentimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Uno de los otros cr√≠ticos ha mencionado que de...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Una peque√±a peque√±a producci√≥n.La t√©cnica de f...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Pens√© que esta era una manera maravillosa de p...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>B√°sicamente, hay una familia donde un ni√±o peq...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>El \"amor en el tiempo\" de Petter Mattei es una...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                          review_es sentimiento\n",
       "0   0  Uno de los otros cr√≠ticos ha mencionado que de...    positivo\n",
       "1   1  Una peque√±a peque√±a producci√≥n.La t√©cnica de f...    positivo\n",
       "2   2  Pens√© que esta era una manera maravillosa de p...    positivo\n",
       "3   3  B√°sicamente, hay una familia donde un ni√±o peq...    negativo\n",
       "4   4  El \"amor en el tiempo\" de Petter Mattei es una...    positivo"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar caracteres extra√±os? tipo _&%$#@ )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementaci√≥n del bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '00000' '00000000000' '0000000000001' '00000001' '00001'\n",
      " '0001' '00015' '000dm' '001' '002' '003830' '006' '0069' '007' '0079'\n",
      " '007the' '0080' '0083']\n",
      "['antisocial' 'antisociales' 'antiste' 'antisunciados' 'antit'\n",
      " 'antitabaco' 'antitanque' 'antiterroristas' 'antitesis' 'antitetico'\n",
      " 'antithesis' 'antithetical' 'antitica' 'antitm' 'antitreideros'\n",
      " 'antitrust' 'antivirus' 'antiwar' 'antm' 'antoina']\n",
      "['zyuranger' 'zz' 'zzzz' 'zzzzip' 'zzzzz' 'zzzzzzzzz' 'zzzzzzzzzzzzzz'\n",
      " 'zzzzzzzzzzzzzzzzz' 'zzzzzzzzzzzzzzzzzzzz' 'zzzzzzzzzzzzzzzzzzzzz'\n",
      " 'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz'\n",
      " 'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz' '√¶bler' '√¶on' '√¶sthetic'\n",
      " '√∏stbye' '√æo' '√æorleifsson' '◊ô◊í◊ê◊ú' '◊õ◊®◊û◊ï◊ü']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(strip_accents='unicode')\n",
    "vectorizer.fit_transform(reviews['review_es'])\n",
    "\n",
    "# Primeros 20 elementos\n",
    "print(vectorizer.get_feature_names_out()[:20])\n",
    "# Elementos del medio\n",
    "print(vectorizer.get_feature_names_out()[10000:10020])\n",
    "# √öltimos 20 elementos\n",
    "print(vectorizer.get_feature_names_out()[-20:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ve que varias \"palabras\" ser√°n n√∫meros, algunas tendr√°n s√≠mbolos no pertenecientes al alfabeto espa√±ol y tambi√©n se comprueba que est√°n palabras espa√±olas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering del bag of words\n",
    "\n",
    "En primer lugar, siendo que todas las palabras que inician una oraci√≥n empiezan en may√∫scula, se har√° que todas las letras de palabras con una sola may√∫scula sean transformadas a min√∫sculas. De tal manera que en el siguiente ejemplo, las dos variaciones de hermosa sean una misma palabra: \"Hermosa pel√≠cula\" y \"Esta pel√≠cula es hermosa\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizerTotal = CountVectorizer(strip_accents='unicode')\n",
    "matrizApariciones = vectorizerTotal.fit_transform(reviews['review_es'])\n",
    "matrizSiAparece = vectorizerTotal.fit_transform(reviews['review_es'])\n",
    "\n",
    "# Creo un vectorizador para cr√≠ticas positivas y otro para cr√≠ticas negativas\n",
    "vectorizerPos = CountVectorizer(strip_accents='unicode')\n",
    "reviewsPos = reviews[reviews['sentimiento'] == 'positivo']\n",
    "\n",
    "vectorizerNeg = CountVectorizer(strip_accents='unicode')\n",
    "reviewsNeg = reviews[reviews['sentimiento'] == 'negativo']\n",
    "\n",
    "# Armar matriz de apariciones totales y de si aparece para reviews solo positivas y negativas\n",
    "matrizAparicionesPos = vectorizerPos.fit_transform(reviewsPos['review_es'])\n",
    "matrizSiAparecePos = vectorizerPos.fit_transform(reviewsPos['review_es'])\n",
    "matrizAparicionesNeg = vectorizerNeg.fit_transform(reviewsNeg['review_es'])\n",
    "matrizSiApareceNeg = vectorizerNeg.fit_transform(reviewsNeg['review_es'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se convierte el int64 en uint8 para que ocupe menos espacio en memoria\n",
    "matrizSiAparece = matrizSiAparece.astype('uint8').toarray()\n",
    "matrizSiAparecePos = matrizSiAparecePos.astype('uint8').toarray()\n",
    "matrizSiApareceNeg = matrizSiApareceNeg.astype('uint8').toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrizSiAparece[matrizSiAparece > 0] = 1\n",
    "matrizSiAparecePos[matrizSiAparecePos > 0] = 1\n",
    "matrizSiApareceNeg[matrizSiApareceNeg > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame()\n",
    "words_df['Palabra'] = vectorizerTotal.get_feature_names_out()\n",
    "words_df['Apariciones'] = matrizApariciones.sum(axis=0).tolist()[0]\n",
    "words_df['Tasa de aparici√≥n'] = matrizSiAparece.sum(\n",
    "    axis=0) / len(reviews['review_es'])\n",
    "\n",
    "auxDfPos = pd.DataFrame()\n",
    "auxDfPos['Palabra'] = vectorizerPos.get_feature_names_out()\n",
    "auxDfPos['Tasa de aparici√≥n positivas'] = matrizSiAparecePos.sum(axis=0) / len(reviewsPos)\n",
    "\n",
    "auxDfNeg = pd.DataFrame()\n",
    "auxDfNeg['Palabra'] = vectorizerNeg.get_feature_names_out()\n",
    "auxDfNeg['Tasa de aparici√≥n negativas'] = matrizSiApareceNeg.sum(axis=0) / len(reviewsNeg)\n",
    "\n",
    "# Unir los dos DataFrames en uno solo\n",
    "words_df = pd.merge(words_df, auxDfPos, on=['Palabra'], how='outer')\n",
    "words_df = pd.merge(words_df, auxDfNeg, on=['Palabra'], how='outer')\n",
    "\n",
    "# Rellenar los valores NaN con 0\n",
    "words_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasa de positividad\n",
    "# f(TP, TN, TA) = (TP - TN) / (2*TA)\n",
    "\n",
    "words_df['Tasa de positividad'] = (words_df['Tasa de aparici√≥n positivas'] -\n",
    "                                   words_df['Tasa de aparici√≥n negativas']) / (2 * words_df['Tasa de aparici√≥n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palabra</th>\n",
       "      <th>Apariciones</th>\n",
       "      <th>Tasa de aparici√≥n</th>\n",
       "      <th>Tasa de aparici√≥n positivas</th>\n",
       "      <th>Tasa de aparici√≥n negativas</th>\n",
       "      <th>Tasa de positividad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00</td>\n",
       "      <td>213</td>\n",
       "      <td>0.00368</td>\n",
       "      <td>0.00316</td>\n",
       "      <td>0.00420</td>\n",
       "      <td>-0.141304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000</td>\n",
       "      <td>613</td>\n",
       "      <td>0.00896</td>\n",
       "      <td>0.00780</td>\n",
       "      <td>0.01012</td>\n",
       "      <td>-0.129464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000000000001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00000001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00015</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>000dm</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>001</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>002</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>003830</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>006</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0069</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>007</td>\n",
       "      <td>73</td>\n",
       "      <td>0.00086</td>\n",
       "      <td>0.00136</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>0.581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0079</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>007the</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0080</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0083</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Palabra  Apariciones  Tasa de aparici√≥n  \\\n",
       "0              00          213            0.00368   \n",
       "1             000          613            0.00896   \n",
       "2           00000            4            0.00004   \n",
       "3     00000000000            2            0.00002   \n",
       "4   0000000000001            1            0.00002   \n",
       "5        00000001            1            0.00002   \n",
       "6           00001            2            0.00004   \n",
       "7            0001            1            0.00002   \n",
       "8           00015            1            0.00002   \n",
       "9           000dm            1            0.00002   \n",
       "10            001            5            0.00010   \n",
       "11            002            1            0.00002   \n",
       "12         003830            1            0.00002   \n",
       "13            006            1            0.00002   \n",
       "14           0069            1            0.00002   \n",
       "15            007           73            0.00086   \n",
       "16           0079            2            0.00004   \n",
       "17         007the            1            0.00002   \n",
       "18           0080            4            0.00006   \n",
       "19           0083            3            0.00006   \n",
       "\n",
       "    Tasa de aparici√≥n positivas  Tasa de aparici√≥n negativas  \\\n",
       "0                       0.00316                      0.00420   \n",
       "1                       0.00780                      0.01012   \n",
       "2                       0.00008                      0.00000   \n",
       "3                       0.00000                      0.00004   \n",
       "4                       0.00000                      0.00004   \n",
       "5                       0.00000                      0.00004   \n",
       "6                       0.00000                      0.00008   \n",
       "7                       0.00004                      0.00000   \n",
       "8                       0.00000                      0.00004   \n",
       "9                       0.00000                      0.00004   \n",
       "10                      0.00000                      0.00020   \n",
       "11                      0.00004                      0.00000   \n",
       "12                      0.00004                      0.00000   \n",
       "13                      0.00004                      0.00000   \n",
       "14                      0.00000                      0.00004   \n",
       "15                      0.00136                      0.00036   \n",
       "16                      0.00008                      0.00000   \n",
       "17                      0.00004                      0.00000   \n",
       "18                      0.00012                      0.00000   \n",
       "19                      0.00012                      0.00000   \n",
       "\n",
       "    Tasa de positividad  \n",
       "0             -0.141304  \n",
       "1             -0.129464  \n",
       "2              1.000000  \n",
       "3             -1.000000  \n",
       "4             -1.000000  \n",
       "5             -1.000000  \n",
       "6             -1.000000  \n",
       "7              1.000000  \n",
       "8             -1.000000  \n",
       "9             -1.000000  \n",
       "10            -1.000000  \n",
       "11             1.000000  \n",
       "12             1.000000  \n",
       "13             1.000000  \n",
       "14            -1.000000  \n",
       "15             0.581395  \n",
       "16             1.000000  \n",
       "17             1.000000  \n",
       "18             1.000000  \n",
       "19             1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172382"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas donde la columna \"Palabra\" contiene un n√∫mero\n",
    "words_df = words_df[~words_df['Palabra'].str.contains('\\d', na=False)]\n",
    "\n",
    "# Eliminar filas donde la columna \"Apariciones\" es menor a 3\n",
    "words_df = words_df[words_df['Apariciones'] >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70607"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palabra</th>\n",
       "      <th>Apariciones</th>\n",
       "      <th>Tasa de aparici√≥n</th>\n",
       "      <th>Tasa de aparici√≥n positivas</th>\n",
       "      <th>Tasa de aparici√≥n negativas</th>\n",
       "      <th>Tasa de positividad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>____</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>-0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>_____</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>_a</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>_atlantis_</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>_el</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>_is_</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657</th>\n",
       "      <td>aa</td>\n",
       "      <td>27</td>\n",
       "      <td>0.00044</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>0.00064</td>\n",
       "      <td>-0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1658</th>\n",
       "      <td>aaa</td>\n",
       "      <td>16</td>\n",
       "      <td>0.00026</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.00032</td>\n",
       "      <td>-0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>aaah</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>aaargh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>-0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>aadd</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>aag</td>\n",
       "      <td>23</td>\n",
       "      <td>0.00022</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00040</td>\n",
       "      <td>-0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>aage</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>aaliyah</td>\n",
       "      <td>27</td>\n",
       "      <td>0.00032</td>\n",
       "      <td>0.00040</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>aames</td>\n",
       "      <td>14</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>aamir</td>\n",
       "      <td>49</td>\n",
       "      <td>0.00054</td>\n",
       "      <td>0.00084</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>aankhen</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>aapke</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>-0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>aardman</td>\n",
       "      <td>28</td>\n",
       "      <td>0.00034</td>\n",
       "      <td>0.00052</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>0.529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>aargh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Palabra  Apariciones  Tasa de aparici√≥n  Tasa de aparici√≥n positivas  \\\n",
       "1505        ____            5            0.00010                      0.00008   \n",
       "1506       _____            3            0.00006                      0.00008   \n",
       "1521          _a            4            0.00006                      0.00004   \n",
       "1539  _atlantis_            3            0.00002                      0.00000   \n",
       "1561         _el            5            0.00010                      0.00012   \n",
       "1585        _is_            6            0.00012                      0.00008   \n",
       "1657          aa           27            0.00044                      0.00024   \n",
       "1658         aaa           16            0.00026                      0.00020   \n",
       "1677        aaah            3            0.00006                      0.00004   \n",
       "1682      aaargh            5            0.00010                      0.00004   \n",
       "1693        aadd            3            0.00006                      0.00004   \n",
       "1695         aag           23            0.00022                      0.00004   \n",
       "1696        aage            3            0.00006                      0.00012   \n",
       "1719     aaliyah           27            0.00032                      0.00040   \n",
       "1724       aames           14            0.00020                      0.00036   \n",
       "1725       aamir           49            0.00054                      0.00084   \n",
       "1733     aankhen            6            0.00010                      0.00012   \n",
       "1736       aapke            7            0.00010                      0.00008   \n",
       "1741     aardman           28            0.00034                      0.00052   \n",
       "1746       aargh            5            0.00008                      0.00000   \n",
       "\n",
       "      Tasa de aparici√≥n negativas  Tasa de positividad  \n",
       "1505                      0.00012            -0.200000  \n",
       "1506                      0.00004             0.333333  \n",
       "1521                      0.00008            -0.333333  \n",
       "1539                      0.00004            -1.000000  \n",
       "1561                      0.00008             0.200000  \n",
       "1585                      0.00016            -0.333333  \n",
       "1657                      0.00064            -0.454545  \n",
       "1658                      0.00032            -0.230769  \n",
       "1677                      0.00008            -0.333333  \n",
       "1682                      0.00016            -0.600000  \n",
       "1693                      0.00008            -0.333333  \n",
       "1695                      0.00040            -0.818182  \n",
       "1696                      0.00000             1.000000  \n",
       "1719                      0.00024             0.250000  \n",
       "1724                      0.00004             0.800000  \n",
       "1725                      0.00024             0.555556  \n",
       "1733                      0.00008             0.200000  \n",
       "1736                      0.00012            -0.200000  \n",
       "1741                      0.00016             0.529412  \n",
       "1746                      0.00016            -1.000000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se entrena un m√≥delo posible de Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_default = RandomForestClassifier()\n",
    "rfc_default.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels_df_x = pd.get_dummies(reviews, columns=[\"ID\", \"review_es\", \"sentimiento\"], drop_first=True)\n",
    "\n",
    "hotels_df_x = hotels_df_x.drop(['sentimiento'], axis='columns')\n",
    "\n",
    "hotels_df_x = hotels_df_x.reindex(sorted(hotels_df_x.columns), axis=1)\n",
    "\n",
    "hotels_df_y = hotels_df['is_canceled'].copy()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(hotels_df_x,\n",
    "                                                    hotels_df_y,\n",
    "                                                    test_size=0.3,  # proporcion 70/30\n",
    "                                                    random_state=2)  # semilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_features='sqrt',\n",
    "                             oob_score=True,\n",
    "                             random_state=2,\n",
    "                             n_jobs=-1,\n",
    "                             criterion=\"entropy\",\n",
    "                             min_samples_leaf=5,\n",
    "                             min_samples_split=5,\n",
    "                             n_estimators=50)\n",
    "\n",
    "rfc_model = rfc.fit(X=x_train, y=y_train)\n",
    "\n",
    "y_test_pred = rfc_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negativo       0.84      0.83      0.84      4961\n",
      "    positivo       0.84      0.84      0.84      5039\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.84      0.84      0.84     10000\n",
      "weighted avg       0.84      0.84      0.84     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    reviews['review_es'], reviews['sentimiento'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear una matriz de t√©rminos de documento utilizando CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Entrenar el modelo de Random Forest\n",
    "clf = RandomForestClassifier(max_features='sqrt',\n",
    "                             oob_score=True,\n",
    "                             random_state=2,\n",
    "                             n_jobs=-1,\n",
    "                             criterion=\"entropy\",\n",
    "                             min_samples_leaf=5,\n",
    "                             min_samples_split=5,\n",
    "                             n_estimators=50)\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "# Transformar los datos de prueba y hacer predicciones\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "y_pred = clf.predict(X_test_counts)\n",
    "\n",
    "# Imprimir el informe de clasificaci√≥n\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos de prueba\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Aseg√∫rate de que tu DataFrame de prueba tiene la misma estructura que el DataFrame de entrenamiento\n",
    "# En este caso, necesitamos asegurarnos de que tiene una columna 'review_es'\n",
    "\n",
    "# Transformar los datos de prueba y hacer predicciones\n",
    "X_test_counts = vectorizer.transform(df_test['review_es'])\n",
    "y_pred_test = clf.predict(X_test_counts)\n",
    "\n",
    "# A√±adir las predicciones al DataFrame de prueba\n",
    "df_test['sentimiento'] = y_pred_test\n",
    "\n",
    "df_test.drop(\"review_es\", axis=1, inplace=True)\n",
    "\n",
    "# Guardar el DataFrame de prueba con las predicciones en un nuevo archivo csv\n",
    "df_test.to_csv('sample_solution.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo (A PARTIR DE ACA NO ESTA HECHO :C)\n",
    "\n",
    "Primero se ve cu√°l es el mejor tipo de clasificador para el modelo, se prueba con Bernoulli, Multinomial y Gaussiano. Luego se optimizan sus hiperpar√°metros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_x = reviews['review_es'].copy()\n",
    "reviews_y = reviews['sentimiento'].copy()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(reviews_x, reviews_y, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    MultinomialNB(),\n",
    "    ComplementNB(),\n",
    "    BernoulliNB(),\n",
    "]\n",
    "\n",
    "vectorizers = [\n",
    "    CountVectorizer(),\n",
    "    TfidfVectorizer()\n",
    "]\n",
    "\n",
    "for v in vectorizers:\n",
    "    for c in classifiers:\n",
    "        model = make_pipeline(v, c)\n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        predicted_categories = model.predict(x_test)\n",
    "\n",
    "        print(\"Para\", v, \",\", c, \"la precision es\", round(accuracy_score(y_test, predicted_categories), 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An√°lisis del mejor modelo entrenado\n",
    "\n",
    "Se obtuvo que el mejor modelo es un CountVectorizer con clasificador multinomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bool = (y_train == 'positivo').astype(int)\n",
    "y_train_pred_bool = (y_train_pred == 'positivo').astype(int)\n",
    "y_test_bool = (y_test == 'positivo').astype(int)\n",
    "y_test_pred_bool = (y_test_pred == 'positivo').astype(int)\n",
    "\n",
    "train_score = f1_score(y_train_bool.values, y_train_pred_bool)\n",
    "test_score = f1_score(y_test_bool.values, y_test_pred_bool)\n",
    "\n",
    "print(\"Matriz de confusi√≥n de los datos de prueba:\")\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(cm, cmap='Blues',annot=True,fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=accuracy_score(y_train_bool, y_train_pred_bool)\n",
    "recall=recall_score(y_train_bool, y_train_pred_bool)\n",
    "f1=f1_score(y_train_bool, y_train_pred_bool)\n",
    "precision=precision_score(y_train_bool, y_train_pred_bool)\n",
    "\n",
    "print(\"C√°lculo de las m√©tricas en el conjunto de entrenamiento\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))\n",
    "\n",
    "accuracy=accuracy_score(y_test_bool,y_test_pred_bool)\n",
    "recall=recall_score(y_test_bool,y_test_pred_bool)\n",
    "f1=f1_score(y_test_bool,y_test_pred_bool)\n",
    "precision=precision_score(y_test_bool,y_test_pred_bool)\n",
    "\n",
    "print(\"\\nC√°lculo de las m√©tricas en el conjunto de pruebas\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"mnb\", MultinomialNB())])\n",
    "\n",
    "params_grid = {\n",
    "        'tfidf__ngram_range': [(1,1), (1,2), (2,2)],\n",
    "        'tfidf__max_features': [1000, 10000, 100000],\n",
    "        'mnb__alpha': [0.001, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "scorer_fn = make_scorer(f1_score, pos_label='positivo')\n",
    "kfoldcv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "gridcv = GridSearchCV(estimator=model,\n",
    "                      param_grid = params_grid,\n",
    "                      scoring=scorer_fn,\n",
    "                      cv=kfoldcv\n",
    "                      )\n",
    "\n",
    "model = gridcv.fit(x_train,y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "score = f1_score(y_test, y_pred, pos_label='positivo')\n",
    "print(\"Par√°metros:\", gridcv.best_params_, \"\\nF1 score: \", round(score, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(x_train)\n",
    "\n",
    "y_train_bool = (y_train == 'positivo').astype(int)\n",
    "y_train_pred_bool = (y_train_pred == 'positivo').astype(int)\n",
    "y_test_bool = (y_test == 'positivo').astype(int)\n",
    "y_test_pred_bool = (y_pred == 'positivo').astype(int)\n",
    "\n",
    "train_score = f1_score(y_train_bool.values, y_train_pred_bool)\n",
    "test_score = f1_score(y_test_bool.values, y_test_pred_bool)\n",
    "\n",
    "print(\"Matriz de confusi√≥n de los datos de prueba:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, cmap='Blues',annot=True,fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=accuracy_score(y_train_bool, y_train_pred_bool)\n",
    "recall=recall_score(y_train_bool, y_train_pred_bool)\n",
    "f1=f1_score(y_train_bool, y_train_pred_bool)\n",
    "precision=precision_score(y_train_bool, y_train_pred_bool)\n",
    "\n",
    "print(\"C√°lculo de las m√©tricas en el conjunto de entrenamiento\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))\n",
    "\n",
    "accuracy=accuracy_score(y_test_bool,y_test_pred_bool)\n",
    "recall=recall_score(y_test_bool,y_test_pred_bool)\n",
    "f1=f1_score(y_test_bool,y_test_pred_bool)\n",
    "precision=precision_score(y_test_bool,y_test_pred_bool)\n",
    "\n",
    "print(\"\\nC√°lculo de las m√©tricas en el conjunto de pruebas\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nueva Hip√≥tesis: Filtrar los reviews"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este TP no fue necesario analizar y filtrar la base de datos antes de crear el Bayes Naive model. Pero se pens√≥ cuantificar qu√© tan negativo o positivo son los reviews y luego modificar el dataset seg√∫n eso. Habr√≠a dos maneras de filtrarlo: solo tomando los reviews m√°s extremos o solamente tomar los mas neutrales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe una librer√≠a llamada TextBlob para intentar lograr esto. El an√°lisis de sentimiento de TextBlob implica el uso de un modelo de aprendizaje autom√°tico previamente entrenado para asignar una puntuaci√≥n de polaridad a un fragmento de texto determinado. El modelo eval√∫a las palabras y frases del texto y proporciona una puntuaci√≥n num√©rica que indica la positividad o negatividad del sentimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "reviews_hip = reviews.copy()\n",
    "\n",
    "def quantify_reviews(review):\n",
    "    analysis = TextBlob(review)\n",
    "    return analysis.sentiment.polarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funci√≥n quantify_reviews() agarra cada review, lo transforma en un objeto TextBlob y analiza el sentimiento del review, devolviendo un float entre -1 y 1 donde 1 es extremadamente positivo y -1 se extremadamente negativo.\n",
    "\n",
    "Ahora se deben encontrar los thresholds optimos para probar nuestra hip√≥tesis y saber qu√© reviews eliminar antes de entrenar al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_hip['score'] = reviews_hip['review_es'].apply(quantify_reviews)\n",
    "\n",
    "pos_threshold = 0.1\n",
    "neg_threshold = -0.305\n",
    "filtered_data = reviews_hip[((reviews_hip['score'] < pos_threshold) & (reviews_hip['score'] > neg_threshold))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_hip_x = filtered_data['review_es'].copy()\n",
    "reviews_hip_y = filtered_data['sentimiento'].copy()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(reviews_hip_x, reviews_hip_y, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bool = (y_train == 'positivo').astype(int)\n",
    "y_train_pred_bool = (y_train_pred == 'positivo').astype(int)\n",
    "y_test_bool = (y_test == 'positivo').astype(int)\n",
    "y_test_pred_bool = (y_test_pred == 'positivo').astype(int)\n",
    "\n",
    "train_score = f1_score(y_train_bool.values, y_train_pred_bool)\n",
    "test_score = f1_score(y_test_bool.values, y_test_pred_bool)\n",
    "\n",
    "print(\"Matriz de confusi√≥n de los datos de prueba:\")\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(cm, cmap='Blues',annot=True,fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=accuracy_score(y_train_bool, y_train_pred_bool)\n",
    "recall=recall_score(y_train_bool, y_train_pred_bool)\n",
    "f1=f1_score(y_train_bool, y_train_pred_bool)\n",
    "precision=precision_score(y_train_bool, y_train_pred_bool)\n",
    "\n",
    "print(\"C√°lculo de las m√©tricas en el conjunto de entrenamiento\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))\n",
    "\n",
    "accuracy=accuracy_score(y_test_bool,y_test_pred_bool)\n",
    "recall=recall_score(y_test_bool,y_test_pred_bool)\n",
    "f1=f1_score(y_test_bool,y_test_pred_bool)\n",
    "precision=precision_score(y_test_bool,y_test_pred_bool)\n",
    "\n",
    "print(\"\\nC√°lculo de las m√©tricas en el conjunto de pruebas\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de un probar diferentes thesholds manualmente y probar ambas maneras de filtrar, nos dio mejores resultados tomar los reviews m√°s neutrales con un threshold de -0.305 < x < 0.1. Intentaremos probar con valores m√°s cercanos a este threshold para conseguir un mejor resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_theshold = (0,0)\n",
    "best_score = 0\n",
    "\n",
    "\n",
    "for pos_threshold in np.arange(0.095, 0.1055, 0.0025):\n",
    "    for neg_threshold in np.arange(-0.315, -0.295, 0.0025):\n",
    "        #print(\"Threshold: \" + str(neg_threshold) + \" < x < \" + str(pos_threshold))\n",
    "\n",
    "        filtered_data = reviews_hip[((reviews_hip['score'] < pos_threshold) & (reviews_hip['score'] > neg_threshold))]\n",
    "\n",
    "        reviews_hip_x = filtered_data['review_es'].copy()\n",
    "        reviews_hip_y = filtered_data['sentimiento'].copy()\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(reviews_hip_x, reviews_hip_y, test_size=0.30, random_state=0)\n",
    "        \n",
    "        model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(x_train)\n",
    "        y_test_pred = model.predict(x_test)\n",
    "\n",
    "        y_train_bool = (y_train == 'positivo').astype(int)\n",
    "        y_train_pred_bool = (y_train_pred == 'positivo').astype(int)\n",
    "        y_test_bool = (y_test == 'positivo').astype(int)\n",
    "        y_test_pred_bool = (y_test_pred == 'positivo').astype(int)\n",
    "\n",
    "        f1=f1_score(y_test_bool,y_test_pred_bool)\n",
    "\n",
    "        #print(\"\\nC√°lculo de las m√©tricas en el conjunto de pruebas\")\n",
    "        #print(\"F1 score: \", round(f1, 3))\n",
    "\n",
    "        #print(\"-----------------------------------------------------\")\n",
    "\n",
    "        if f1 > best_score:\n",
    "            best_score = f1\n",
    "            best_theshold = [neg_threshold, pos_threshold]\n",
    "\n",
    "\n",
    "print(\"Best threshold: \" + str(best_theshold[0]) + \" < x < \" + str(round(best_theshold[1], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = reviews_hip[((reviews_hip['score'] < best_theshold[1]) & (reviews_hip['score'] > best_theshold[0]))]\n",
    "\n",
    "reviews_hip_x = filtered_data['review_es'].copy()\n",
    "reviews_hip_y = filtered_data['sentimiento'].copy()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(reviews_hip_x, reviews_hip_y, test_size=0.30, random_state=0)\n",
    "\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train_bool = (y_train == 'positivo').astype(int)\n",
    "y_train_pred_bool = (y_train_pred == 'positivo').astype(int)\n",
    "y_test_bool = (y_test == 'positivo').astype(int)\n",
    "y_test_pred_bool = (y_test_pred == 'positivo').astype(int)\n",
    "\n",
    "train_score = f1_score(y_train_bool.values, y_train_pred_bool)\n",
    "test_score = f1_score(y_test_bool.values, y_test_pred_bool)\n",
    "\n",
    "print(\"Matriz de confusi√≥n de los datos de prueba:\")\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(cm, cmap='Blues',annot=True,fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=accuracy_score(y_train_bool, y_train_pred_bool)\n",
    "recall=recall_score(y_train_bool, y_train_pred_bool)\n",
    "f1=f1_score(y_train_bool, y_train_pred_bool)\n",
    "precision=precision_score(y_train_bool, y_train_pred_bool)\n",
    "\n",
    "print(\"C√°lculo de las m√©tricas en el conjunto de entrenamiento\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))\n",
    "\n",
    "accuracy=accuracy_score(y_test_bool,y_test_pred_bool)\n",
    "recall=recall_score(y_test_bool,y_test_pred_bool)\n",
    "f1=f1_score(y_test_bool,y_test_pred_bool)\n",
    "precision=precision_score(y_test_bool,y_test_pred_bool)\n",
    "\n",
    "print(\"\\nC√°lculo de las m√©tricas en el conjunto de pruebas\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hip√≥tesis + Grid Search\n",
    "\n",
    "Le aplico un Grid Search a nuestro nuevo dataset filtrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = reviews_hip[((reviews_hip['score'] < best_theshold[1]) & (reviews_hip['score'] > best_theshold[0]))]\n",
    "\n",
    "reviews_hip_x = filtered_data['review_es'].copy()\n",
    "reviews_hip_y = filtered_data['sentimiento'].copy()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(reviews_hip_x, reviews_hip_y, test_size=0.30, random_state=0)\n",
    "\n",
    "\n",
    "model = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"mnb\", MultinomialNB())])\n",
    "\n",
    "params_grid = {\n",
    "        'tfidf__ngram_range': [(1,1), (1,2), (2,2)],\n",
    "        'tfidf__max_features': [1000, 10000, 100000],\n",
    "        'mnb__alpha': [0.001, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "scorer_fn = make_scorer(f1_score, pos_label='positivo')\n",
    "kfoldcv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "gridcv = GridSearchCV(estimator=model,\n",
    "                      param_grid = params_grid,\n",
    "                      scoring=scorer_fn,\n",
    "                      cv=kfoldcv\n",
    "                      )\n",
    "\n",
    "model = gridcv.fit(x_train,y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "score = f1_score(y_test, y_pred, pos_label='positivo')\n",
    "print(\"Par√°metros:\", gridcv.best_params_, \"\\nF1 score: \", round(score, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(x_train)\n",
    "\n",
    "y_train_bool = (y_train == 'positivo').astype(int)\n",
    "y_train_pred_bool = (y_train_pred == 'positivo').astype(int)\n",
    "y_test_bool = (y_test == 'positivo').astype(int)\n",
    "y_test_pred_bool = (y_pred == 'positivo').astype(int)\n",
    "\n",
    "train_score = f1_score(y_train_bool.values, y_train_pred_bool)\n",
    "test_score = f1_score(y_test_bool.values, y_test_pred_bool)\n",
    "\n",
    "print(\"Matriz de confusi√≥n de los datos de prueba:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, cmap='Blues',annot=True,fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=accuracy_score(y_train_bool, y_train_pred_bool)\n",
    "recall=recall_score(y_train_bool, y_train_pred_bool)\n",
    "f1=f1_score(y_train_bool, y_train_pred_bool)\n",
    "precision=precision_score(y_train_bool, y_train_pred_bool)\n",
    "\n",
    "print(\"C√°lculo de las m√©tricas en el conjunto de entrenamiento\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))\n",
    "\n",
    "accuracy=accuracy_score(y_test_bool,y_test_pred_bool)\n",
    "recall=recall_score(y_test_bool,y_test_pred_bool)\n",
    "f1=f1_score(y_test_bool,y_test_pred_bool)\n",
    "precision=precision_score(y_test_bool,y_test_pred_bool)\n",
    "\n",
    "print(\"\\nC√°lculo de las m√©tricas en el conjunto de pruebas\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicci√≥n del conjunto test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "predictions['ID'] = test['ID'].values\n",
    "predictions['sentimiento'] = model.predict(test['review_es'])\n",
    "\n",
    "predictions.to_csv('sample_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

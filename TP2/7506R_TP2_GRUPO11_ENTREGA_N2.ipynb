{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://i.ytimg.com/vi/Wm8ftqDZUVk/maxresdefault.jpg\" alt=\"FIUBA\" width=\"25%\"/>\n",
    "  </p>\n",
    "  \n",
    "# **Trabajo Pr√°ctico 2: Cr√≠ticas Cinematogr√°ficas**\n",
    "### **Grupo**: 11 - Los Pandas üêº\n",
    "### **Cuatrimestre**: 2¬∫C 2023\n",
    "### **Corrector**: Mateo Suster\n",
    "### **Integrantes**:\n",
    "- ### 106861 - Labollita, Francisco\n",
    "- ### 102312 - Mundani Vegega, Ezequiel\n",
    "- ###  97263 - Otegui, Mat√≠as I√±aki"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('train.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementaci√≥n del bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '00000' '00000000000' '0000000000001' '00000001' '00001'\n",
      " '0001' '00015' '000dm' '001' '002' '003830' '006' '0069' '007' '0079'\n",
      " '007the' '0080' '0083']\n",
      "['antisocial' 'antisociales' 'antiste' 'antisunciados' 'antit'\n",
      " 'antitabaco' 'antitanque' 'antiterroristas' 'antitesis' 'antitetico'\n",
      " 'antithesis' 'antithetical' 'antitica' 'antitm' 'antitreideros'\n",
      " 'antitrust' 'antivirus' 'antiwar' 'antm' 'antoina']\n",
      "['zyuranger' 'zz' 'zzzz' 'zzzzip' 'zzzzz' 'zzzzzzzzz' 'zzzzzzzzzzzzzz'\n",
      " 'zzzzzzzzzzzzzzzzz' 'zzzzzzzzzzzzzzzzzzzz' 'zzzzzzzzzzzzzzzzzzzzz'\n",
      " 'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz'\n",
      " 'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz' '√¶bler' '√¶on' '√¶sthetic'\n",
      " '√∏stbye' '√æo' '√æorleifsson' '◊ô◊í◊ê◊ú' '◊õ◊®◊û◊ï◊ü']\n"
     ]
    }
   ],
   "source": [
    "vectorizerTotal = CountVectorizer(strip_accents='unicode', dtype='uint16')\n",
    "vectorizerTotal.fit_transform(reviews['review_es'])\n",
    "\n",
    "# Primeros 20 elementos\n",
    "print(vectorizerTotal.get_feature_names_out()[:20])\n",
    "# Elementos del medio\n",
    "print(vectorizerTotal.get_feature_names_out()[10000:10020])\n",
    "# √öltimos 20 elementos\n",
    "print(vectorizerTotal.get_feature_names_out()[-20:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ve que varias \"palabras\" ser√°n n√∫meros, algunas tendr√°n s√≠mbolos no pertenecientes al alfabeto espa√±ol y tambi√©n se comprueba que est√°n palabras espa√±olas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering del bag of words\n",
    "\n",
    "En primer lugar, siendo que todas las palabras que inician una oraci√≥n empiezan en may√∫scula, se har√° que todas las letras de palabras con una sola may√∫scula sean transformadas a min√∫sculas. De tal manera que en el siguiente ejemplo, las dos variaciones de hermosa sean una misma palabra: \"Hermosa pel√≠cula\" y \"Esta pel√≠cula es hermosa\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrizApariciones = vectorizerTotal.fit_transform(reviews['review_es'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrizSiAparece = matrizApariciones.toarray()\n",
    "matrizApariciones = matrizApariciones.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrizSiAparece[matrizSiAparece > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame()\n",
    "words_df['Palabra'] = vectorizerTotal.get_feature_names_out()\n",
    "words_df['Apariciones Totales'] = matrizApariciones.sum(axis=0).tolist() #Cu√°ntas veces aparece la palabra\n",
    "words_df['Apariciones'] = matrizSiAparece.sum(axis=0).tolist()           #En cu√°ntas reviews aparece la palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se cuentan en cu√°ntas reviews positivas aparece cada palabra\n",
    "listaAparicionesPositivas = np.zeros(shape=len(matrizSiAparece[0])).astype('int32')\n",
    "for i in range(reviews.shape[0]):\n",
    "    if (reviews.iloc[i]['sentimiento'] == 'positivo'):\n",
    "        listaAparicionesPositivas += matrizSiAparece[i]\n",
    "\n",
    "#Se cuentan en cu√°ntas reviews negativas aparece cada palabra\n",
    "listaAparicionesNegativas = np.zeros(shape=len(matrizSiAparece[0])).astype('int32')\n",
    "for i in range(reviews.shape[0]):\n",
    "    if (reviews.iloc[i]['sentimiento'] == 'negativo'):\n",
    "        listaAparicionesNegativas += matrizSiAparece[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palabra</th>\n",
       "      <th>Apariciones Totales</th>\n",
       "      <th>Apariciones</th>\n",
       "      <th>Apariciones positivas</th>\n",
       "      <th>Apariciones negativas</th>\n",
       "      <th>Fracci√≥n apariciones positivas</th>\n",
       "      <th>Fracci√≥n apariciones negativas</th>\n",
       "      <th>Tasa de positividad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41364</th>\n",
       "      <td>de</td>\n",
       "      <td>661907</td>\n",
       "      <td>47992</td>\n",
       "      <td>23949</td>\n",
       "      <td>24043</td>\n",
       "      <td>0.499021</td>\n",
       "      <td>0.500979</td>\n",
       "      <td>-0.001959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128119</th>\n",
       "      <td>que</td>\n",
       "      <td>395365</td>\n",
       "      <td>47245</td>\n",
       "      <td>23501</td>\n",
       "      <td>23744</td>\n",
       "      <td>0.497428</td>\n",
       "      <td>0.502572</td>\n",
       "      <td>-0.005143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93125</th>\n",
       "      <td>la</td>\n",
       "      <td>405160</td>\n",
       "      <td>47147</td>\n",
       "      <td>23496</td>\n",
       "      <td>23651</td>\n",
       "      <td>0.498356</td>\n",
       "      <td>0.501644</td>\n",
       "      <td>-0.003288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55382</th>\n",
       "      <td>en</td>\n",
       "      <td>276429</td>\n",
       "      <td>45938</td>\n",
       "      <td>22882</td>\n",
       "      <td>23056</td>\n",
       "      <td>0.498106</td>\n",
       "      <td>0.501894</td>\n",
       "      <td>-0.003788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53511</th>\n",
       "      <td>el</td>\n",
       "      <td>253915</td>\n",
       "      <td>45037</td>\n",
       "      <td>22381</td>\n",
       "      <td>22656</td>\n",
       "      <td>0.496947</td>\n",
       "      <td>0.503053</td>\n",
       "      <td>-0.006106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162711</th>\n",
       "      <td>una</td>\n",
       "      <td>170883</td>\n",
       "      <td>43530</td>\n",
       "      <td>21815</td>\n",
       "      <td>21715</td>\n",
       "      <td>0.501149</td>\n",
       "      <td>0.498851</td>\n",
       "      <td>0.002297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58627</th>\n",
       "      <td>es</td>\n",
       "      <td>183244</td>\n",
       "      <td>43210</td>\n",
       "      <td>21708</td>\n",
       "      <td>21502</td>\n",
       "      <td>0.502384</td>\n",
       "      <td>0.497616</td>\n",
       "      <td>0.004767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162707</th>\n",
       "      <td>un</td>\n",
       "      <td>186195</td>\n",
       "      <td>43041</td>\n",
       "      <td>21390</td>\n",
       "      <td>21651</td>\n",
       "      <td>0.496968</td>\n",
       "      <td>0.503032</td>\n",
       "      <td>-0.006064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111087</th>\n",
       "      <td>no</td>\n",
       "      <td>145805</td>\n",
       "      <td>42253</td>\n",
       "      <td>19918</td>\n",
       "      <td>22335</td>\n",
       "      <td>0.471398</td>\n",
       "      <td>0.528602</td>\n",
       "      <td>-0.057203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60252</th>\n",
       "      <td>esta</td>\n",
       "      <td>119728</td>\n",
       "      <td>40952</td>\n",
       "      <td>20067</td>\n",
       "      <td>20885</td>\n",
       "      <td>0.490013</td>\n",
       "      <td>0.509987</td>\n",
       "      <td>-0.019975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118438</th>\n",
       "      <td>pelicula</td>\n",
       "      <td>152104</td>\n",
       "      <td>40806</td>\n",
       "      <td>19855</td>\n",
       "      <td>20951</td>\n",
       "      <td>0.486571</td>\n",
       "      <td>0.513429</td>\n",
       "      <td>-0.026859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97832</th>\n",
       "      <td>los</td>\n",
       "      <td>152856</td>\n",
       "      <td>40578</td>\n",
       "      <td>20261</td>\n",
       "      <td>20317</td>\n",
       "      <td>0.499310</td>\n",
       "      <td>0.500690</td>\n",
       "      <td>-0.001380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141390</th>\n",
       "      <td>se</td>\n",
       "      <td>133582</td>\n",
       "      <td>39130</td>\n",
       "      <td>19190</td>\n",
       "      <td>19940</td>\n",
       "      <td>0.490417</td>\n",
       "      <td>0.509583</td>\n",
       "      <td>-0.019167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116552</th>\n",
       "      <td>para</td>\n",
       "      <td>97435</td>\n",
       "      <td>36707</td>\n",
       "      <td>18044</td>\n",
       "      <td>18663</td>\n",
       "      <td>0.491568</td>\n",
       "      <td>0.508432</td>\n",
       "      <td>-0.016863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97217</th>\n",
       "      <td>lo</td>\n",
       "      <td>95253</td>\n",
       "      <td>36142</td>\n",
       "      <td>17692</td>\n",
       "      <td>18450</td>\n",
       "      <td>0.489514</td>\n",
       "      <td>0.510486</td>\n",
       "      <td>-0.020973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123007</th>\n",
       "      <td>por</td>\n",
       "      <td>90928</td>\n",
       "      <td>35212</td>\n",
       "      <td>17239</td>\n",
       "      <td>17973</td>\n",
       "      <td>0.489577</td>\n",
       "      <td>0.510423</td>\n",
       "      <td>-0.020845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94141</th>\n",
       "      <td>las</td>\n",
       "      <td>91951</td>\n",
       "      <td>35105</td>\n",
       "      <td>17806</td>\n",
       "      <td>17299</td>\n",
       "      <td>0.507221</td>\n",
       "      <td>0.492779</td>\n",
       "      <td>0.014442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33831</th>\n",
       "      <td>con</td>\n",
       "      <td>91465</td>\n",
       "      <td>34970</td>\n",
       "      <td>17586</td>\n",
       "      <td>17384</td>\n",
       "      <td>0.502888</td>\n",
       "      <td>0.497112</td>\n",
       "      <td>0.005776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32985</th>\n",
       "      <td>como</td>\n",
       "      <td>84761</td>\n",
       "      <td>34018</td>\n",
       "      <td>17018</td>\n",
       "      <td>17000</td>\n",
       "      <td>0.500265</td>\n",
       "      <td>0.499735</td>\n",
       "      <td>0.000529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119542</th>\n",
       "      <td>pero</td>\n",
       "      <td>73518</td>\n",
       "      <td>33475</td>\n",
       "      <td>16249</td>\n",
       "      <td>17226</td>\n",
       "      <td>0.485407</td>\n",
       "      <td>0.514593</td>\n",
       "      <td>-0.029186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151202</th>\n",
       "      <td>su</td>\n",
       "      <td>88452</td>\n",
       "      <td>30757</td>\n",
       "      <td>15848</td>\n",
       "      <td>14909</td>\n",
       "      <td>0.515265</td>\n",
       "      <td>0.484735</td>\n",
       "      <td>0.030530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101488</th>\n",
       "      <td>mas</td>\n",
       "      <td>58220</td>\n",
       "      <td>28718</td>\n",
       "      <td>14532</td>\n",
       "      <td>14186</td>\n",
       "      <td>0.506024</td>\n",
       "      <td>0.493976</td>\n",
       "      <td>0.012048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42710</th>\n",
       "      <td>del</td>\n",
       "      <td>53680</td>\n",
       "      <td>26292</td>\n",
       "      <td>13441</td>\n",
       "      <td>12851</td>\n",
       "      <td>0.511220</td>\n",
       "      <td>0.488780</td>\n",
       "      <td>0.022440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144451</th>\n",
       "      <td>si</td>\n",
       "      <td>44728</td>\n",
       "      <td>25604</td>\n",
       "      <td>11897</td>\n",
       "      <td>13707</td>\n",
       "      <td>0.464654</td>\n",
       "      <td>0.535346</td>\n",
       "      <td>-0.070692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6321</th>\n",
       "      <td>al</td>\n",
       "      <td>40893</td>\n",
       "      <td>23656</td>\n",
       "      <td>11802</td>\n",
       "      <td>11854</td>\n",
       "      <td>0.498901</td>\n",
       "      <td>0.501099</td>\n",
       "      <td>-0.002198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102728</th>\n",
       "      <td>me</td>\n",
       "      <td>37814</td>\n",
       "      <td>21582</td>\n",
       "      <td>10532</td>\n",
       "      <td>11050</td>\n",
       "      <td>0.487999</td>\n",
       "      <td>0.512001</td>\n",
       "      <td>-0.024001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60570</th>\n",
       "      <td>este</td>\n",
       "      <td>32320</td>\n",
       "      <td>20714</td>\n",
       "      <td>9860</td>\n",
       "      <td>10854</td>\n",
       "      <td>0.476007</td>\n",
       "      <td>0.523993</td>\n",
       "      <td>-0.047987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69397</th>\n",
       "      <td>fue</td>\n",
       "      <td>35072</td>\n",
       "      <td>19974</td>\n",
       "      <td>9355</td>\n",
       "      <td>10619</td>\n",
       "      <td>0.468359</td>\n",
       "      <td>0.531641</td>\n",
       "      <td>-0.063282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147792</th>\n",
       "      <td>solo</td>\n",
       "      <td>29436</td>\n",
       "      <td>19525</td>\n",
       "      <td>8818</td>\n",
       "      <td>10707</td>\n",
       "      <td>0.451626</td>\n",
       "      <td>0.548374</td>\n",
       "      <td>-0.096748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60811</th>\n",
       "      <td>esto</td>\n",
       "      <td>27721</td>\n",
       "      <td>18323</td>\n",
       "      <td>7959</td>\n",
       "      <td>10364</td>\n",
       "      <td>0.434372</td>\n",
       "      <td>0.565628</td>\n",
       "      <td>-0.131256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Palabra  Apariciones Totales  Apariciones  Apariciones positivas  \\\n",
       "41364         de               661907        47992                  23949   \n",
       "128119       que               395365        47245                  23501   \n",
       "93125         la               405160        47147                  23496   \n",
       "55382         en               276429        45938                  22882   \n",
       "53511         el               253915        45037                  22381   \n",
       "162711       una               170883        43530                  21815   \n",
       "58627         es               183244        43210                  21708   \n",
       "162707        un               186195        43041                  21390   \n",
       "111087        no               145805        42253                  19918   \n",
       "60252       esta               119728        40952                  20067   \n",
       "118438  pelicula               152104        40806                  19855   \n",
       "97832        los               152856        40578                  20261   \n",
       "141390        se               133582        39130                  19190   \n",
       "116552      para                97435        36707                  18044   \n",
       "97217         lo                95253        36142                  17692   \n",
       "123007       por                90928        35212                  17239   \n",
       "94141        las                91951        35105                  17806   \n",
       "33831        con                91465        34970                  17586   \n",
       "32985       como                84761        34018                  17018   \n",
       "119542      pero                73518        33475                  16249   \n",
       "151202        su                88452        30757                  15848   \n",
       "101488       mas                58220        28718                  14532   \n",
       "42710        del                53680        26292                  13441   \n",
       "144451        si                44728        25604                  11897   \n",
       "6321          al                40893        23656                  11802   \n",
       "102728        me                37814        21582                  10532   \n",
       "60570       este                32320        20714                   9860   \n",
       "69397        fue                35072        19974                   9355   \n",
       "147792      solo                29436        19525                   8818   \n",
       "60811       esto                27721        18323                   7959   \n",
       "\n",
       "        Apariciones negativas  Fracci√≥n apariciones positivas  \\\n",
       "41364                   24043                        0.499021   \n",
       "128119                  23744                        0.497428   \n",
       "93125                   23651                        0.498356   \n",
       "55382                   23056                        0.498106   \n",
       "53511                   22656                        0.496947   \n",
       "162711                  21715                        0.501149   \n",
       "58627                   21502                        0.502384   \n",
       "162707                  21651                        0.496968   \n",
       "111087                  22335                        0.471398   \n",
       "60252                   20885                        0.490013   \n",
       "118438                  20951                        0.486571   \n",
       "97832                   20317                        0.499310   \n",
       "141390                  19940                        0.490417   \n",
       "116552                  18663                        0.491568   \n",
       "97217                   18450                        0.489514   \n",
       "123007                  17973                        0.489577   \n",
       "94141                   17299                        0.507221   \n",
       "33831                   17384                        0.502888   \n",
       "32985                   17000                        0.500265   \n",
       "119542                  17226                        0.485407   \n",
       "151202                  14909                        0.515265   \n",
       "101488                  14186                        0.506024   \n",
       "42710                   12851                        0.511220   \n",
       "144451                  13707                        0.464654   \n",
       "6321                    11854                        0.498901   \n",
       "102728                  11050                        0.487999   \n",
       "60570                   10854                        0.476007   \n",
       "69397                   10619                        0.468359   \n",
       "147792                  10707                        0.451626   \n",
       "60811                   10364                        0.434372   \n",
       "\n",
       "        Fracci√≥n apariciones negativas  Tasa de positividad  \n",
       "41364                         0.500979            -0.001959  \n",
       "128119                        0.502572            -0.005143  \n",
       "93125                         0.501644            -0.003288  \n",
       "55382                         0.501894            -0.003788  \n",
       "53511                         0.503053            -0.006106  \n",
       "162711                        0.498851             0.002297  \n",
       "58627                         0.497616             0.004767  \n",
       "162707                        0.503032            -0.006064  \n",
       "111087                        0.528602            -0.057203  \n",
       "60252                         0.509987            -0.019975  \n",
       "118438                        0.513429            -0.026859  \n",
       "97832                         0.500690            -0.001380  \n",
       "141390                        0.509583            -0.019167  \n",
       "116552                        0.508432            -0.016863  \n",
       "97217                         0.510486            -0.020973  \n",
       "123007                        0.510423            -0.020845  \n",
       "94141                         0.492779             0.014442  \n",
       "33831                         0.497112             0.005776  \n",
       "32985                         0.499735             0.000529  \n",
       "119542                        0.514593            -0.029186  \n",
       "151202                        0.484735             0.030530  \n",
       "101488                        0.493976             0.012048  \n",
       "42710                         0.488780             0.022440  \n",
       "144451                        0.535346            -0.070692  \n",
       "6321                          0.501099            -0.002198  \n",
       "102728                        0.512001            -0.024001  \n",
       "60570                         0.523993            -0.047987  \n",
       "69397                         0.531641            -0.063282  \n",
       "147792                        0.548374            -0.096748  \n",
       "60811                         0.565628            -0.131256  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df['Apariciones positivas'] = listaAparicionesPositivas\n",
    "words_df['Apariciones negativas'] = listaAparicionesNegativas\n",
    "words_df['Fracci√≥n apariciones positivas'] = words_df['Apariciones positivas'] / words_df['Apariciones']\n",
    "words_df['Fracci√≥n apariciones negativas'] = words_df['Apariciones negativas'] / words_df['Apariciones']\n",
    "words_df['Tasa de positividad'] = (words_df['Apariciones positivas'] - words_df['Apariciones negativas']) / words_df['Apariciones']\n",
    "words_df.sort_values(by='Apariciones', inplace=True, ascending=False)\n",
    "words_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lo q est√° para abajo ni idea si funciona o no, despu√©s lo corrijo -Eze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, words_df.shape[0]):\n",
    "    words_df['Apariciones positivas'][i] = reviews[reviews['sentiment'] == 'positive'].sum(axis=0).tolist()[0][i]\n",
    "#obtener la cantidad de filas de un dataframe\n",
    "\n",
    "#seleccionar solo las filas en que sentimiento es positivo\n",
    "reviewsPos = reviews[reviews['sentiment'] == 'positive']\n",
    "\n",
    "words_df['Tasa de aparici√≥n'] = matrizSiAparece.sum(\n",
    "    axis=0) / len(reviews['review_es'])\n",
    "\n",
    "auxDfPos['Palabra'] = vectorizerPos.get_feature_names_out()\n",
    "auxDfPos['Tasa de aparici√≥n positivas'] = matrizSiAparecePos.sum(axis=0) / len(reviewsPos)\n",
    "\n",
    "auxDfNeg['Palabra'] = vectorizerNeg.get_feature_names_out()\n",
    "auxDfNeg['Tasa de aparici√≥n negativas'] = matrizSiApareceNeg.sum(axis=0) / len(reviewsNeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasa de positividad\n",
    "# f(TP, TN, TA) = (TP - TN) / (2*TA)\n",
    "\n",
    "words_df['Tasa de positividad'] = (words_df['Tasa de aparici√≥n positivas'] -\n",
    "                                   words_df['Tasa de aparici√≥n negativas']) / (2 * words_df['Tasa de aparici√≥n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palabra</th>\n",
       "      <th>Apariciones</th>\n",
       "      <th>Tasa de aparici√≥n</th>\n",
       "      <th>Tasa de aparici√≥n positivas</th>\n",
       "      <th>Tasa de aparici√≥n negativas</th>\n",
       "      <th>Tasa de positividad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00</td>\n",
       "      <td>213</td>\n",
       "      <td>0.00368</td>\n",
       "      <td>0.00316</td>\n",
       "      <td>0.00420</td>\n",
       "      <td>-0.141304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000</td>\n",
       "      <td>613</td>\n",
       "      <td>0.00896</td>\n",
       "      <td>0.00780</td>\n",
       "      <td>0.01012</td>\n",
       "      <td>-0.129464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000000000001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00000001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00015</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>000dm</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>001</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>002</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>003830</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>006</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0069</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>007</td>\n",
       "      <td>73</td>\n",
       "      <td>0.00086</td>\n",
       "      <td>0.00136</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>0.581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0079</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>007the</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0080</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0083</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Palabra  Apariciones  Tasa de aparici√≥n  \\\n",
       "0              00          213            0.00368   \n",
       "1             000          613            0.00896   \n",
       "2           00000            4            0.00004   \n",
       "3     00000000000            2            0.00002   \n",
       "4   0000000000001            1            0.00002   \n",
       "5        00000001            1            0.00002   \n",
       "6           00001            2            0.00004   \n",
       "7            0001            1            0.00002   \n",
       "8           00015            1            0.00002   \n",
       "9           000dm            1            0.00002   \n",
       "10            001            5            0.00010   \n",
       "11            002            1            0.00002   \n",
       "12         003830            1            0.00002   \n",
       "13            006            1            0.00002   \n",
       "14           0069            1            0.00002   \n",
       "15            007           73            0.00086   \n",
       "16           0079            2            0.00004   \n",
       "17         007the            1            0.00002   \n",
       "18           0080            4            0.00006   \n",
       "19           0083            3            0.00006   \n",
       "\n",
       "    Tasa de aparici√≥n positivas  Tasa de aparici√≥n negativas  \\\n",
       "0                       0.00316                      0.00420   \n",
       "1                       0.00780                      0.01012   \n",
       "2                       0.00008                      0.00000   \n",
       "3                       0.00000                      0.00004   \n",
       "4                       0.00000                      0.00004   \n",
       "5                       0.00000                      0.00004   \n",
       "6                       0.00000                      0.00008   \n",
       "7                       0.00004                      0.00000   \n",
       "8                       0.00000                      0.00004   \n",
       "9                       0.00000                      0.00004   \n",
       "10                      0.00000                      0.00020   \n",
       "11                      0.00004                      0.00000   \n",
       "12                      0.00004                      0.00000   \n",
       "13                      0.00004                      0.00000   \n",
       "14                      0.00000                      0.00004   \n",
       "15                      0.00136                      0.00036   \n",
       "16                      0.00008                      0.00000   \n",
       "17                      0.00004                      0.00000   \n",
       "18                      0.00012                      0.00000   \n",
       "19                      0.00012                      0.00000   \n",
       "\n",
       "    Tasa de positividad  \n",
       "0             -0.141304  \n",
       "1             -0.129464  \n",
       "2              1.000000  \n",
       "3             -1.000000  \n",
       "4             -1.000000  \n",
       "5             -1.000000  \n",
       "6             -1.000000  \n",
       "7              1.000000  \n",
       "8             -1.000000  \n",
       "9             -1.000000  \n",
       "10            -1.000000  \n",
       "11             1.000000  \n",
       "12             1.000000  \n",
       "13             1.000000  \n",
       "14            -1.000000  \n",
       "15             0.581395  \n",
       "16             1.000000  \n",
       "17             1.000000  \n",
       "18             1.000000  \n",
       "19             1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172382"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas donde la columna \"Palabra\" contiene un n√∫mero\n",
    "words_df = words_df[~words_df['Palabra'].str.contains('\\d', na=False)]\n",
    "\n",
    "# Eliminar filas donde la columna \"Apariciones\" es menor a 3\n",
    "words_df = words_df[words_df['Apariciones'] >= 3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se entrena un m√≥delo posible de Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_default = RandomForestClassifier()\n",
    "rfc_default.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels_df_x = pd.get_dummies(reviews, columns=[\"ID\", \"review_es\", \"sentimiento\"], drop_first=True)\n",
    "\n",
    "hotels_df_x = hotels_df_x.drop(['sentimiento'], axis='columns')\n",
    "\n",
    "hotels_df_x = hotels_df_x.reindex(sorted(hotels_df_x.columns), axis=1)\n",
    "\n",
    "hotels_df_y = hotels_df['is_canceled'].copy()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(hotels_df_x,\n",
    "                                                    hotels_df_y,\n",
    "                                                    test_size=0.3,  # proporcion 70/30\n",
    "                                                    random_state=2)  # semilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_features='sqrt',\n",
    "                             oob_score=True,\n",
    "                             random_state=2,\n",
    "                             n_jobs=-1,\n",
    "                             criterion=\"entropy\",\n",
    "                             min_samples_leaf=5,\n",
    "                             min_samples_split=5,\n",
    "                             n_estimators=50)\n",
    "\n",
    "rfc_model = rfc.fit(X=x_train, y=y_train)\n",
    "\n",
    "y_test_pred = rfc_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negativo       0.84      0.83      0.84      4961\n",
      "    positivo       0.84      0.84      0.84      5039\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.84      0.84      0.84     10000\n",
      "weighted avg       0.84      0.84      0.84     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    reviews['review_es'], reviews['sentimiento'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear una matriz de t√©rminos de documento utilizando CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Entrenar el modelo de Random Forest\n",
    "clf = RandomForestClassifier(max_features='sqrt',\n",
    "                             oob_score=True,\n",
    "                             random_state=2,\n",
    "                             n_jobs=-1,\n",
    "                             criterion=\"entropy\",\n",
    "                             min_samples_leaf=5,\n",
    "                             min_samples_split=5,\n",
    "                             n_estimators=50)\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "# Transformar los datos de prueba y hacer predicciones\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "y_pred = clf.predict(X_test_counts)\n",
    "\n",
    "# Imprimir el informe de clasificaci√≥n\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos de prueba\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Aseg√∫rate de que tu DataFrame de prueba tiene la misma estructura que el DataFrame de entrenamiento\n",
    "# En este caso, necesitamos asegurarnos de que tiene una columna 'review_es'\n",
    "\n",
    "# Transformar los datos de prueba y hacer predicciones\n",
    "X_test_counts = vectorizer.transform(df_test['review_es'])\n",
    "y_pred_test = clf.predict(X_test_counts)\n",
    "\n",
    "# A√±adir las predicciones al DataFrame de prueba\n",
    "df_test['sentimiento'] = y_pred_test\n",
    "\n",
    "df_test.drop(\"review_es\", axis=1, inplace=True)\n",
    "\n",
    "# Guardar el DataFrame de prueba con las predicciones en un nuevo archivo csv\n",
    "df_test.to_csv('sample_solution.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo (A PARTIR DE ACA NO ESTA HECHO :C)\n",
    "\n",
    "Primero se ve cu√°l es el mejor tipo de clasificador para el modelo, se prueba con Bernoulli, Multinomial y Gaussiano. Luego se optimizan sus hiperpar√°metros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_x = reviews['review_es'].copy()\n",
    "reviews_y = reviews['sentimiento'].copy()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(reviews_x, reviews_y, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    MultinomialNB(),\n",
    "    ComplementNB(),\n",
    "    BernoulliNB(),\n",
    "]\n",
    "\n",
    "vectorizers = [\n",
    "    CountVectorizer(),\n",
    "    TfidfVectorizer()\n",
    "]\n",
    "\n",
    "for v in vectorizers:\n",
    "    for c in classifiers:\n",
    "        model = make_pipeline(v, c)\n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        predicted_categories = model.predict(x_test)\n",
    "\n",
    "        print(\"Para\", v, \",\", c, \"la precision es\", round(accuracy_score(y_test, predicted_categories), 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An√°lisis del mejor modelo entrenado\n",
    "\n",
    "Se obtuvo que el mejor modelo es un CountVectorizer con clasificador multinomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bool = (y_train == 'positivo').astype(int)\n",
    "y_train_pred_bool = (y_train_pred == 'positivo').astype(int)\n",
    "y_test_bool = (y_test == 'positivo').astype(int)\n",
    "y_test_pred_bool = (y_test_pred == 'positivo').astype(int)\n",
    "\n",
    "train_score = f1_score(y_train_bool.values, y_train_pred_bool)\n",
    "test_score = f1_score(y_test_bool.values, y_test_pred_bool)\n",
    "\n",
    "print(\"Matriz de confusi√≥n de los datos de prueba:\")\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(cm, cmap='Blues',annot=True,fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=accuracy_score(y_train_bool, y_train_pred_bool)\n",
    "recall=recall_score(y_train_bool, y_train_pred_bool)\n",
    "f1=f1_score(y_train_bool, y_train_pred_bool)\n",
    "precision=precision_score(y_train_bool, y_train_pred_bool)\n",
    "\n",
    "print(\"C√°lculo de las m√©tricas en el conjunto de entrenamiento\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))\n",
    "\n",
    "accuracy=accuracy_score(y_test_bool,y_test_pred_bool)\n",
    "recall=recall_score(y_test_bool,y_test_pred_bool)\n",
    "f1=f1_score(y_test_bool,y_test_pred_bool)\n",
    "precision=precision_score(y_test_bool,y_test_pred_bool)\n",
    "\n",
    "print(\"\\nC√°lculo de las m√©tricas en el conjunto de pruebas\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"mnb\", MultinomialNB())])\n",
    "\n",
    "params_grid = {\n",
    "        'tfidf__ngram_range': [(1,1), (1,2), (2,2)],\n",
    "        'tfidf__max_features': [1000, 10000, 100000],\n",
    "        'mnb__alpha': [0.001, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "scorer_fn = make_scorer(f1_score, pos_label='positivo')\n",
    "kfoldcv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "gridcv = GridSearchCV(estimator=model,\n",
    "                      param_grid = params_grid,\n",
    "                      scoring=scorer_fn,\n",
    "                      cv=kfoldcv\n",
    "                      )\n",
    "\n",
    "model = gridcv.fit(x_train,y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "score = f1_score(y_test, y_pred, pos_label='positivo')\n",
    "print(\"Par√°metros:\", gridcv.best_params_, \"\\nF1 score: \", round(score, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(x_train)\n",
    "\n",
    "y_train_bool = (y_train == 'positivo').astype(int)\n",
    "y_train_pred_bool = (y_train_pred == 'positivo').astype(int)\n",
    "y_test_bool = (y_test == 'positivo').astype(int)\n",
    "y_test_pred_bool = (y_pred == 'positivo').astype(int)\n",
    "\n",
    "train_score = f1_score(y_train_bool.values, y_train_pred_bool)\n",
    "test_score = f1_score(y_test_bool.values, y_test_pred_bool)\n",
    "\n",
    "print(\"Matriz de confusi√≥n de los datos de prueba:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, cmap='Blues',annot=True,fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=accuracy_score(y_train_bool, y_train_pred_bool)\n",
    "recall=recall_score(y_train_bool, y_train_pred_bool)\n",
    "f1=f1_score(y_train_bool, y_train_pred_bool)\n",
    "precision=precision_score(y_train_bool, y_train_pred_bool)\n",
    "\n",
    "print(\"C√°lculo de las m√©tricas en el conjunto de entrenamiento\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))\n",
    "\n",
    "accuracy=accuracy_score(y_test_bool,y_test_pred_bool)\n",
    "recall=recall_score(y_test_bool,y_test_pred_bool)\n",
    "f1=f1_score(y_test_bool,y_test_pred_bool)\n",
    "precision=precision_score(y_test_bool,y_test_pred_bool)\n",
    "\n",
    "print(\"\\nC√°lculo de las m√©tricas en el conjunto de pruebas\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nueva Hip√≥tesis: Filtrar los reviews"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este TP no fue necesario analizar y filtrar la base de datos antes de crear el Bayes Naive model. Pero se pens√≥ cuantificar qu√© tan negativo o positivo son los reviews y luego modificar el dataset seg√∫n eso. Habr√≠a dos maneras de filtrarlo: solo tomando los reviews m√°s extremos o solamente tomar los mas neutrales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe una librer√≠a llamada TextBlob para intentar lograr esto. El an√°lisis de sentimiento de TextBlob implica el uso de un modelo de aprendizaje autom√°tico previamente entrenado para asignar una puntuaci√≥n de polaridad a un fragmento de texto determinado. El modelo eval√∫a las palabras y frases del texto y proporciona una puntuaci√≥n num√©rica que indica la positividad o negatividad del sentimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "reviews_hip = reviews.copy()\n",
    "\n",
    "def quantify_reviews(review):\n",
    "    analysis = TextBlob(review)\n",
    "    return analysis.sentiment.polarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funci√≥n quantify_reviews() agarra cada review, lo transforma en un objeto TextBlob y analiza el sentimiento del review, devolviendo un float entre -1 y 1 donde 1 es extremadamente positivo y -1 se extremadamente negativo.\n",
    "\n",
    "Ahora se deben encontrar los thresholds optimos para probar nuestra hip√≥tesis y saber qu√© reviews eliminar antes de entrenar al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_hip['score'] = reviews_hip['review_es'].apply(quantify_reviews)\n",
    "\n",
    "pos_threshold = 0.1\n",
    "neg_threshold = -0.305\n",
    "filtered_data = reviews_hip[((reviews_hip['score'] < pos_threshold) & (reviews_hip['score'] > neg_threshold))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_hip_x = filtered_data['review_es'].copy()\n",
    "reviews_hip_y = filtered_data['sentimiento'].copy()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(reviews_hip_x, reviews_hip_y, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bool = (y_train == 'positivo').astype(int)\n",
    "y_train_pred_bool = (y_train_pred == 'positivo').astype(int)\n",
    "y_test_bool = (y_test == 'positivo').astype(int)\n",
    "y_test_pred_bool = (y_test_pred == 'positivo').astype(int)\n",
    "\n",
    "train_score = f1_score(y_train_bool.values, y_train_pred_bool)\n",
    "test_score = f1_score(y_test_bool.values, y_test_pred_bool)\n",
    "\n",
    "print(\"Matriz de confusi√≥n de los datos de prueba:\")\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(cm, cmap='Blues',annot=True,fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=accuracy_score(y_train_bool, y_train_pred_bool)\n",
    "recall=recall_score(y_train_bool, y_train_pred_bool)\n",
    "f1=f1_score(y_train_bool, y_train_pred_bool)\n",
    "precision=precision_score(y_train_bool, y_train_pred_bool)\n",
    "\n",
    "print(\"C√°lculo de las m√©tricas en el conjunto de entrenamiento\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))\n",
    "\n",
    "accuracy=accuracy_score(y_test_bool,y_test_pred_bool)\n",
    "recall=recall_score(y_test_bool,y_test_pred_bool)\n",
    "f1=f1_score(y_test_bool,y_test_pred_bool)\n",
    "precision=precision_score(y_test_bool,y_test_pred_bool)\n",
    "\n",
    "print(\"\\nC√°lculo de las m√©tricas en el conjunto de pruebas\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de un probar diferentes thesholds manualmente y probar ambas maneras de filtrar, nos dio mejores resultados tomar los reviews m√°s neutrales con un threshold de -0.305 < x < 0.1. Intentaremos probar con valores m√°s cercanos a este threshold para conseguir un mejor resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_theshold = (0,0)\n",
    "best_score = 0\n",
    "\n",
    "\n",
    "for pos_threshold in np.arange(0.095, 0.1055, 0.0025):\n",
    "    for neg_threshold in np.arange(-0.315, -0.295, 0.0025):\n",
    "        #print(\"Threshold: \" + str(neg_threshold) + \" < x < \" + str(pos_threshold))\n",
    "\n",
    "        filtered_data = reviews_hip[((reviews_hip['score'] < pos_threshold) & (reviews_hip['score'] > neg_threshold))]\n",
    "\n",
    "        reviews_hip_x = filtered_data['review_es'].copy()\n",
    "        reviews_hip_y = filtered_data['sentimiento'].copy()\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(reviews_hip_x, reviews_hip_y, test_size=0.30, random_state=0)\n",
    "        \n",
    "        model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(x_train)\n",
    "        y_test_pred = model.predict(x_test)\n",
    "\n",
    "        y_train_bool = (y_train == 'positivo').astype(int)\n",
    "        y_train_pred_bool = (y_train_pred == 'positivo').astype(int)\n",
    "        y_test_bool = (y_test == 'positivo').astype(int)\n",
    "        y_test_pred_bool = (y_test_pred == 'positivo').astype(int)\n",
    "\n",
    "        f1=f1_score(y_test_bool,y_test_pred_bool)\n",
    "\n",
    "        #print(\"\\nC√°lculo de las m√©tricas en el conjunto de pruebas\")\n",
    "        #print(\"F1 score: \", round(f1, 3))\n",
    "\n",
    "        #print(\"-----------------------------------------------------\")\n",
    "\n",
    "        if f1 > best_score:\n",
    "            best_score = f1\n",
    "            best_theshold = [neg_threshold, pos_threshold]\n",
    "\n",
    "\n",
    "print(\"Best threshold: \" + str(best_theshold[0]) + \" < x < \" + str(round(best_theshold[1], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = reviews_hip[((reviews_hip['score'] < best_theshold[1]) & (reviews_hip['score'] > best_theshold[0]))]\n",
    "\n",
    "reviews_hip_x = filtered_data['review_es'].copy()\n",
    "reviews_hip_y = filtered_data['sentimiento'].copy()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(reviews_hip_x, reviews_hip_y, test_size=0.30, random_state=0)\n",
    "\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train_bool = (y_train == 'positivo').astype(int)\n",
    "y_train_pred_bool = (y_train_pred == 'positivo').astype(int)\n",
    "y_test_bool = (y_test == 'positivo').astype(int)\n",
    "y_test_pred_bool = (y_test_pred == 'positivo').astype(int)\n",
    "\n",
    "train_score = f1_score(y_train_bool.values, y_train_pred_bool)\n",
    "test_score = f1_score(y_test_bool.values, y_test_pred_bool)\n",
    "\n",
    "print(\"Matriz de confusi√≥n de los datos de prueba:\")\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(cm, cmap='Blues',annot=True,fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=accuracy_score(y_train_bool, y_train_pred_bool)\n",
    "recall=recall_score(y_train_bool, y_train_pred_bool)\n",
    "f1=f1_score(y_train_bool, y_train_pred_bool)\n",
    "precision=precision_score(y_train_bool, y_train_pred_bool)\n",
    "\n",
    "print(\"C√°lculo de las m√©tricas en el conjunto de entrenamiento\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))\n",
    "\n",
    "accuracy=accuracy_score(y_test_bool,y_test_pred_bool)\n",
    "recall=recall_score(y_test_bool,y_test_pred_bool)\n",
    "f1=f1_score(y_test_bool,y_test_pred_bool)\n",
    "precision=precision_score(y_test_bool,y_test_pred_bool)\n",
    "\n",
    "print(\"\\nC√°lculo de las m√©tricas en el conjunto de pruebas\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hip√≥tesis + Grid Search\n",
    "\n",
    "Le aplico un Grid Search a nuestro nuevo dataset filtrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = reviews_hip[((reviews_hip['score'] < best_theshold[1]) & (reviews_hip['score'] > best_theshold[0]))]\n",
    "\n",
    "reviews_hip_x = filtered_data['review_es'].copy()\n",
    "reviews_hip_y = filtered_data['sentimiento'].copy()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(reviews_hip_x, reviews_hip_y, test_size=0.30, random_state=0)\n",
    "\n",
    "\n",
    "model = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"mnb\", MultinomialNB())])\n",
    "\n",
    "params_grid = {\n",
    "        'tfidf__ngram_range': [(1,1), (1,2), (2,2)],\n",
    "        'tfidf__max_features': [1000, 10000, 100000],\n",
    "        'mnb__alpha': [0.001, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "scorer_fn = make_scorer(f1_score, pos_label='positivo')\n",
    "kfoldcv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "gridcv = GridSearchCV(estimator=model,\n",
    "                      param_grid = params_grid,\n",
    "                      scoring=scorer_fn,\n",
    "                      cv=kfoldcv\n",
    "                      )\n",
    "\n",
    "model = gridcv.fit(x_train,y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "score = f1_score(y_test, y_pred, pos_label='positivo')\n",
    "print(\"Par√°metros:\", gridcv.best_params_, \"\\nF1 score: \", round(score, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(x_train)\n",
    "\n",
    "y_train_bool = (y_train == 'positivo').astype(int)\n",
    "y_train_pred_bool = (y_train_pred == 'positivo').astype(int)\n",
    "y_test_bool = (y_test == 'positivo').astype(int)\n",
    "y_test_pred_bool = (y_pred == 'positivo').astype(int)\n",
    "\n",
    "train_score = f1_score(y_train_bool.values, y_train_pred_bool)\n",
    "test_score = f1_score(y_test_bool.values, y_test_pred_bool)\n",
    "\n",
    "print(\"Matriz de confusi√≥n de los datos de prueba:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, cmap='Blues',annot=True,fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=accuracy_score(y_train_bool, y_train_pred_bool)\n",
    "recall=recall_score(y_train_bool, y_train_pred_bool)\n",
    "f1=f1_score(y_train_bool, y_train_pred_bool)\n",
    "precision=precision_score(y_train_bool, y_train_pred_bool)\n",
    "\n",
    "print(\"C√°lculo de las m√©tricas en el conjunto de entrenamiento\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))\n",
    "\n",
    "accuracy=accuracy_score(y_test_bool,y_test_pred_bool)\n",
    "recall=recall_score(y_test_bool,y_test_pred_bool)\n",
    "f1=f1_score(y_test_bool,y_test_pred_bool)\n",
    "precision=precision_score(y_test_bool,y_test_pred_bool)\n",
    "\n",
    "print(\"\\nC√°lculo de las m√©tricas en el conjunto de pruebas\")\n",
    "print(\"Accuracy: \", round(accuracy, 3))\n",
    "print(\"Recall: \", round(recall, 3))\n",
    "print(\"Precision: \", round(precision, 3))\n",
    "print(\"F1 score: \", round(f1, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicci√≥n del conjunto test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "predictions['ID'] = test['ID'].values\n",
    "predictions['sentimiento'] = model.predict(test['review_es'])\n",
    "\n",
    "predictions.to_csv('sample_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

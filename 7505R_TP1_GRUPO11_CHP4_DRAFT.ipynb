{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://i.ytimg.com/vi/Wm8ftqDZUVk/maxresdefault.jpg\" alt=\"FIUBA\" width=\"33%\"/>\n",
    "  </p>\n",
    "  \n",
    "# **Trabajo Práctico 1: Reservas de Hotel**\n",
    "### **Checkpoint**: 4\n",
    "### **Grupo**: 11 - Los Pandas\n",
    "### **Cuatrimestre**: 2ºC 2023\n",
    "### **Corrector**: Mateo\n",
    "### **Integrantes**:\n",
    "### 103456 - Labollita, Francisco\n",
    "### 102312 - Mundani Vegega, Ezequiel\n",
    "###  97263 - Otegui, Matías Iñaki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga inicial de dependencias y datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import calendar\n",
    "#import dtreeviz\n",
    "import warnings\n",
    "\n",
    "#modelos y métricas\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score,f1_score#, precision_recall_curve, roc_curve,\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "#preprocesamiento\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "##KFOLD CV Random Search para buscar el mejor arbol (los mejores atributos, hiperparametros,etc)\n",
    "from sklearn.model_selection import StratifiedKFold, KFold,RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Import Keras and Tensor\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "#Random Seed\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "warnings.filterwarnings('ignore', 'is_categorical_dtype is deprecated')\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels_df = pd.read_csv('hotels_train.csv')\n",
    "hotels_df_backup = hotels_df.copy()\n",
    "\n",
    "#Eliminación de columnas irrelevantes\n",
    "hotels_df_mod = hotels_df.drop(['arrival_date_year', 'arrival_date_day_of_month', 'stays_in_weekend_nights', 'stays_in_week_nights', 'children', 'company', 'adr', 'id'], axis=1)\n",
    "\n",
    "#Eliminación de filas con valores nulos\n",
    "hotels_df_mod = hotels_df_mod.dropna(subset=['country', 'distribution_channel', 'market_segment'])\n",
    "\n",
    "#Eliminación de filas con outliers\n",
    "hotels_df_mod = hotels_df_mod.drop(hotels_df_mod[hotels_df_mod['adults'] > 4].index)\n",
    "\n",
    "#Agent sin definir es un valor válido, por lo que se reemplaza por Undefined\n",
    "hotels_df_mod['agent'] = hotels_df_mod['agent'].astype(str)\n",
    "hotels_df_mod['agent'] = hotels_df_mod['agent'].replace('nan', 'Undefined')\n",
    "\n",
    "#Se crea la columna que dice si se asignó la habitación pedida\n",
    "hotels_df_mod = hotels_df_mod.rename(columns={'reserved_room_type': 'room_type_match'})\n",
    "\n",
    "hotels_df_mod.loc[hotels_df_mod['room_type_match'] == hotels_df_mod['assigned_room_type'], 'room_type_match'] = True\n",
    "hotels_df_mod.loc[hotels_df_mod['room_type_match'] != hotels_df_mod['assigned_room_type'], 'room_type_match'] = False\n",
    "hotels_df_mod['room_type_match'] = hotels_df_mod['room_type_match'].astype(bool)\n",
    "\n",
    "#Se normalizan los valores de las columnas numéricas cuantitativas\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "for col in hotels_df_mod.select_dtypes(include=[np.number, \"int64\", \"float64\"]).columns:\n",
    "    hotels_df_mod[col] = scaler.fit_transform(hotels_df_mod[[col]])\n",
    "\n",
    "#One-hot encoding para las columnas categóricas\n",
    "hotels_df_mod = pd.get_dummies(hotels_df_mod, columns=[\"hotel\",\n",
    "    \"arrival_date_month\", \"meal\", \"country\", \"market_segment\", \"distribution_channel\", \"assigned_room_type\",\n",
    "    \"deposit_type\", \"customer_type\", \"agent\", 'room_type_match' ], drop_first=True)\n",
    "\n",
    "hotels_df_mod = hotels_df_mod.reindex(sorted(hotels_df_mod.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the columns with data type 'bool'\n",
    "bool_columns = hotels_df_mod.select_dtypes(include=['bool'])\n",
    "\n",
    "# Convert the 'bool' columns to 'float64'\n",
    "for column in bool_columns.columns:\n",
    "    hotels_df_mod[column] = hotels_df_mod[column].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 61681 entries, 0 to 61912\n",
      "Columns: 499 entries, adults to total_of_special_requests\n",
      "dtypes: float64(499)\n",
      "memory usage: 235.3 MB\n"
     ]
    }
   ],
   "source": [
    "hotels_df_mod.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empiezo a crear el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = hotels_df_mod.drop(['is_canceled'], axis=1)\n",
    "df_y = hotels_df_mod['is_canceled'].copy()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, train_size= 0.7, test_size=0.30, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_132 (Dense)           (None, 8)                 3992      \n",
      "                                                                 \n",
      " dense_133 (Dense)           (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_134 (Dense)           (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4073 (15.91 KB)\n",
      "Trainable params: 4073 (15.91 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "d_in=len(df_x)\n",
    "# Create a simple feedforward neural network\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(8, activation='relu', input_shape=(498,)),  # Input layer\n",
    "    keras.layers.Dense(8, activation='relu'), \n",
    "    keras.layers.Dense(1, activation='sigmoid')  # Output layer \n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1080/1080 [==============================] - 1s 695us/step - loss: 0.4625 - accuracy: 0.7753 - auc: 0.8610 - val_loss: 0.3882 - val_accuracy: 0.8099 - val_auc: 0.9014\n",
      "Epoch 2/50\n",
      "1080/1080 [==============================] - 1s 552us/step - loss: 0.3726 - accuracy: 0.8154 - auc: 0.9091 - val_loss: 0.3622 - val_accuracy: 0.8202 - val_auc: 0.9138\n",
      "Epoch 3/50\n",
      "1080/1080 [==============================] - 1s 569us/step - loss: 0.3540 - accuracy: 0.8252 - auc: 0.9180 - val_loss: 0.3558 - val_accuracy: 0.8238 - val_auc: 0.9167\n",
      "Epoch 4/50\n",
      "1080/1080 [==============================] - 1s 531us/step - loss: 0.3450 - accuracy: 0.8303 - auc: 0.9222 - val_loss: 0.3493 - val_accuracy: 0.8231 - val_auc: 0.9191\n",
      "Epoch 5/50\n",
      "1080/1080 [==============================] - 1s 603us/step - loss: 0.3381 - accuracy: 0.8357 - auc: 0.9255 - val_loss: 0.3458 - val_accuracy: 0.8308 - val_auc: 0.9211\n",
      "Epoch 6/50\n",
      "1080/1080 [==============================] - 1s 596us/step - loss: 0.3333 - accuracy: 0.8373 - auc: 0.9276 - val_loss: 0.3489 - val_accuracy: 0.8246 - val_auc: 0.9215\n",
      "Epoch 7/50\n",
      "1080/1080 [==============================] - 1s 570us/step - loss: 0.3290 - accuracy: 0.8388 - auc: 0.9296 - val_loss: 0.3433 - val_accuracy: 0.8333 - val_auc: 0.9230\n",
      "Epoch 8/50\n",
      "1080/1080 [==============================] - 1s 623us/step - loss: 0.3259 - accuracy: 0.8426 - auc: 0.9311 - val_loss: 0.3464 - val_accuracy: 0.8254 - val_auc: 0.9218\n",
      "Epoch 9/50\n",
      "1080/1080 [==============================] - 1s 574us/step - loss: 0.3233 - accuracy: 0.8436 - auc: 0.9321 - val_loss: 0.3380 - val_accuracy: 0.8345 - val_auc: 0.9247\n",
      "Epoch 10/50\n",
      "1080/1080 [==============================] - 1s 545us/step - loss: 0.3212 - accuracy: 0.8450 - auc: 0.9330 - val_loss: 0.3410 - val_accuracy: 0.8350 - val_auc: 0.9247\n",
      "Epoch 11/50\n",
      "1080/1080 [==============================] - 1s 531us/step - loss: 0.3192 - accuracy: 0.8468 - auc: 0.9339 - val_loss: 0.3381 - val_accuracy: 0.8327 - val_auc: 0.9252\n",
      "Epoch 12/50\n",
      "1080/1080 [==============================] - 1s 532us/step - loss: 0.3182 - accuracy: 0.8443 - auc: 0.9343 - val_loss: 0.3380 - val_accuracy: 0.8350 - val_auc: 0.9256\n",
      "Epoch 13/50\n",
      "1080/1080 [==============================] - 1s 530us/step - loss: 0.3158 - accuracy: 0.8476 - auc: 0.9354 - val_loss: 0.3372 - val_accuracy: 0.8358 - val_auc: 0.9260\n",
      "Epoch 14/50\n",
      "1080/1080 [==============================] - 1s 532us/step - loss: 0.3139 - accuracy: 0.8488 - auc: 0.9364 - val_loss: 0.3378 - val_accuracy: 0.8396 - val_auc: 0.9266\n",
      "Epoch 15/50\n",
      "1080/1080 [==============================] - 1s 531us/step - loss: 0.3121 - accuracy: 0.8523 - auc: 0.9371 - val_loss: 0.3377 - val_accuracy: 0.8346 - val_auc: 0.9270\n",
      "Epoch 16/50\n",
      "1080/1080 [==============================] - 1s 530us/step - loss: 0.3115 - accuracy: 0.8508 - auc: 0.9373 - val_loss: 0.3357 - val_accuracy: 0.8384 - val_auc: 0.9275\n",
      "Epoch 17/50\n",
      "1080/1080 [==============================] - 1s 531us/step - loss: 0.3096 - accuracy: 0.8504 - auc: 0.9381 - val_loss: 0.3370 - val_accuracy: 0.8417 - val_auc: 0.9276\n",
      "Epoch 18/50\n",
      "1080/1080 [==============================] - 1s 530us/step - loss: 0.3091 - accuracy: 0.8524 - auc: 0.9383 - val_loss: 0.3380 - val_accuracy: 0.8386 - val_auc: 0.9274\n",
      "Epoch 19/50\n",
      "1080/1080 [==============================] - 1s 556us/step - loss: 0.3072 - accuracy: 0.8535 - auc: 0.9392 - val_loss: 0.3456 - val_accuracy: 0.8311 - val_auc: 0.9272\n",
      "Epoch 20/50\n",
      "1080/1080 [==============================] - 1s 530us/step - loss: 0.3068 - accuracy: 0.8526 - auc: 0.9392 - val_loss: 0.3366 - val_accuracy: 0.8386 - val_auc: 0.9279\n",
      "Epoch 21/50\n",
      "1080/1080 [==============================] - 1s 527us/step - loss: 0.3056 - accuracy: 0.8539 - auc: 0.9399 - val_loss: 0.3340 - val_accuracy: 0.8422 - val_auc: 0.9293\n",
      "Epoch 22/50\n",
      "1080/1080 [==============================] - 1s 527us/step - loss: 0.3054 - accuracy: 0.8541 - auc: 0.9399 - val_loss: 0.3370 - val_accuracy: 0.8382 - val_auc: 0.9285\n",
      "Epoch 23/50\n",
      "1080/1080 [==============================] - 1s 530us/step - loss: 0.3036 - accuracy: 0.8544 - auc: 0.9406 - val_loss: 0.3355 - val_accuracy: 0.8379 - val_auc: 0.9291\n",
      "Epoch 24/50\n",
      "1080/1080 [==============================] - 1s 529us/step - loss: 0.3032 - accuracy: 0.8544 - auc: 0.9408 - val_loss: 0.3370 - val_accuracy: 0.8404 - val_auc: 0.9297\n",
      "Epoch 25/50\n",
      "1080/1080 [==============================] - 1s 528us/step - loss: 0.3025 - accuracy: 0.8548 - auc: 0.9410 - val_loss: 0.3325 - val_accuracy: 0.8403 - val_auc: 0.9302\n",
      "Epoch 26/50\n",
      "1080/1080 [==============================] - 1s 528us/step - loss: 0.3009 - accuracy: 0.8550 - auc: 0.9418 - val_loss: 0.3348 - val_accuracy: 0.8412 - val_auc: 0.9297\n",
      "Epoch 27/50\n",
      "1080/1080 [==============================] - 1s 523us/step - loss: 0.3009 - accuracy: 0.8550 - auc: 0.9417 - val_loss: 0.3354 - val_accuracy: 0.8400 - val_auc: 0.9301\n",
      "Epoch 28/50\n",
      "1080/1080 [==============================] - 1s 520us/step - loss: 0.3008 - accuracy: 0.8562 - auc: 0.9417 - val_loss: 0.3348 - val_accuracy: 0.8419 - val_auc: 0.9310\n",
      "Epoch 29/50\n",
      "1080/1080 [==============================] - 1s 522us/step - loss: 0.2998 - accuracy: 0.8571 - auc: 0.9422 - val_loss: 0.3339 - val_accuracy: 0.8409 - val_auc: 0.9304\n",
      "Epoch 30/50\n",
      "1080/1080 [==============================] - 1s 546us/step - loss: 0.2992 - accuracy: 0.8558 - auc: 0.9424 - val_loss: 0.3509 - val_accuracy: 0.8293 - val_auc: 0.9293\n",
      "Epoch 31/50\n",
      "1080/1080 [==============================] - 1s 548us/step - loss: 0.2982 - accuracy: 0.8564 - auc: 0.9427 - val_loss: 0.3352 - val_accuracy: 0.8424 - val_auc: 0.9304\n",
      "Epoch 32/50\n",
      "1080/1080 [==============================] - 1s 519us/step - loss: 0.2982 - accuracy: 0.8567 - auc: 0.9428 - val_loss: 0.3341 - val_accuracy: 0.8425 - val_auc: 0.9306\n",
      "Epoch 33/50\n",
      "1080/1080 [==============================] - 1s 514us/step - loss: 0.2973 - accuracy: 0.8575 - auc: 0.9432 - val_loss: 0.3398 - val_accuracy: 0.8397 - val_auc: 0.9310\n",
      "Epoch 34/50\n",
      "1080/1080 [==============================] - 1s 517us/step - loss: 0.2975 - accuracy: 0.8565 - auc: 0.9430 - val_loss: 0.3335 - val_accuracy: 0.8434 - val_auc: 0.9311\n",
      "Epoch 35/50\n",
      "1080/1080 [==============================] - 1s 544us/step - loss: 0.2959 - accuracy: 0.8585 - auc: 0.9438 - val_loss: 0.3340 - val_accuracy: 0.8425 - val_auc: 0.9313\n",
      "Epoch 36/50\n",
      "1080/1080 [==============================] - 1s 518us/step - loss: 0.2955 - accuracy: 0.8588 - auc: 0.9438 - val_loss: 0.3396 - val_accuracy: 0.8387 - val_auc: 0.9305\n",
      "Epoch 37/50\n",
      "1080/1080 [==============================] - 1s 517us/step - loss: 0.2961 - accuracy: 0.8589 - auc: 0.9436 - val_loss: 0.3360 - val_accuracy: 0.8397 - val_auc: 0.9311\n",
      "Epoch 38/50\n",
      "1080/1080 [==============================] - 1s 518us/step - loss: 0.2949 - accuracy: 0.8588 - auc: 0.9441 - val_loss: 0.3345 - val_accuracy: 0.8418 - val_auc: 0.9310\n",
      "Epoch 39/50\n",
      "1080/1080 [==============================] - 1s 519us/step - loss: 0.2947 - accuracy: 0.8577 - auc: 0.9442 - val_loss: 0.3397 - val_accuracy: 0.8395 - val_auc: 0.9307\n",
      "Epoch 40/50\n",
      "1080/1080 [==============================] - 1s 522us/step - loss: 0.2944 - accuracy: 0.8591 - auc: 0.9444 - val_loss: 0.3348 - val_accuracy: 0.8447 - val_auc: 0.9314\n",
      "Epoch 41/50\n",
      "1080/1080 [==============================] - 1s 520us/step - loss: 0.2939 - accuracy: 0.8594 - auc: 0.9446 - val_loss: 0.3432 - val_accuracy: 0.8384 - val_auc: 0.9308\n",
      "Epoch 42/50\n",
      "1080/1080 [==============================] - 1s 548us/step - loss: 0.2938 - accuracy: 0.8594 - auc: 0.9445 - val_loss: 0.3353 - val_accuracy: 0.8428 - val_auc: 0.9311\n",
      "Epoch 43/50\n",
      "1080/1080 [==============================] - 1s 527us/step - loss: 0.2934 - accuracy: 0.8594 - auc: 0.9446 - val_loss: 0.3383 - val_accuracy: 0.8394 - val_auc: 0.9299\n",
      "Epoch 44/50\n",
      "1080/1080 [==============================] - 1s 525us/step - loss: 0.2933 - accuracy: 0.8596 - auc: 0.9447 - val_loss: 0.3388 - val_accuracy: 0.8399 - val_auc: 0.9308\n",
      "Epoch 45/50\n",
      "1080/1080 [==============================] - 1s 525us/step - loss: 0.2928 - accuracy: 0.8595 - auc: 0.9450 - val_loss: 0.3377 - val_accuracy: 0.8415 - val_auc: 0.9298\n",
      "Epoch 46/50\n",
      "1080/1080 [==============================] - 1s 529us/step - loss: 0.2929 - accuracy: 0.8599 - auc: 0.9450 - val_loss: 0.3386 - val_accuracy: 0.8439 - val_auc: 0.9303\n",
      "Epoch 47/50\n",
      "1080/1080 [==============================] - 1s 528us/step - loss: 0.2922 - accuracy: 0.8613 - auc: 0.9452 - val_loss: 0.3375 - val_accuracy: 0.8421 - val_auc: 0.9302\n",
      "Epoch 48/50\n",
      "1080/1080 [==============================] - 1s 526us/step - loss: 0.2919 - accuracy: 0.8617 - auc: 0.9455 - val_loss: 0.3444 - val_accuracy: 0.8359 - val_auc: 0.9306\n",
      "Epoch 49/50\n",
      "1080/1080 [==============================] - 1s 527us/step - loss: 0.2914 - accuracy: 0.8614 - auc: 0.9455 - val_loss: 0.3368 - val_accuracy: 0.8396 - val_auc: 0.9299\n",
      "Epoch 50/50\n",
      "1080/1080 [==============================] - 1s 556us/step - loss: 0.2919 - accuracy: 0.8603 - auc: 0.9453 - val_loss: 0.3371 - val_accuracy: 0.8409 - val_auc: 0.9307\n",
      "579/579 [==============================] - 0s 354us/step - loss: 0.3400 - accuracy: 0.8431 - auc: 0.9303\n",
      "Test accuracy: 0.8430694341659546\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "\n",
    "cant_epochs=50\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=cant_epochs, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test dataß\n",
    "test_loss, test_accuracy, test_auc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de Confusion y otras métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 257us/step\n",
      "579/579 [==============================] - 0s 256us/step\n",
      "Matriz de confusión de los datos de prueba\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(50.722222222222214, 0.5, 'True')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "binary_predictions_test = (y_test_pred > 0.5).astype(int)\n",
    "binary_predictions_train = (y_train_pred > 0.5).astype(int)\n",
    "\n",
    "print(\"Matriz de confusión de los datos de prueba\")\n",
    "cm = confusion_matrix(y_test, binary_predictions_test)\n",
    "sns.heatmap(cm, cmap='Blues',annot=True,fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score sobre el set de entrenamiento: 0.861\n",
      "F1-Score sobre el set de prueba: 0.844\n"
     ]
    }
   ],
   "source": [
    "train_score = f1_score(y_train, binary_predictions_train)\n",
    "test_score = f1_score(y_test, binary_predictions_test)\n",
    "\n",
    "print(\"F1-Score sobre el set de entrenamiento:\", round(train_score, 3))\n",
    "print(\"F1-Score sobre el set de prueba:\", round(test_score, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], color='black', label='Loss')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['auc'], color='orange', label='AUC')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejoro Modelo mediante un Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Híperparametros:\n",
    "- Epochs\n",
    "- Activation\n",
    "- Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sklearn model for the network\n",
    "\n",
    "def create_model():\n",
    "\n",
    "    # Create a simple feedforward neural network\n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Dense(8, activation='relu', input_shape=(498,), kernel_regularizer=l2(0.01)),  # Input layer\n",
    "    keras.layers.Dense(8, activation='relu', kernel_regularizer=l2(0.01)), \n",
    "    keras.layers.Dense(1, activation='sigmoid')  # Output layer \n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "      optimizer=keras.optimizers.SGD(learning_rate=0.001),\n",
    "      loss='binary_crossentropy',\n",
    "      # metricas para ir calculando en cada iteracion o batch\n",
    "      metrics=['AUC'],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_cv = KerasClassifier(build_fn=create_model)\n",
    "\n",
    "params_grid = {\n",
    "    'epochs': [100, 150, 200],\n",
    "    #'neurons_per_layer': [3, 6, 9],\n",
    "    #'activation': ['relu', 'sigmoid'],\n",
    "    'batch_size': [5, 10, 15],\n",
    "    'optimizer': ['nadam', 'adamax']\n",
    "}\n",
    "\n",
    "scorer_fn = make_scorer(f1_score)\n",
    "kfoldcv = StratifiedKFold(n_splits=2)\n",
    "\n",
    "gridcv = GridSearchCV(estimator=modelo_cv,\n",
    "                      param_grid = params_grid,\n",
    "                      scoring=scorer_fn,\n",
    "                      cv=kfoldcv,\n",
    "                      )\n",
    "\n",
    "model = gridcv.fit(x_train,y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "score = f1_score(y_test, y_pred)\n",
    "print(\"Parámetros:\", gridcv.best_params_, \"\\nF1 score: \", round(score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de intentar agregar el hiperparamtro 'batch_size' al param_grid, se noto un costo computacional exponencial y no se logro mejorar el F1-Score del modelo. No se justifica usarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/franciscolabollita/Library/Python/3.8/lib/python/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/175\n",
      "675/675 [==============================] - 1s 735us/step - loss: 0.6988 - auc: 0.4833\n",
      "Epoch 2/175\n",
      "675/675 [==============================] - 0s 444us/step - loss: 0.6847 - auc: 0.6416\n",
      "Epoch 3/175\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.6795 - auc: 0.6880\n",
      "Epoch 4/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.6750 - auc: 0.7112\n",
      "Epoch 5/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.6699 - auc: 0.7292\n",
      "Epoch 6/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.6639 - auc: 0.7461\n",
      "Epoch 7/175\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.6570 - auc: 0.7608\n",
      "Epoch 8/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.6492 - auc: 0.7729\n",
      "Epoch 9/175\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.6405 - auc: 0.7828\n",
      "Epoch 10/175\n",
      "675/675 [==============================] - 0s 429us/step - loss: 0.6314 - auc: 0.7906\n",
      "Epoch 11/175\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.6220 - auc: 0.7967\n",
      "Epoch 12/175\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.6126 - auc: 0.8022\n",
      "Epoch 13/175\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.6031 - auc: 0.8069\n",
      "Epoch 14/175\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.5937 - auc: 0.8110\n",
      "Epoch 15/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.5844 - auc: 0.8151\n",
      "Epoch 16/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.5753 - auc: 0.8189\n",
      "Epoch 17/175\n",
      "675/675 [==============================] - 0s 389us/step - loss: 0.5664 - auc: 0.8223\n",
      "Epoch 18/175\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.5578 - auc: 0.8258\n",
      "Epoch 19/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.5496 - auc: 0.8292\n",
      "Epoch 20/175\n",
      "675/675 [==============================] - 0s 389us/step - loss: 0.5416 - auc: 0.8329\n",
      "Epoch 21/175\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.5339 - auc: 0.8363\n",
      "Epoch 22/175\n",
      "675/675 [==============================] - 0s 397us/step - loss: 0.5265 - auc: 0.8395\n",
      "Epoch 23/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.5195 - auc: 0.8427\n",
      "Epoch 24/175\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.5128 - auc: 0.8455\n",
      "Epoch 25/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.5064 - auc: 0.8483\n",
      "Epoch 26/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.5003 - auc: 0.8510\n",
      "Epoch 27/175\n",
      "675/675 [==============================] - 0s 389us/step - loss: 0.4946 - auc: 0.8535\n",
      "Epoch 28/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.4891 - auc: 0.8557\n",
      "Epoch 29/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4840 - auc: 0.8578\n",
      "Epoch 30/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.4791 - auc: 0.8598\n",
      "Epoch 31/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.4745 - auc: 0.8616\n",
      "Epoch 32/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.4701 - auc: 0.8635\n",
      "Epoch 33/175\n",
      "675/675 [==============================] - 0s 422us/step - loss: 0.4659 - auc: 0.8651\n",
      "Epoch 34/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.4620 - auc: 0.8667\n",
      "Epoch 35/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.4582 - auc: 0.8683\n",
      "Epoch 36/175\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.4547 - auc: 0.8697\n",
      "Epoch 37/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4514 - auc: 0.8712\n",
      "Epoch 38/175\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.4482 - auc: 0.8725\n",
      "Epoch 39/175\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.4452 - auc: 0.8738\n",
      "Epoch 40/175\n",
      "675/675 [==============================] - 0s 394us/step - loss: 0.4423 - auc: 0.8750\n",
      "Epoch 41/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.4395 - auc: 0.8762\n",
      "Epoch 42/175\n",
      "675/675 [==============================] - 0s 396us/step - loss: 0.4370 - auc: 0.8772\n",
      "Epoch 43/175\n",
      "675/675 [==============================] - 0s 389us/step - loss: 0.4345 - auc: 0.8783\n",
      "Epoch 44/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.4322 - auc: 0.8794\n",
      "Epoch 45/175\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.4300 - auc: 0.8803\n",
      "Epoch 46/175\n",
      "675/675 [==============================] - 0s 412us/step - loss: 0.4279 - auc: 0.8812\n",
      "Epoch 47/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.4259 - auc: 0.8822\n",
      "Epoch 48/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.4240 - auc: 0.8830\n",
      "Epoch 49/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.4222 - auc: 0.8839\n",
      "Epoch 50/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.4205 - auc: 0.8846\n",
      "Epoch 51/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.4189 - auc: 0.8854\n",
      "Epoch 52/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.4173 - auc: 0.8861\n",
      "Epoch 53/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.4157 - auc: 0.8869\n",
      "Epoch 54/175\n",
      "675/675 [==============================] - 0s 420us/step - loss: 0.4143 - auc: 0.8876\n",
      "Epoch 55/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4129 - auc: 0.8882\n",
      "Epoch 56/175\n",
      "675/675 [==============================] - 0s 392us/step - loss: 0.4116 - auc: 0.8887\n",
      "Epoch 57/175\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.4104 - auc: 0.8894\n",
      "Epoch 58/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.4091 - auc: 0.8901\n",
      "Epoch 59/175\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.4079 - auc: 0.8905\n",
      "Epoch 60/175\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.4068 - auc: 0.8912\n",
      "Epoch 61/175\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.4057 - auc: 0.8917\n",
      "Epoch 62/175\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.4046 - auc: 0.8923\n",
      "Epoch 63/175\n",
      "675/675 [==============================] - 0s 396us/step - loss: 0.4037 - auc: 0.8927\n",
      "Epoch 64/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.4027 - auc: 0.8932\n",
      "Epoch 65/175\n",
      "675/675 [==============================] - 0s 381us/step - loss: 0.4017 - auc: 0.8938\n",
      "Epoch 66/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.4007 - auc: 0.8943\n",
      "Epoch 67/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.4000 - auc: 0.8947\n",
      "Epoch 68/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.3991 - auc: 0.8951\n",
      "Epoch 69/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.3981 - auc: 0.8956\n",
      "Epoch 70/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.3975 - auc: 0.8960\n",
      "Epoch 71/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.3967 - auc: 0.8964\n",
      "Epoch 72/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3959 - auc: 0.8968\n",
      "Epoch 73/175\n",
      "675/675 [==============================] - 0s 421us/step - loss: 0.3952 - auc: 0.8971\n",
      "Epoch 74/175\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.3945 - auc: 0.8975\n",
      "Epoch 75/175\n",
      "675/675 [==============================] - 0s 402us/step - loss: 0.3938 - auc: 0.8979\n",
      "Epoch 76/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.3931 - auc: 0.8983\n",
      "Epoch 77/175\n",
      "675/675 [==============================] - 0s 422us/step - loss: 0.3925 - auc: 0.8986\n",
      "Epoch 78/175\n",
      "675/675 [==============================] - 0s 404us/step - loss: 0.3919 - auc: 0.8989\n",
      "Epoch 79/175\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.3912 - auc: 0.8993\n",
      "Epoch 80/175\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.3908 - auc: 0.8995\n",
      "Epoch 81/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.3901 - auc: 0.8998\n",
      "Epoch 82/175\n",
      "675/675 [==============================] - 0s 389us/step - loss: 0.3895 - auc: 0.9001\n",
      "Epoch 83/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3890 - auc: 0.9005\n",
      "Epoch 84/175\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.3885 - auc: 0.9007\n",
      "Epoch 85/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.3879 - auc: 0.9010\n",
      "Epoch 86/175\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.3874 - auc: 0.9013\n",
      "Epoch 87/175\n",
      "675/675 [==============================] - 0s 395us/step - loss: 0.3870 - auc: 0.9015\n",
      "Epoch 88/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3864 - auc: 0.9018\n",
      "Epoch 89/175\n",
      "675/675 [==============================] - 0s 421us/step - loss: 0.3860 - auc: 0.9020\n",
      "Epoch 90/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3855 - auc: 0.9024\n",
      "Epoch 91/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.3850 - auc: 0.9026\n",
      "Epoch 92/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3846 - auc: 0.9028\n",
      "Epoch 93/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.3842 - auc: 0.9030\n",
      "Epoch 94/175\n",
      "675/675 [==============================] - 0s 381us/step - loss: 0.3836 - auc: 0.9033\n",
      "Epoch 95/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3832 - auc: 0.9035\n",
      "Epoch 96/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3828 - auc: 0.9037\n",
      "Epoch 97/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3825 - auc: 0.9039\n",
      "Epoch 98/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.3820 - auc: 0.9041\n",
      "Epoch 99/175\n",
      "675/675 [==============================] - 0s 381us/step - loss: 0.3816 - auc: 0.9044\n",
      "Epoch 100/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3812 - auc: 0.9045\n",
      "Epoch 101/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.3809 - auc: 0.9048\n",
      "Epoch 102/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3804 - auc: 0.9050\n",
      "Epoch 103/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3801 - auc: 0.9052\n",
      "Epoch 104/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3798 - auc: 0.9053\n",
      "Epoch 105/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3794 - auc: 0.9055\n",
      "Epoch 106/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3791 - auc: 0.9057\n",
      "Epoch 107/175\n",
      "675/675 [==============================] - 0s 422us/step - loss: 0.3787 - auc: 0.9059\n",
      "Epoch 108/175\n",
      "675/675 [==============================] - 0s 424us/step - loss: 0.3784 - auc: 0.9061\n",
      "Epoch 109/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3780 - auc: 0.9063\n",
      "Epoch 110/175\n",
      "675/675 [==============================] - 0s 377us/step - loss: 0.3777 - auc: 0.9065\n",
      "Epoch 111/175\n",
      "675/675 [==============================] - 0s 372us/step - loss: 0.3773 - auc: 0.9067\n",
      "Epoch 112/175\n",
      "675/675 [==============================] - 0s 373us/step - loss: 0.3770 - auc: 0.9068\n",
      "Epoch 113/175\n",
      "675/675 [==============================] - 0s 381us/step - loss: 0.3767 - auc: 0.9070\n",
      "Epoch 114/175\n",
      "675/675 [==============================] - 0s 441us/step - loss: 0.3764 - auc: 0.9072\n",
      "Epoch 115/175\n",
      "675/675 [==============================] - 0s 377us/step - loss: 0.3760 - auc: 0.9074\n",
      "Epoch 116/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.3758 - auc: 0.9076\n",
      "Epoch 117/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3755 - auc: 0.9077\n",
      "Epoch 118/175\n",
      "675/675 [==============================] - 0s 379us/step - loss: 0.3751 - auc: 0.9079\n",
      "Epoch 119/175\n",
      "675/675 [==============================] - 0s 378us/step - loss: 0.3748 - auc: 0.9081\n",
      "Epoch 120/175\n",
      "675/675 [==============================] - 0s 375us/step - loss: 0.3744 - auc: 0.9083\n",
      "Epoch 121/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.3743 - auc: 0.9084\n",
      "Epoch 122/175\n",
      "675/675 [==============================] - 0s 410us/step - loss: 0.3739 - auc: 0.9085\n",
      "Epoch 123/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.3736 - auc: 0.9087\n",
      "Epoch 124/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.3733 - auc: 0.9089\n",
      "Epoch 125/175\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.3731 - auc: 0.9090\n",
      "Epoch 126/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.3727 - auc: 0.9092\n",
      "Epoch 127/175\n",
      "675/675 [==============================] - 0s 375us/step - loss: 0.3725 - auc: 0.9095\n",
      "Epoch 128/175\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.3723 - auc: 0.9094\n",
      "Epoch 129/175\n",
      "675/675 [==============================] - 0s 405us/step - loss: 0.3719 - auc: 0.9097\n",
      "Epoch 130/175\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.3717 - auc: 0.9098\n",
      "Epoch 131/175\n",
      "675/675 [==============================] - 0s 381us/step - loss: 0.3715 - auc: 0.9099\n",
      "Epoch 132/175\n",
      "675/675 [==============================] - 0s 373us/step - loss: 0.3712 - auc: 0.9101\n",
      "Epoch 133/175\n",
      "675/675 [==============================] - 0s 377us/step - loss: 0.3710 - auc: 0.9102\n",
      "Epoch 134/175\n",
      "675/675 [==============================] - 0s 381us/step - loss: 0.3707 - auc: 0.9104\n",
      "Epoch 135/175\n",
      "675/675 [==============================] - 0s 377us/step - loss: 0.3704 - auc: 0.9105\n",
      "Epoch 136/175\n",
      "675/675 [==============================] - 0s 414us/step - loss: 0.3701 - auc: 0.9107\n",
      "Epoch 137/175\n",
      "675/675 [==============================] - 0s 373us/step - loss: 0.3699 - auc: 0.9107\n",
      "Epoch 138/175\n",
      "675/675 [==============================] - 0s 380us/step - loss: 0.3697 - auc: 0.9109\n",
      "Epoch 139/175\n",
      "675/675 [==============================] - 0s 377us/step - loss: 0.3694 - auc: 0.9111\n",
      "Epoch 140/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.3692 - auc: 0.9112\n",
      "Epoch 141/175\n",
      "675/675 [==============================] - 0s 379us/step - loss: 0.3688 - auc: 0.9114\n",
      "Epoch 142/175\n",
      "675/675 [==============================] - 0s 375us/step - loss: 0.3686 - auc: 0.9115\n",
      "Epoch 143/175\n",
      "675/675 [==============================] - 0s 376us/step - loss: 0.3684 - auc: 0.9115\n",
      "Epoch 144/175\n",
      "675/675 [==============================] - 0s 371us/step - loss: 0.3682 - auc: 0.9117\n",
      "Epoch 145/175\n",
      "675/675 [==============================] - 0s 373us/step - loss: 0.3680 - auc: 0.9118\n",
      "Epoch 146/175\n",
      "675/675 [==============================] - 0s 380us/step - loss: 0.3677 - auc: 0.9120\n",
      "Epoch 147/175\n",
      "675/675 [==============================] - 0s 377us/step - loss: 0.3673 - auc: 0.9122\n",
      "Epoch 148/175\n",
      "675/675 [==============================] - 0s 376us/step - loss: 0.3672 - auc: 0.9123\n",
      "Epoch 149/175\n",
      "675/675 [==============================] - 0s 376us/step - loss: 0.3671 - auc: 0.9123\n",
      "Epoch 150/175\n",
      "675/675 [==============================] - 0s 375us/step - loss: 0.3667 - auc: 0.9125\n",
      "Epoch 151/175\n",
      "675/675 [==============================] - 0s 419us/step - loss: 0.3666 - auc: 0.9126\n",
      "Epoch 152/175\n",
      "675/675 [==============================] - 0s 372us/step - loss: 0.3664 - auc: 0.9128\n",
      "Epoch 153/175\n",
      "675/675 [==============================] - 0s 375us/step - loss: 0.3661 - auc: 0.9129\n",
      "Epoch 154/175\n",
      "675/675 [==============================] - 0s 372us/step - loss: 0.3658 - auc: 0.9130\n",
      "Epoch 155/175\n",
      "675/675 [==============================] - 0s 373us/step - loss: 0.3655 - auc: 0.9132\n",
      "Epoch 156/175\n",
      "675/675 [==============================] - 0s 373us/step - loss: 0.3654 - auc: 0.9133\n",
      "Epoch 157/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.3651 - auc: 0.9134\n",
      "Epoch 158/175\n",
      "675/675 [==============================] - 0s 376us/step - loss: 0.3648 - auc: 0.9136\n",
      "Epoch 159/175\n",
      "675/675 [==============================] - 0s 377us/step - loss: 0.3646 - auc: 0.9137\n",
      "Epoch 160/175\n",
      "675/675 [==============================] - 0s 376us/step - loss: 0.3645 - auc: 0.9137\n",
      "Epoch 161/175\n",
      "675/675 [==============================] - 0s 377us/step - loss: 0.3642 - auc: 0.9140\n",
      "Epoch 162/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.3641 - auc: 0.9140\n",
      "Epoch 163/175\n",
      "675/675 [==============================] - 0s 414us/step - loss: 0.3638 - auc: 0.9142\n",
      "Epoch 164/175\n",
      "675/675 [==============================] - 0s 373us/step - loss: 0.3636 - auc: 0.9142\n",
      "Epoch 165/175\n",
      "675/675 [==============================] - 0s 376us/step - loss: 0.3634 - auc: 0.9144\n",
      "Epoch 166/175\n",
      "675/675 [==============================] - 0s 375us/step - loss: 0.3632 - auc: 0.9145\n",
      "Epoch 167/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.3630 - auc: 0.9146\n",
      "Epoch 168/175\n",
      "675/675 [==============================] - 0s 375us/step - loss: 0.3627 - auc: 0.9148\n",
      "Epoch 169/175\n",
      "675/675 [==============================] - 0s 372us/step - loss: 0.3625 - auc: 0.9148\n",
      "Epoch 170/175\n",
      "675/675 [==============================] - 0s 372us/step - loss: 0.3623 - auc: 0.9150\n",
      "Epoch 171/175\n",
      "675/675 [==============================] - 0s 372us/step - loss: 0.3621 - auc: 0.9151\n",
      "Epoch 172/175\n",
      "675/675 [==============================] - 0s 378us/step - loss: 0.3619 - auc: 0.9152\n",
      "Epoch 173/175\n",
      "675/675 [==============================] - 0s 372us/step - loss: 0.3616 - auc: 0.9153\n",
      "Epoch 174/175\n",
      "675/675 [==============================] - 0s 376us/step - loss: 0.3615 - auc: 0.9154\n",
      "Epoch 175/175\n",
      "675/675 [==============================] - 0s 372us/step - loss: 0.3612 - auc: 0.9157\n",
      "675/675 [==============================] - 0s 261us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/franciscolabollita/Library/Python/3.8/lib/python/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/175\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.6925 - auc: 0.5301\n",
      "Epoch 2/175\n",
      "675/675 [==============================] - 0s 427us/step - loss: 0.6865 - auc: 0.6234\n",
      "Epoch 3/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.6758 - auc: 0.6971\n",
      "Epoch 4/175\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.6635 - auc: 0.7378\n",
      "Epoch 5/175\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.6523 - auc: 0.7570\n",
      "Epoch 6/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.6410 - auc: 0.7677\n",
      "Epoch 7/175\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.6292 - auc: 0.7765\n",
      "Epoch 8/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.6168 - auc: 0.7845\n",
      "Epoch 9/175\n",
      "675/675 [==============================] - 0s 454us/step - loss: 0.6041 - auc: 0.7917\n",
      "Epoch 10/175\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.5915 - auc: 0.7993\n",
      "Epoch 11/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.5794 - auc: 0.8054\n",
      "Epoch 12/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.5680 - auc: 0.8112\n",
      "Epoch 13/175\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.5574 - auc: 0.8167\n",
      "Epoch 14/175\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.5476 - auc: 0.8210\n",
      "Epoch 15/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.5386 - auc: 0.8252\n",
      "Epoch 16/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.5304 - auc: 0.8286\n",
      "Epoch 17/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.5229 - auc: 0.8319\n",
      "Epoch 18/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.5161 - auc: 0.8348\n",
      "Epoch 19/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.5098 - auc: 0.8375\n",
      "Epoch 20/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.5041 - auc: 0.8397\n",
      "Epoch 21/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4989 - auc: 0.8419\n",
      "Epoch 22/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.4941 - auc: 0.8438\n",
      "Epoch 23/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.4897 - auc: 0.8460\n",
      "Epoch 24/175\n",
      "675/675 [==============================] - 0s 416us/step - loss: 0.4857 - auc: 0.8478\n",
      "Epoch 25/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4819 - auc: 0.8495\n",
      "Epoch 26/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.4783 - auc: 0.8513\n",
      "Epoch 27/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.4749 - auc: 0.8530\n",
      "Epoch 28/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.4718 - auc: 0.8546\n",
      "Epoch 29/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.4687 - auc: 0.8563\n",
      "Epoch 30/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.4658 - auc: 0.8581\n",
      "Epoch 31/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.4631 - auc: 0.8595\n",
      "Epoch 32/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.4605 - auc: 0.8609\n",
      "Epoch 33/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.4580 - auc: 0.8624\n",
      "Epoch 34/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.4555 - auc: 0.8637\n",
      "Epoch 35/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4532 - auc: 0.8650\n",
      "Epoch 36/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4509 - auc: 0.8664\n",
      "Epoch 37/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4487 - auc: 0.8678\n",
      "Epoch 38/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.4466 - auc: 0.8689\n",
      "Epoch 39/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.4445 - auc: 0.8700\n",
      "Epoch 40/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.4425 - auc: 0.8711\n",
      "Epoch 41/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.4406 - auc: 0.8723\n",
      "Epoch 42/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.4386 - auc: 0.8734\n",
      "Epoch 43/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4367 - auc: 0.8743\n",
      "Epoch 44/175\n",
      "675/675 [==============================] - 0s 389us/step - loss: 0.4351 - auc: 0.8752\n",
      "Epoch 45/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.4334 - auc: 0.8760\n",
      "Epoch 46/175\n",
      "675/675 [==============================] - 0s 426us/step - loss: 0.4318 - auc: 0.8769\n",
      "Epoch 47/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.4302 - auc: 0.8778\n",
      "Epoch 48/175\n",
      "675/675 [==============================] - 0s 439us/step - loss: 0.4287 - auc: 0.8786\n",
      "Epoch 49/175\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.4271 - auc: 0.8794\n",
      "Epoch 50/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4258 - auc: 0.8801\n",
      "Epoch 51/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.4244 - auc: 0.8808\n",
      "Epoch 52/175\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.4231 - auc: 0.8815\n",
      "Epoch 53/175\n",
      "675/675 [==============================] - 0s 469us/step - loss: 0.4218 - auc: 0.8822\n",
      "Epoch 54/175\n",
      "675/675 [==============================] - 0s 401us/step - loss: 0.4206 - auc: 0.8828\n",
      "Epoch 55/175\n",
      "675/675 [==============================] - 0s 389us/step - loss: 0.4194 - auc: 0.8834\n",
      "Epoch 56/175\n",
      "675/675 [==============================] - 0s 416us/step - loss: 0.4182 - auc: 0.8841\n",
      "Epoch 57/175\n",
      "675/675 [==============================] - 0s 403us/step - loss: 0.4170 - auc: 0.8848\n",
      "Epoch 58/175\n",
      "675/675 [==============================] - 0s 435us/step - loss: 0.4159 - auc: 0.8853\n",
      "Epoch 59/175\n",
      "675/675 [==============================] - 0s 428us/step - loss: 0.4149 - auc: 0.8859\n",
      "Epoch 60/175\n",
      "675/675 [==============================] - 0s 425us/step - loss: 0.4139 - auc: 0.8864\n",
      "Epoch 61/175\n",
      "675/675 [==============================] - 0s 431us/step - loss: 0.4129 - auc: 0.8869\n",
      "Epoch 62/175\n",
      "675/675 [==============================] - 0s 479us/step - loss: 0.4119 - auc: 0.8875\n",
      "Epoch 63/175\n",
      "675/675 [==============================] - 0s 421us/step - loss: 0.4109 - auc: 0.8880\n",
      "Epoch 64/175\n",
      "675/675 [==============================] - 0s 419us/step - loss: 0.4100 - auc: 0.8886\n",
      "Epoch 65/175\n",
      "675/675 [==============================] - 0s 456us/step - loss: 0.4092 - auc: 0.8890\n",
      "Epoch 66/175\n",
      "675/675 [==============================] - 0s 431us/step - loss: 0.4082 - auc: 0.8896\n",
      "Epoch 67/175\n",
      "675/675 [==============================] - 0s 400us/step - loss: 0.4074 - auc: 0.8901\n",
      "Epoch 68/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4066 - auc: 0.8905\n",
      "Epoch 69/175\n",
      "675/675 [==============================] - 0s 407us/step - loss: 0.4057 - auc: 0.8910\n",
      "Epoch 70/175\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.4049 - auc: 0.8915\n",
      "Epoch 71/175\n",
      "675/675 [==============================] - 0s 389us/step - loss: 0.4042 - auc: 0.8919\n",
      "Epoch 72/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.4034 - auc: 0.8924\n",
      "Epoch 73/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.4027 - auc: 0.8927\n",
      "Epoch 74/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.4020 - auc: 0.8932\n",
      "Epoch 75/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.4012 - auc: 0.8936\n",
      "Epoch 76/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.4006 - auc: 0.8939\n",
      "Epoch 77/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.3999 - auc: 0.8944\n",
      "Epoch 78/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.3992 - auc: 0.8947\n",
      "Epoch 79/175\n",
      "675/675 [==============================] - 0s 417us/step - loss: 0.3986 - auc: 0.8951\n",
      "Epoch 80/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3980 - auc: 0.8955\n",
      "Epoch 81/175\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.3974 - auc: 0.8958\n",
      "Epoch 82/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3967 - auc: 0.8962\n",
      "Epoch 83/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.3962 - auc: 0.8965\n",
      "Epoch 84/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.3956 - auc: 0.8969\n",
      "Epoch 85/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.3951 - auc: 0.8971\n",
      "Epoch 86/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3945 - auc: 0.8974\n",
      "Epoch 87/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3940 - auc: 0.8978\n",
      "Epoch 88/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3933 - auc: 0.8982\n",
      "Epoch 89/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3929 - auc: 0.8984\n",
      "Epoch 90/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.3924 - auc: 0.8986\n",
      "Epoch 91/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3918 - auc: 0.8990\n",
      "Epoch 92/175\n",
      "675/675 [==============================] - 0s 381us/step - loss: 0.3914 - auc: 0.8993\n",
      "Epoch 93/175\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.3909 - auc: 0.8995\n",
      "Epoch 94/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.3905 - auc: 0.8998\n",
      "Epoch 95/175\n",
      "675/675 [==============================] - 0s 419us/step - loss: 0.3900 - auc: 0.9001\n",
      "Epoch 96/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3896 - auc: 0.9003\n",
      "Epoch 97/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.3889 - auc: 0.9007\n",
      "Epoch 98/175\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.3888 - auc: 0.9007\n",
      "Epoch 99/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3882 - auc: 0.9011\n",
      "Epoch 100/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3878 - auc: 0.9013\n",
      "Epoch 101/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3874 - auc: 0.9015\n",
      "Epoch 102/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3870 - auc: 0.9017\n",
      "Epoch 103/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3867 - auc: 0.9019\n",
      "Epoch 104/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.3861 - auc: 0.9022\n",
      "Epoch 105/175\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.3858 - auc: 0.9024\n",
      "Epoch 106/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.3855 - auc: 0.9026\n",
      "Epoch 107/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3851 - auc: 0.9029\n",
      "Epoch 108/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3847 - auc: 0.9030\n",
      "Epoch 109/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3843 - auc: 0.9032\n",
      "Epoch 110/175\n",
      "675/675 [==============================] - 0s 381us/step - loss: 0.3840 - auc: 0.9034\n",
      "Epoch 111/175\n",
      "675/675 [==============================] - 0s 389us/step - loss: 0.3836 - auc: 0.9037\n",
      "Epoch 112/175\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.3832 - auc: 0.9039\n",
      "Epoch 113/175\n",
      "675/675 [==============================] - 0s 465us/step - loss: 0.3829 - auc: 0.9040\n",
      "Epoch 114/175\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.3827 - auc: 0.9042\n",
      "Epoch 115/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.3823 - auc: 0.9044\n",
      "Epoch 116/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3820 - auc: 0.9045\n",
      "Epoch 117/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3816 - auc: 0.9048\n",
      "Epoch 118/175\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.3812 - auc: 0.9050\n",
      "Epoch 119/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3810 - auc: 0.9050\n",
      "Epoch 120/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3807 - auc: 0.9053\n",
      "Epoch 121/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3804 - auc: 0.9054\n",
      "Epoch 122/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3801 - auc: 0.9056\n",
      "Epoch 123/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3799 - auc: 0.9057\n",
      "Epoch 124/175\n",
      "675/675 [==============================] - 0s 392us/step - loss: 0.3795 - auc: 0.9059\n",
      "Epoch 125/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.3793 - auc: 0.9061\n",
      "Epoch 126/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3790 - auc: 0.9062\n",
      "Epoch 127/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.3785 - auc: 0.9065\n",
      "Epoch 128/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.3782 - auc: 0.9067\n",
      "Epoch 129/175\n",
      "675/675 [==============================] - 0s 405us/step - loss: 0.3782 - auc: 0.9067\n",
      "Epoch 130/175\n",
      "675/675 [==============================] - 0s 428us/step - loss: 0.3779 - auc: 0.9069\n",
      "Epoch 131/175\n",
      "675/675 [==============================] - 0s 394us/step - loss: 0.3776 - auc: 0.9071\n",
      "Epoch 132/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.3773 - auc: 0.9072\n",
      "Epoch 133/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.3770 - auc: 0.9073\n",
      "Epoch 134/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3767 - auc: 0.9075\n",
      "Epoch 135/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.3765 - auc: 0.9076\n",
      "Epoch 136/175\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.3762 - auc: 0.9077\n",
      "Epoch 137/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.3760 - auc: 0.9078\n",
      "Epoch 138/175\n",
      "675/675 [==============================] - 0s 381us/step - loss: 0.3757 - auc: 0.9080\n",
      "Epoch 139/175\n",
      "675/675 [==============================] - 0s 381us/step - loss: 0.3755 - auc: 0.9082\n",
      "Epoch 140/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3752 - auc: 0.9083\n",
      "Epoch 141/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.3749 - auc: 0.9084\n",
      "Epoch 142/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.3747 - auc: 0.9086\n",
      "Epoch 143/175\n",
      "675/675 [==============================] - 0s 418us/step - loss: 0.3745 - auc: 0.9087\n",
      "Epoch 144/175\n",
      "675/675 [==============================] - 0s 405us/step - loss: 0.3740 - auc: 0.9089\n",
      "Epoch 145/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3740 - auc: 0.9089\n",
      "Epoch 146/175\n",
      "675/675 [==============================] - 0s 402us/step - loss: 0.3737 - auc: 0.9091\n",
      "Epoch 147/175\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.3735 - auc: 0.9093\n",
      "Epoch 148/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3733 - auc: 0.9093\n",
      "Epoch 149/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3731 - auc: 0.9094\n",
      "Epoch 150/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3728 - auc: 0.9097\n",
      "Epoch 151/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3726 - auc: 0.9097\n",
      "Epoch 152/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3724 - auc: 0.9098\n",
      "Epoch 153/175\n",
      "675/675 [==============================] - 0s 394us/step - loss: 0.3721 - auc: 0.9099\n",
      "Epoch 154/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3719 - auc: 0.9100\n",
      "Epoch 155/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3717 - auc: 0.9102\n",
      "Epoch 156/175\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.3715 - auc: 0.9104\n",
      "Epoch 157/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.3713 - auc: 0.9105\n",
      "Epoch 158/175\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.3710 - auc: 0.9107\n",
      "Epoch 159/175\n",
      "675/675 [==============================] - 0s 417us/step - loss: 0.3708 - auc: 0.9107\n",
      "Epoch 160/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.3706 - auc: 0.9108\n",
      "Epoch 161/175\n",
      "675/675 [==============================] - 0s 380us/step - loss: 0.3703 - auc: 0.9110\n",
      "Epoch 162/175\n",
      "675/675 [==============================] - 0s 381us/step - loss: 0.3701 - auc: 0.9111\n",
      "Epoch 163/175\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.3700 - auc: 0.9111\n",
      "Epoch 164/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.3697 - auc: 0.9113\n",
      "Epoch 165/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3697 - auc: 0.9113\n",
      "Epoch 166/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.3694 - auc: 0.9115\n",
      "Epoch 167/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3692 - auc: 0.9116\n",
      "Epoch 168/175\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.3690 - auc: 0.9117\n",
      "Epoch 169/175\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.3688 - auc: 0.9118\n",
      "Epoch 170/175\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.3686 - auc: 0.9120\n",
      "Epoch 171/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3684 - auc: 0.9120\n",
      "Epoch 172/175\n",
      "675/675 [==============================] - 0s 417us/step - loss: 0.3682 - auc: 0.9121\n",
      "Epoch 173/175\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.3679 - auc: 0.9123\n",
      "Epoch 174/175\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.3677 - auc: 0.9125\n",
      "Epoch 175/175\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.3675 - auc: 0.9125\n",
      "675/675 [==============================] - 0s 260us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/franciscolabollita/Library/Python/3.8/lib/python/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "675/675 [==============================] - 0s 402us/step - loss: 0.6931 - auc: 0.5109\n",
      "Epoch 2/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.6897 - auc: 0.5884\n",
      "Epoch 3/200\n",
      "675/675 [==============================] - 0s 397us/step - loss: 0.6866 - auc: 0.6421\n",
      "Epoch 4/200\n",
      "675/675 [==============================] - 0s 392us/step - loss: 0.6833 - auc: 0.6788\n",
      "Epoch 5/200\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.6793 - auc: 0.7053\n",
      "Epoch 6/200\n",
      "675/675 [==============================] - 0s 389us/step - loss: 0.6744 - auc: 0.7270\n",
      "Epoch 7/200\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.6686 - auc: 0.7471\n",
      "Epoch 8/200\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.6617 - auc: 0.7643\n",
      "Epoch 9/200\n",
      "675/675 [==============================] - 0s 389us/step - loss: 0.6538 - auc: 0.7797\n",
      "Epoch 10/200\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.6449 - auc: 0.7924\n",
      "Epoch 11/200\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.6352 - auc: 0.8021\n",
      "Epoch 12/200\n",
      "675/675 [==============================] - 0s 389us/step - loss: 0.6247 - auc: 0.8098\n",
      "Epoch 13/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.6134 - auc: 0.8160\n",
      "Epoch 14/200\n",
      "675/675 [==============================] - 0s 436us/step - loss: 0.6015 - auc: 0.8204\n",
      "Epoch 15/200\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.5891 - auc: 0.8242\n",
      "Epoch 16/200\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.5766 - auc: 0.8279\n",
      "Epoch 17/200\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.5642 - auc: 0.8307\n",
      "Epoch 18/200\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.5522 - auc: 0.8338\n",
      "Epoch 19/200\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.5410 - auc: 0.8364\n",
      "Epoch 20/200\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.5306 - auc: 0.8390\n",
      "Epoch 21/200\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.5212 - auc: 0.8415\n",
      "Epoch 22/200\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.5129 - auc: 0.8437\n",
      "Epoch 23/200\n",
      "675/675 [==============================] - 0s 392us/step - loss: 0.5054 - auc: 0.8459\n",
      "Epoch 24/200\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.4988 - auc: 0.8477\n",
      "Epoch 25/200\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.4930 - auc: 0.8496\n",
      "Epoch 26/200\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.4879 - auc: 0.8514\n",
      "Epoch 27/200\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.4834 - auc: 0.8531\n",
      "Epoch 28/200\n",
      "675/675 [==============================] - 0s 493us/step - loss: 0.4793 - auc: 0.8546\n",
      "Epoch 29/200\n",
      "675/675 [==============================] - 0s 434us/step - loss: 0.4757 - auc: 0.8561\n",
      "Epoch 30/200\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.4724 - auc: 0.8576\n",
      "Epoch 31/200\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.4695 - auc: 0.8589\n",
      "Epoch 32/200\n",
      "675/675 [==============================] - 0s 392us/step - loss: 0.4667 - auc: 0.8602\n",
      "Epoch 33/200\n",
      "675/675 [==============================] - 0s 437us/step - loss: 0.4642 - auc: 0.8613\n",
      "Epoch 34/200\n",
      "675/675 [==============================] - 0s 394us/step - loss: 0.4618 - auc: 0.8624\n",
      "Epoch 35/200\n",
      "675/675 [==============================] - 0s 438us/step - loss: 0.4596 - auc: 0.8636\n",
      "Epoch 36/200\n",
      "675/675 [==============================] - 0s 429us/step - loss: 0.4576 - auc: 0.8646\n",
      "Epoch 37/200\n",
      "675/675 [==============================] - 0s 442us/step - loss: 0.4556 - auc: 0.8655\n",
      "Epoch 38/200\n",
      "675/675 [==============================] - 0s 441us/step - loss: 0.4538 - auc: 0.8665\n",
      "Epoch 39/200\n",
      "675/675 [==============================] - 0s 479us/step - loss: 0.4521 - auc: 0.8674\n",
      "Epoch 40/200\n",
      "675/675 [==============================] - 0s 446us/step - loss: 0.4504 - auc: 0.8683\n",
      "Epoch 41/200\n",
      "675/675 [==============================] - 0s 436us/step - loss: 0.4489 - auc: 0.8691\n",
      "Epoch 42/200\n",
      "675/675 [==============================] - 0s 430us/step - loss: 0.4474 - auc: 0.8698\n",
      "Epoch 43/200\n",
      "675/675 [==============================] - 0s 413us/step - loss: 0.4460 - auc: 0.8705\n",
      "Epoch 44/200\n",
      "675/675 [==============================] - 0s 425us/step - loss: 0.4446 - auc: 0.8713\n",
      "Epoch 45/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.4433 - auc: 0.8720\n",
      "Epoch 46/200\n",
      "675/675 [==============================] - 0s 426us/step - loss: 0.4421 - auc: 0.8725\n",
      "Epoch 47/200\n",
      "675/675 [==============================] - 0s 405us/step - loss: 0.4410 - auc: 0.8731\n",
      "Epoch 48/200\n",
      "675/675 [==============================] - 0s 438us/step - loss: 0.4399 - auc: 0.8736\n",
      "Epoch 49/200\n",
      "675/675 [==============================] - 0s 407us/step - loss: 0.4388 - auc: 0.8742\n",
      "Epoch 50/200\n",
      "675/675 [==============================] - 0s 408us/step - loss: 0.4378 - auc: 0.8748\n",
      "Epoch 51/200\n",
      "675/675 [==============================] - 0s 417us/step - loss: 0.4369 - auc: 0.8752\n",
      "Epoch 52/200\n",
      "675/675 [==============================] - 0s 392us/step - loss: 0.4359 - auc: 0.8758\n",
      "Epoch 53/200\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.4350 - auc: 0.8762\n",
      "Epoch 54/200\n",
      "675/675 [==============================] - 0s 392us/step - loss: 0.4342 - auc: 0.8767\n",
      "Epoch 55/200\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.4333 - auc: 0.8771\n",
      "Epoch 56/200\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.4326 - auc: 0.8775\n",
      "Epoch 57/200\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.4319 - auc: 0.8778\n",
      "Epoch 58/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.4311 - auc: 0.8783\n",
      "Epoch 59/200\n",
      "675/675 [==============================] - 0s 389us/step - loss: 0.4305 - auc: 0.8786\n",
      "Epoch 60/200\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.4298 - auc: 0.8790\n",
      "Epoch 61/200\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.4291 - auc: 0.8794\n",
      "Epoch 62/200\n",
      "675/675 [==============================] - 0s 392us/step - loss: 0.4285 - auc: 0.8796\n",
      "Epoch 63/200\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.4279 - auc: 0.8799\n",
      "Epoch 64/200\n",
      "675/675 [==============================] - 0s 510us/step - loss: 0.4273 - auc: 0.8803\n",
      "Epoch 65/200\n",
      "675/675 [==============================] - 0s 410us/step - loss: 0.4267 - auc: 0.8807\n",
      "Epoch 66/200\n",
      "675/675 [==============================] - 0s 402us/step - loss: 0.4261 - auc: 0.8809\n",
      "Epoch 67/200\n",
      "675/675 [==============================] - 0s 406us/step - loss: 0.4257 - auc: 0.8811\n",
      "Epoch 68/200\n",
      "675/675 [==============================] - 0s 438us/step - loss: 0.4252 - auc: 0.8814\n",
      "Epoch 69/200\n",
      "675/675 [==============================] - 0s 418us/step - loss: 0.4246 - auc: 0.8817\n",
      "Epoch 70/200\n",
      "675/675 [==============================] - 0s 408us/step - loss: 0.4242 - auc: 0.8819\n",
      "Epoch 71/200\n",
      "675/675 [==============================] - 0s 403us/step - loss: 0.4237 - auc: 0.8822\n",
      "Epoch 72/200\n",
      "675/675 [==============================] - 0s 413us/step - loss: 0.4232 - auc: 0.8824\n",
      "Epoch 73/200\n",
      "675/675 [==============================] - 0s 409us/step - loss: 0.4227 - auc: 0.8827\n",
      "Epoch 74/200\n",
      "675/675 [==============================] - 0s 449us/step - loss: 0.4223 - auc: 0.8829\n",
      "Epoch 75/200\n",
      "675/675 [==============================] - 0s 410us/step - loss: 0.4218 - auc: 0.8832\n",
      "Epoch 76/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.4213 - auc: 0.8835\n",
      "Epoch 77/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.4210 - auc: 0.8836\n",
      "Epoch 78/200\n",
      "675/675 [==============================] - 0s 396us/step - loss: 0.4206 - auc: 0.8838\n",
      "Epoch 79/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.4201 - auc: 0.8842\n",
      "Epoch 80/200\n",
      "675/675 [==============================] - 0s 396us/step - loss: 0.4198 - auc: 0.8842\n",
      "Epoch 81/200\n",
      "675/675 [==============================] - 0s 405us/step - loss: 0.4193 - auc: 0.8845\n",
      "Epoch 82/200\n",
      "675/675 [==============================] - 0s 405us/step - loss: 0.4189 - auc: 0.8848\n",
      "Epoch 83/200\n",
      "675/675 [==============================] - 0s 401us/step - loss: 0.4185 - auc: 0.8849\n",
      "Epoch 84/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.4181 - auc: 0.8851\n",
      "Epoch 85/200\n",
      "675/675 [==============================] - 0s 412us/step - loss: 0.4177 - auc: 0.8854\n",
      "Epoch 86/200\n",
      "675/675 [==============================] - 0s 421us/step - loss: 0.4174 - auc: 0.8855\n",
      "Epoch 87/200\n",
      "675/675 [==============================] - 0s 412us/step - loss: 0.4170 - auc: 0.8857\n",
      "Epoch 88/200\n",
      "675/675 [==============================] - 0s 405us/step - loss: 0.4166 - auc: 0.8860\n",
      "Epoch 89/200\n",
      "675/675 [==============================] - 0s 403us/step - loss: 0.4163 - auc: 0.8861\n",
      "Epoch 90/200\n",
      "675/675 [==============================] - 0s 413us/step - loss: 0.4159 - auc: 0.8863\n",
      "Epoch 91/200\n",
      "675/675 [==============================] - 0s 441us/step - loss: 0.4155 - auc: 0.8866\n",
      "Epoch 92/200\n",
      "675/675 [==============================] - 0s 397us/step - loss: 0.4152 - auc: 0.8867\n",
      "Epoch 93/200\n",
      "675/675 [==============================] - 0s 458us/step - loss: 0.4149 - auc: 0.8870\n",
      "Epoch 94/200\n",
      "675/675 [==============================] - 0s 422us/step - loss: 0.4144 - auc: 0.8872\n",
      "Epoch 95/200\n",
      "675/675 [==============================] - 0s 421us/step - loss: 0.4141 - auc: 0.8874\n",
      "Epoch 96/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.4137 - auc: 0.8875\n",
      "Epoch 97/200\n",
      "675/675 [==============================] - 0s 414us/step - loss: 0.4134 - auc: 0.8877\n",
      "Epoch 98/200\n",
      "675/675 [==============================] - 0s 412us/step - loss: 0.4130 - auc: 0.8879\n",
      "Epoch 99/200\n",
      "675/675 [==============================] - 0s 400us/step - loss: 0.4127 - auc: 0.8881\n",
      "Epoch 100/200\n",
      "675/675 [==============================] - 0s 403us/step - loss: 0.4123 - auc: 0.8883\n",
      "Epoch 101/200\n",
      "675/675 [==============================] - 0s 408us/step - loss: 0.4120 - auc: 0.8885\n",
      "Epoch 102/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.4115 - auc: 0.8887\n",
      "Epoch 103/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.4112 - auc: 0.8889\n",
      "Epoch 104/200\n",
      "675/675 [==============================] - 0s 401us/step - loss: 0.4109 - auc: 0.8890\n",
      "Epoch 105/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.4105 - auc: 0.8893\n",
      "Epoch 106/200\n",
      "675/675 [==============================] - 0s 435us/step - loss: 0.4101 - auc: 0.8894\n",
      "Epoch 107/200\n",
      "675/675 [==============================] - 0s 405us/step - loss: 0.4098 - auc: 0.8897\n",
      "Epoch 108/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.4094 - auc: 0.8898\n",
      "Epoch 109/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.4090 - auc: 0.8901\n",
      "Epoch 110/200\n",
      "675/675 [==============================] - 0s 402us/step - loss: 0.4086 - auc: 0.8903\n",
      "Epoch 111/200\n",
      "675/675 [==============================] - 0s 401us/step - loss: 0.4082 - auc: 0.8905\n",
      "Epoch 112/200\n",
      "675/675 [==============================] - 0s 402us/step - loss: 0.4078 - auc: 0.8907\n",
      "Epoch 113/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.4075 - auc: 0.8909\n",
      "Epoch 114/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.4071 - auc: 0.8911\n",
      "Epoch 115/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.4067 - auc: 0.8914\n",
      "Epoch 116/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.4063 - auc: 0.8915\n",
      "Epoch 117/200\n",
      "675/675 [==============================] - 0s 401us/step - loss: 0.4059 - auc: 0.8917\n",
      "Epoch 118/200\n",
      "675/675 [==============================] - 0s 397us/step - loss: 0.4055 - auc: 0.8920\n",
      "Epoch 119/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.4051 - auc: 0.8922\n",
      "Epoch 120/200\n",
      "675/675 [==============================] - 0s 400us/step - loss: 0.4047 - auc: 0.8924\n",
      "Epoch 121/200\n",
      "675/675 [==============================] - 0s 440us/step - loss: 0.4043 - auc: 0.8926\n",
      "Epoch 122/200\n",
      "675/675 [==============================] - 0s 401us/step - loss: 0.4039 - auc: 0.8927\n",
      "Epoch 123/200\n",
      "675/675 [==============================] - 0s 400us/step - loss: 0.4035 - auc: 0.8930\n",
      "Epoch 124/200\n",
      "675/675 [==============================] - 0s 400us/step - loss: 0.4030 - auc: 0.8933\n",
      "Epoch 125/200\n",
      "675/675 [==============================] - 0s 397us/step - loss: 0.4027 - auc: 0.8934\n",
      "Epoch 126/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.4022 - auc: 0.8938\n",
      "Epoch 127/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.4019 - auc: 0.8940\n",
      "Epoch 128/200\n",
      "675/675 [==============================] - 0s 397us/step - loss: 0.4015 - auc: 0.8941\n",
      "Epoch 129/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.4010 - auc: 0.8943\n",
      "Epoch 130/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.4005 - auc: 0.8946\n",
      "Epoch 131/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.4002 - auc: 0.8947\n",
      "Epoch 132/200\n",
      "675/675 [==============================] - 0s 401us/step - loss: 0.3998 - auc: 0.8950\n",
      "Epoch 133/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3994 - auc: 0.8952\n",
      "Epoch 134/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.3989 - auc: 0.8955\n",
      "Epoch 135/200\n",
      "675/675 [==============================] - 0s 435us/step - loss: 0.3985 - auc: 0.8957\n",
      "Epoch 136/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3980 - auc: 0.8960\n",
      "Epoch 137/200\n",
      "675/675 [==============================] - 0s 402us/step - loss: 0.3976 - auc: 0.8961\n",
      "Epoch 138/200\n",
      "675/675 [==============================] - 0s 397us/step - loss: 0.3972 - auc: 0.8963\n",
      "Epoch 139/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3968 - auc: 0.8966\n",
      "Epoch 140/200\n",
      "675/675 [==============================] - 0s 403us/step - loss: 0.3964 - auc: 0.8968\n",
      "Epoch 141/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3958 - auc: 0.8970\n",
      "Epoch 142/200\n",
      "675/675 [==============================] - 0s 400us/step - loss: 0.3954 - auc: 0.8972\n",
      "Epoch 143/200\n",
      "675/675 [==============================] - 0s 401us/step - loss: 0.3950 - auc: 0.8975\n",
      "Epoch 144/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3946 - auc: 0.8977\n",
      "Epoch 145/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.3942 - auc: 0.8979\n",
      "Epoch 146/200\n",
      "675/675 [==============================] - 0s 402us/step - loss: 0.3937 - auc: 0.8982\n",
      "Epoch 147/200\n",
      "675/675 [==============================] - 0s 400us/step - loss: 0.3932 - auc: 0.8985\n",
      "Epoch 148/200\n",
      "675/675 [==============================] - 0s 401us/step - loss: 0.3928 - auc: 0.8986\n",
      "Epoch 149/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3925 - auc: 0.8988\n",
      "Epoch 150/200\n",
      "675/675 [==============================] - 0s 436us/step - loss: 0.3920 - auc: 0.8991\n",
      "Epoch 151/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.3916 - auc: 0.8993\n",
      "Epoch 152/200\n",
      "675/675 [==============================] - 0s 403us/step - loss: 0.3912 - auc: 0.8995\n",
      "Epoch 153/200\n",
      "675/675 [==============================] - 0s 397us/step - loss: 0.3907 - auc: 0.8998\n",
      "Epoch 154/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3903 - auc: 0.9000\n",
      "Epoch 155/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3898 - auc: 0.9003\n",
      "Epoch 156/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3894 - auc: 0.9005\n",
      "Epoch 157/200\n",
      "675/675 [==============================] - 0s 405us/step - loss: 0.3890 - auc: 0.9007\n",
      "Epoch 158/200\n",
      "675/675 [==============================] - 0s 400us/step - loss: 0.3885 - auc: 0.9010\n",
      "Epoch 159/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.3881 - auc: 0.9012\n",
      "Epoch 160/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3878 - auc: 0.9013\n",
      "Epoch 161/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.3873 - auc: 0.9016\n",
      "Epoch 162/200\n",
      "675/675 [==============================] - 0s 400us/step - loss: 0.3869 - auc: 0.9017\n",
      "Epoch 163/200\n",
      "675/675 [==============================] - 0s 433us/step - loss: 0.3865 - auc: 0.9020\n",
      "Epoch 164/200\n",
      "675/675 [==============================] - 0s 400us/step - loss: 0.3861 - auc: 0.9022\n",
      "Epoch 165/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3857 - auc: 0.9024\n",
      "Epoch 166/200\n",
      "675/675 [==============================] - 0s 404us/step - loss: 0.3853 - auc: 0.9027\n",
      "Epoch 167/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3848 - auc: 0.9029\n",
      "Epoch 168/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3844 - auc: 0.9032\n",
      "Epoch 169/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3840 - auc: 0.9033\n",
      "Epoch 170/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.3836 - auc: 0.9036\n",
      "Epoch 171/200\n",
      "675/675 [==============================] - 0s 397us/step - loss: 0.3833 - auc: 0.9038\n",
      "Epoch 172/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3828 - auc: 0.9040\n",
      "Epoch 173/200\n",
      "675/675 [==============================] - 0s 406us/step - loss: 0.3824 - auc: 0.9041\n",
      "Epoch 174/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3821 - auc: 0.9044\n",
      "Epoch 175/200\n",
      "675/675 [==============================] - 0s 410us/step - loss: 0.3817 - auc: 0.9046\n",
      "Epoch 176/200\n",
      "675/675 [==============================] - 0s 439us/step - loss: 0.3813 - auc: 0.9048\n",
      "Epoch 177/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3809 - auc: 0.9050\n",
      "Epoch 178/200\n",
      "675/675 [==============================] - 0s 396us/step - loss: 0.3804 - auc: 0.9053\n",
      "Epoch 179/200\n",
      "675/675 [==============================] - 0s 400us/step - loss: 0.3801 - auc: 0.9055\n",
      "Epoch 180/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.3798 - auc: 0.9056\n",
      "Epoch 181/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3794 - auc: 0.9058\n",
      "Epoch 182/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.3790 - auc: 0.9060\n",
      "Epoch 183/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3787 - auc: 0.9062\n",
      "Epoch 184/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.3782 - auc: 0.9065\n",
      "Epoch 185/200\n",
      "675/675 [==============================] - 0s 401us/step - loss: 0.3780 - auc: 0.9066\n",
      "Epoch 186/200\n",
      "675/675 [==============================] - 0s 400us/step - loss: 0.3776 - auc: 0.9068\n",
      "Epoch 187/200\n",
      "675/675 [==============================] - 0s 400us/step - loss: 0.3771 - auc: 0.9070\n",
      "Epoch 188/200\n",
      "675/675 [==============================] - 0s 405us/step - loss: 0.3768 - auc: 0.9073\n",
      "Epoch 189/200\n",
      "675/675 [==============================] - 0s 434us/step - loss: 0.3765 - auc: 0.9074\n",
      "Epoch 190/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3762 - auc: 0.9075\n",
      "Epoch 191/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3759 - auc: 0.9077\n",
      "Epoch 192/200\n",
      "675/675 [==============================] - 0s 402us/step - loss: 0.3755 - auc: 0.9079\n",
      "Epoch 193/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.3752 - auc: 0.9081\n",
      "Epoch 194/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.3749 - auc: 0.9082\n",
      "Epoch 195/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.3745 - auc: 0.9084\n",
      "Epoch 196/200\n",
      "675/675 [==============================] - 0s 400us/step - loss: 0.3743 - auc: 0.9086\n",
      "Epoch 197/200\n",
      "675/675 [==============================] - 0s 397us/step - loss: 0.3739 - auc: 0.9087\n",
      "Epoch 198/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.3737 - auc: 0.9089\n",
      "Epoch 199/200\n",
      "675/675 [==============================] - 0s 401us/step - loss: 0.3733 - auc: 0.9091\n",
      "Epoch 200/200\n",
      "675/675 [==============================] - 0s 421us/step - loss: 0.3730 - auc: 0.9093\n",
      "675/675 [==============================] - 0s 268us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/franciscolabollita/Library/Python/3.8/lib/python/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "675/675 [==============================] - 0s 410us/step - loss: 0.6935 - auc: 0.5164\n",
      "Epoch 2/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.6902 - auc: 0.5863\n",
      "Epoch 3/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.6864 - auc: 0.6471\n",
      "Epoch 4/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.6820 - auc: 0.6928\n",
      "Epoch 5/200\n",
      "675/675 [==============================] - 0s 397us/step - loss: 0.6772 - auc: 0.7221\n",
      "Epoch 6/200\n",
      "675/675 [==============================] - 0s 406us/step - loss: 0.6720 - auc: 0.7450\n",
      "Epoch 7/200\n",
      "675/675 [==============================] - 0s 395us/step - loss: 0.6663 - auc: 0.7610\n",
      "Epoch 8/200\n",
      "675/675 [==============================] - 0s 397us/step - loss: 0.6600 - auc: 0.7726\n",
      "Epoch 9/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.6532 - auc: 0.7812\n",
      "Epoch 10/200\n",
      "675/675 [==============================] - 0s 445us/step - loss: 0.6458 - auc: 0.7875\n",
      "Epoch 11/200\n",
      "675/675 [==============================] - 0s 406us/step - loss: 0.6379 - auc: 0.7929\n",
      "Epoch 12/200\n",
      "675/675 [==============================] - 0s 402us/step - loss: 0.6294 - auc: 0.7970\n",
      "Epoch 13/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.6207 - auc: 0.8011\n",
      "Epoch 14/200\n",
      "675/675 [==============================] - 0s 403us/step - loss: 0.6118 - auc: 0.8035\n",
      "Epoch 15/200\n",
      "675/675 [==============================] - 0s 403us/step - loss: 0.6030 - auc: 0.8063\n",
      "Epoch 16/200\n",
      "675/675 [==============================] - 0s 403us/step - loss: 0.5943 - auc: 0.8088\n",
      "Epoch 17/200\n",
      "675/675 [==============================] - 0s 409us/step - loss: 0.5860 - auc: 0.8115\n",
      "Epoch 18/200\n",
      "675/675 [==============================] - 0s 401us/step - loss: 0.5781 - auc: 0.8141\n",
      "Epoch 19/200\n",
      "675/675 [==============================] - 0s 403us/step - loss: 0.5706 - auc: 0.8169\n",
      "Epoch 20/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.5636 - auc: 0.8191\n",
      "Epoch 21/200\n",
      "675/675 [==============================] - 0s 400us/step - loss: 0.5571 - auc: 0.8217\n",
      "Epoch 22/200\n",
      "675/675 [==============================] - 0s 406us/step - loss: 0.5510 - auc: 0.8244\n",
      "Epoch 23/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.5453 - auc: 0.8272\n",
      "Epoch 24/200\n",
      "675/675 [==============================] - 0s 396us/step - loss: 0.5400 - auc: 0.8294\n",
      "Epoch 25/200\n",
      "675/675 [==============================] - 0s 397us/step - loss: 0.5349 - auc: 0.8314\n",
      "Epoch 26/200\n",
      "675/675 [==============================] - 0s 397us/step - loss: 0.5302 - auc: 0.8338\n",
      "Epoch 27/200\n",
      "675/675 [==============================] - 0s 402us/step - loss: 0.5258 - auc: 0.8360\n",
      "Epoch 28/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.5217 - auc: 0.8379\n",
      "Epoch 29/200\n",
      "675/675 [==============================] - 0s 419us/step - loss: 0.5177 - auc: 0.8395\n",
      "Epoch 30/200\n",
      "675/675 [==============================] - 0s 407us/step - loss: 0.5140 - auc: 0.8414\n",
      "Epoch 31/200\n",
      "675/675 [==============================] - 0s 402us/step - loss: 0.5105 - auc: 0.8430\n",
      "Epoch 32/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.5073 - auc: 0.8443\n",
      "Epoch 33/200\n",
      "675/675 [==============================] - 0s 436us/step - loss: 0.5042 - auc: 0.8456\n",
      "Epoch 34/200\n",
      "675/675 [==============================] - 0s 404us/step - loss: 0.5013 - auc: 0.8468\n",
      "Epoch 35/200\n",
      "675/675 [==============================] - 0s 407us/step - loss: 0.4985 - auc: 0.8478\n",
      "Epoch 36/200\n",
      "675/675 [==============================] - 0s 403us/step - loss: 0.4959 - auc: 0.8489\n",
      "Epoch 37/200\n",
      "675/675 [==============================] - 0s 404us/step - loss: 0.4934 - auc: 0.8500\n",
      "Epoch 38/200\n",
      "675/675 [==============================] - 0s 422us/step - loss: 0.4911 - auc: 0.8509\n",
      "Epoch 39/200\n",
      "675/675 [==============================] - 0s 405us/step - loss: 0.4888 - auc: 0.8518\n",
      "Epoch 40/200\n",
      "675/675 [==============================] - 0s 406us/step - loss: 0.4867 - auc: 0.8528\n",
      "Epoch 41/200\n",
      "675/675 [==============================] - 0s 408us/step - loss: 0.4847 - auc: 0.8537\n",
      "Epoch 42/200\n",
      "675/675 [==============================] - 0s 410us/step - loss: 0.4827 - auc: 0.8543\n",
      "Epoch 43/200\n",
      "675/675 [==============================] - 0s 405us/step - loss: 0.4808 - auc: 0.8554\n",
      "Epoch 44/200\n",
      "675/675 [==============================] - 0s 413us/step - loss: 0.4791 - auc: 0.8559\n",
      "Epoch 45/200\n",
      "675/675 [==============================] - 0s 405us/step - loss: 0.4773 - auc: 0.8567\n",
      "Epoch 46/200\n",
      "675/675 [==============================] - 0s 404us/step - loss: 0.4757 - auc: 0.8574\n",
      "Epoch 47/200\n",
      "675/675 [==============================] - 0s 404us/step - loss: 0.4741 - auc: 0.8582\n",
      "Epoch 48/200\n",
      "675/675 [==============================] - 0s 406us/step - loss: 0.4725 - auc: 0.8590\n",
      "Epoch 49/200\n",
      "675/675 [==============================] - 0s 406us/step - loss: 0.4710 - auc: 0.8598\n",
      "Epoch 50/200\n",
      "675/675 [==============================] - 0s 402us/step - loss: 0.4696 - auc: 0.8604\n",
      "Epoch 51/200\n",
      "675/675 [==============================] - 0s 403us/step - loss: 0.4682 - auc: 0.8610\n",
      "Epoch 52/200\n",
      "675/675 [==============================] - 0s 403us/step - loss: 0.4668 - auc: 0.8618\n",
      "Epoch 53/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.4655 - auc: 0.8623\n",
      "Epoch 54/200\n",
      "675/675 [==============================] - 0s 438us/step - loss: 0.4643 - auc: 0.8626\n",
      "Epoch 55/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.4631 - auc: 0.8634\n",
      "Epoch 56/200\n",
      "675/675 [==============================] - 0s 401us/step - loss: 0.4619 - auc: 0.8641\n",
      "Epoch 57/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.4607 - auc: 0.8646\n",
      "Epoch 58/200\n",
      "675/675 [==============================] - 0s 400us/step - loss: 0.4596 - auc: 0.8654\n",
      "Epoch 59/200\n",
      "675/675 [==============================] - 0s 403us/step - loss: 0.4586 - auc: 0.8657\n",
      "Epoch 60/200\n",
      "675/675 [==============================] - 0s 403us/step - loss: 0.4575 - auc: 0.8662\n",
      "Epoch 61/200\n",
      "675/675 [==============================] - 0s 407us/step - loss: 0.4565 - auc: 0.8668\n",
      "Epoch 62/200\n",
      "675/675 [==============================] - 0s 404us/step - loss: 0.4555 - auc: 0.8675\n",
      "Epoch 63/200\n",
      "675/675 [==============================] - 0s 403us/step - loss: 0.4545 - auc: 0.8680\n",
      "Epoch 64/200\n",
      "675/675 [==============================] - 0s 401us/step - loss: 0.4536 - auc: 0.8686\n",
      "Epoch 65/200\n",
      "675/675 [==============================] - 0s 402us/step - loss: 0.4526 - auc: 0.8691\n",
      "Epoch 66/200\n",
      "675/675 [==============================] - 0s 407us/step - loss: 0.4516 - auc: 0.8696\n",
      "Epoch 67/200\n",
      "675/675 [==============================] - 0s 400us/step - loss: 0.4508 - auc: 0.8702\n",
      "Epoch 68/200\n",
      "675/675 [==============================] - 0s 408us/step - loss: 0.4500 - auc: 0.8706\n",
      "Epoch 69/200\n",
      "675/675 [==============================] - 0s 399us/step - loss: 0.4491 - auc: 0.8711\n",
      "Epoch 70/200\n",
      "675/675 [==============================] - 0s 393us/step - loss: 0.4482 - auc: 0.8717\n",
      "Epoch 71/200\n",
      "675/675 [==============================] - 0s 396us/step - loss: 0.4474 - auc: 0.8721\n",
      "Epoch 72/200\n",
      "675/675 [==============================] - 0s 395us/step - loss: 0.4466 - auc: 0.8726\n",
      "Epoch 73/200\n",
      "675/675 [==============================] - 0s 476us/step - loss: 0.4458 - auc: 0.8732\n",
      "Epoch 74/200\n",
      "675/675 [==============================] - 0s 393us/step - loss: 0.4450 - auc: 0.8737\n",
      "Epoch 75/200\n",
      "675/675 [==============================] - 0s 407us/step - loss: 0.4442 - auc: 0.8743\n",
      "Epoch 76/200\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.4435 - auc: 0.8746\n",
      "Epoch 77/200\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4428 - auc: 0.8750\n",
      "Epoch 78/200\n",
      "675/675 [==============================] - 0s 394us/step - loss: 0.4421 - auc: 0.8755\n",
      "Epoch 79/200\n",
      "675/675 [==============================] - 0s 416us/step - loss: 0.4414 - auc: 0.8758\n",
      "Epoch 80/200\n",
      "675/675 [==============================] - 0s 389us/step - loss: 0.4407 - auc: 0.8763\n",
      "Epoch 81/200\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4401 - auc: 0.8766\n",
      "Epoch 82/200\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.4394 - auc: 0.8769\n",
      "Epoch 83/200\n",
      "675/675 [==============================] - 0s 392us/step - loss: 0.4387 - auc: 0.8773\n",
      "Epoch 84/200\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.4381 - auc: 0.8777\n",
      "Epoch 85/200\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.4374 - auc: 0.8781\n",
      "Epoch 86/200\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.4367 - auc: 0.8784\n",
      "Epoch 87/200\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.4361 - auc: 0.8788\n",
      "Epoch 88/200\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.4353 - auc: 0.8793\n",
      "Epoch 89/200\n",
      "675/675 [==============================] - 0s 498us/step - loss: 0.4348 - auc: 0.8796\n",
      "Epoch 90/200\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.4341 - auc: 0.8799\n",
      "Epoch 91/200\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4334 - auc: 0.8804\n",
      "Epoch 92/200\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.4328 - auc: 0.8808\n",
      "Epoch 93/200\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.4322 - auc: 0.8811\n",
      "Epoch 94/200\n",
      "675/675 [==============================] - 0s 394us/step - loss: 0.4315 - auc: 0.8815\n",
      "Epoch 95/200\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.4309 - auc: 0.8817\n",
      "Epoch 96/200\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.4303 - auc: 0.8821\n",
      "Epoch 97/200\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.4295 - auc: 0.8825\n",
      "Epoch 98/200\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.4291 - auc: 0.8827\n",
      "Epoch 99/200\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.4284 - auc: 0.8830\n",
      "Epoch 100/200\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.4279 - auc: 0.8834\n",
      "Epoch 101/200\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4273 - auc: 0.8838\n",
      "Epoch 102/200\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.4267 - auc: 0.8839\n",
      "Epoch 103/200\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4261 - auc: 0.8843\n",
      "Epoch 104/200\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4255 - auc: 0.8845\n",
      "Epoch 105/200\n",
      "675/675 [==============================] - 0s 420us/step - loss: 0.4249 - auc: 0.8849\n",
      "Epoch 106/200\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.4243 - auc: 0.8851\n",
      "Epoch 107/200\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.4237 - auc: 0.8855\n",
      "Epoch 108/200\n",
      "675/675 [==============================] - 0s 393us/step - loss: 0.4232 - auc: 0.8857\n",
      "Epoch 109/200\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.4226 - auc: 0.8859\n",
      "Epoch 110/200\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.4221 - auc: 0.8862\n",
      "Epoch 111/200\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.4215 - auc: 0.8866\n",
      "Epoch 112/200\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.4209 - auc: 0.8870\n",
      "Epoch 113/200\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.4204 - auc: 0.8872\n",
      "Epoch 114/200\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4199 - auc: 0.8874\n",
      "Epoch 115/200\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.4193 - auc: 0.8877\n",
      "Epoch 116/200\n",
      "675/675 [==============================] - 0s 389us/step - loss: 0.4188 - auc: 0.8880\n",
      "Epoch 117/200\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.4181 - auc: 0.8884\n",
      "Epoch 118/200\n",
      "675/675 [==============================] - 0s 425us/step - loss: 0.4176 - auc: 0.8887\n",
      "Epoch 119/200\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.4171 - auc: 0.8888\n",
      "Epoch 120/200\n",
      "675/675 [==============================] - 0s 389us/step - loss: 0.4166 - auc: 0.8891\n",
      "Epoch 121/200\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.4160 - auc: 0.8894\n",
      "Epoch 122/200\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4155 - auc: 0.8897\n",
      "Epoch 123/200\n",
      "675/675 [==============================] - 0s 384us/step - loss: 0.4150 - auc: 0.8900\n",
      "Epoch 124/200\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4144 - auc: 0.8903\n",
      "Epoch 125/200\n",
      "675/675 [==============================] - 0s 383us/step - loss: 0.4140 - auc: 0.8905\n",
      "Epoch 126/200\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.4134 - auc: 0.8908\n",
      "Epoch 127/200\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4127 - auc: 0.8911\n",
      "Epoch 128/200\n",
      "675/675 [==============================] - 0s 382us/step - loss: 0.4122 - auc: 0.8915\n",
      "Epoch 129/200\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.4118 - auc: 0.8917\n",
      "Epoch 130/200\n",
      "675/675 [==============================] - 0s 417us/step - loss: 0.4113 - auc: 0.8921\n",
      "Epoch 131/200\n",
      "675/675 [==============================] - 0s 405us/step - loss: 0.4107 - auc: 0.8923\n",
      "Epoch 132/200\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.4102 - auc: 0.8926\n",
      "Epoch 133/200\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.4097 - auc: 0.8928\n",
      "Epoch 134/200\n",
      "675/675 [==============================] - 0s 392us/step - loss: 0.4092 - auc: 0.8930\n",
      "Epoch 135/200\n",
      "675/675 [==============================] - 0s 464us/step - loss: 0.4087 - auc: 0.8934\n",
      "Epoch 136/200\n",
      "675/675 [==============================] - 0s 468us/step - loss: 0.4081 - auc: 0.8937\n",
      "Epoch 137/200\n",
      "675/675 [==============================] - 0s 435us/step - loss: 0.4077 - auc: 0.8939\n",
      "Epoch 138/200\n",
      "675/675 [==============================] - 0s 437us/step - loss: 0.4072 - auc: 0.8943\n",
      "Epoch 139/200\n",
      "675/675 [==============================] - 0s 431us/step - loss: 0.4067 - auc: 0.8945\n",
      "Epoch 140/200\n",
      "675/675 [==============================] - 0s 434us/step - loss: 0.4062 - auc: 0.8948\n",
      "Epoch 141/200\n",
      "675/675 [==============================] - 0s 424us/step - loss: 0.4056 - auc: 0.8951\n",
      "Epoch 142/200\n",
      "675/675 [==============================] - 0s 422us/step - loss: 0.4051 - auc: 0.8952\n",
      "Epoch 143/200\n",
      "675/675 [==============================] - 0s 440us/step - loss: 0.4047 - auc: 0.8955\n",
      "Epoch 144/200\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.4040 - auc: 0.8960\n",
      "Epoch 145/200\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.4037 - auc: 0.8961\n",
      "Epoch 146/200\n",
      "675/675 [==============================] - 0s 389us/step - loss: 0.4032 - auc: 0.8964\n",
      "Epoch 147/200\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.4027 - auc: 0.8967\n",
      "Epoch 148/200\n",
      "675/675 [==============================] - 0s 427us/step - loss: 0.4023 - auc: 0.8968\n",
      "Epoch 149/200\n",
      "675/675 [==============================] - 0s 389us/step - loss: 0.4018 - auc: 0.8970\n",
      "Epoch 150/200\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4013 - auc: 0.8974\n",
      "Epoch 151/200\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.4009 - auc: 0.8976\n",
      "Epoch 152/200\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.4005 - auc: 0.8978\n",
      "Epoch 153/200\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.4000 - auc: 0.8981\n",
      "Epoch 154/200\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.3995 - auc: 0.8983\n",
      "Epoch 155/200\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.3991 - auc: 0.8985\n",
      "Epoch 156/200\n",
      "675/675 [==============================] - 0s 392us/step - loss: 0.3987 - auc: 0.8988\n",
      "Epoch 157/200\n",
      "675/675 [==============================] - 0s 389us/step - loss: 0.3982 - auc: 0.8990\n",
      "Epoch 158/200\n",
      "675/675 [==============================] - 0s 392us/step - loss: 0.3977 - auc: 0.8992\n",
      "Epoch 159/200\n",
      "675/675 [==============================] - 0s 390us/step - loss: 0.3973 - auc: 0.8994\n",
      "Epoch 160/200\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.3969 - auc: 0.8996\n",
      "Epoch 161/200\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.3964 - auc: 0.9000\n",
      "Epoch 162/200\n",
      "675/675 [==============================] - 0s 385us/step - loss: 0.3960 - auc: 0.9002\n",
      "Epoch 163/200\n",
      "675/675 [==============================] - 0s 423us/step - loss: 0.3957 - auc: 0.9003\n",
      "Epoch 164/200\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.3952 - auc: 0.9005\n",
      "Epoch 165/200\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.3948 - auc: 0.9007\n",
      "Epoch 166/200\n",
      "675/675 [==============================] - 0s 386us/step - loss: 0.3944 - auc: 0.9009\n",
      "Epoch 167/200\n",
      "675/675 [==============================] - 0s 388us/step - loss: 0.3940 - auc: 0.9012\n",
      "Epoch 168/200\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.3937 - auc: 0.9013\n",
      "Epoch 169/200\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.3932 - auc: 0.9016\n",
      "Epoch 170/200\n",
      "675/675 [==============================] - 0s 389us/step - loss: 0.3929 - auc: 0.9019\n",
      "Epoch 171/200\n",
      "675/675 [==============================] - 0s 387us/step - loss: 0.3925 - auc: 0.9020\n",
      "Epoch 172/200\n",
      "675/675 [==============================] - 0s 443us/step - loss: 0.3921 - auc: 0.9022\n",
      "Epoch 173/200\n",
      "675/675 [==============================] - 0s 408us/step - loss: 0.3917 - auc: 0.9024\n",
      "Epoch 174/200\n",
      "675/675 [==============================] - 0s 440us/step - loss: 0.3912 - auc: 0.9026\n",
      "Epoch 175/200\n",
      "675/675 [==============================] - 0s 398us/step - loss: 0.3909 - auc: 0.9028\n",
      "Epoch 176/200\n",
      "675/675 [==============================] - 0s 401us/step - loss: 0.3905 - auc: 0.9030\n",
      "Epoch 177/200\n",
      "675/675 [==============================] - 0s 404us/step - loss: 0.3902 - auc: 0.9032\n",
      "Epoch 178/200\n",
      "675/675 [==============================] - 0s 401us/step - loss: 0.3897 - auc: 0.9034\n",
      "Epoch 179/200\n",
      "675/675 [==============================] - 0s 407us/step - loss: 0.3895 - auc: 0.9035\n",
      "Epoch 180/200\n",
      "675/675 [==============================] - 0s 423us/step - loss: 0.3891 - auc: 0.9038\n",
      "Epoch 181/200\n",
      "675/675 [==============================] - 0s 417us/step - loss: 0.3888 - auc: 0.9039\n",
      "Epoch 182/200\n",
      "675/675 [==============================] - 0s 416us/step - loss: 0.3884 - auc: 0.9041\n",
      "Epoch 183/200\n",
      "675/675 [==============================] - 0s 401us/step - loss: 0.3880 - auc: 0.9043\n",
      "Epoch 184/200\n",
      "675/675 [==============================] - 0s 452us/step - loss: 0.3877 - auc: 0.9044\n",
      "Epoch 185/200\n",
      "675/675 [==============================] - 0s 402us/step - loss: 0.3873 - auc: 0.9046\n",
      "Epoch 186/200\n",
      "675/675 [==============================] - 0s 400us/step - loss: 0.3870 - auc: 0.9048\n",
      "Epoch 187/200\n",
      "675/675 [==============================] - 0s 403us/step - loss: 0.3866 - auc: 0.9050\n",
      "Epoch 188/200\n",
      "675/675 [==============================] - 0s 394us/step - loss: 0.3863 - auc: 0.9052\n",
      "Epoch 189/200\n",
      "675/675 [==============================] - 0s 403us/step - loss: 0.3860 - auc: 0.9052\n",
      "Epoch 190/200\n",
      "675/675 [==============================] - 0s 419us/step - loss: 0.3857 - auc: 0.9055\n",
      "Epoch 191/200\n",
      "675/675 [==============================] - 0s 403us/step - loss: 0.3854 - auc: 0.9056\n",
      "Epoch 192/200\n",
      "675/675 [==============================] - 0s 456us/step - loss: 0.3850 - auc: 0.9059\n",
      "Epoch 193/200\n",
      "675/675 [==============================] - 0s 396us/step - loss: 0.3847 - auc: 0.9060\n",
      "Epoch 194/200\n",
      "675/675 [==============================] - 0s 407us/step - loss: 0.3843 - auc: 0.9062\n",
      "Epoch 195/200\n",
      "675/675 [==============================] - 0s 393us/step - loss: 0.3841 - auc: 0.9064\n",
      "Epoch 196/200\n",
      "675/675 [==============================] - 0s 391us/step - loss: 0.3837 - auc: 0.9065\n",
      "Epoch 197/200\n",
      "675/675 [==============================] - 0s 394us/step - loss: 0.3835 - auc: 0.9067\n",
      "Epoch 198/200\n",
      "675/675 [==============================] - 0s 453us/step - loss: 0.3831 - auc: 0.9069\n",
      "Epoch 199/200\n",
      "675/675 [==============================] - 0s 402us/step - loss: 0.3828 - auc: 0.9070\n",
      "Epoch 200/200\n",
      "675/675 [==============================] - 0s 396us/step - loss: 0.3826 - auc: 0.9071\n",
      "675/675 [==============================] - 0s 339us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/franciscolabollita/Library/Python/3.8/lib/python/site-packages/scikeras/wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/175\n",
      "1350/1350 [==============================] - 1s 415us/step - loss: 0.6930 - auc: 0.5180\n",
      "Epoch 2/175\n",
      "1350/1350 [==============================] - 1s 408us/step - loss: 0.6883 - auc: 0.6169\n",
      "Epoch 3/175\n",
      "1350/1350 [==============================] - 1s 404us/step - loss: 0.6836 - auc: 0.6732\n",
      "Epoch 4/175\n",
      "1350/1350 [==============================] - 1s 402us/step - loss: 0.6771 - auc: 0.7128\n",
      "Epoch 5/175\n",
      "1350/1350 [==============================] - 1s 408us/step - loss: 0.6688 - auc: 0.7448\n",
      "Epoch 6/175\n",
      "1350/1350 [==============================] - 1s 403us/step - loss: 0.6588 - auc: 0.7678\n",
      "Epoch 7/175\n",
      "1350/1350 [==============================] - 1s 410us/step - loss: 0.6472 - auc: 0.7847\n",
      "Epoch 8/175\n",
      "1350/1350 [==============================] - 1s 409us/step - loss: 0.6340 - auc: 0.7965\n",
      "Epoch 9/175\n",
      "1350/1350 [==============================] - 1s 424us/step - loss: 0.6194 - auc: 0.8062\n",
      "Epoch 10/175\n",
      "1350/1350 [==============================] - 1s 418us/step - loss: 0.6044 - auc: 0.8134\n",
      "Epoch 11/175\n",
      "1350/1350 [==============================] - 1s 454us/step - loss: 0.5901 - auc: 0.8191\n",
      "Epoch 12/175\n",
      "1350/1350 [==============================] - 1s 415us/step - loss: 0.5768 - auc: 0.8240\n",
      "Epoch 13/175\n",
      "1350/1350 [==============================] - 1s 423us/step - loss: 0.5647 - auc: 0.8284\n",
      "Epoch 14/175\n",
      "1350/1350 [==============================] - 1s 417us/step - loss: 0.5536 - auc: 0.8327\n",
      "Epoch 15/175\n",
      "1350/1350 [==============================] - 1s 435us/step - loss: 0.5436 - auc: 0.8369\n",
      "Epoch 16/175\n",
      "1350/1350 [==============================] - 1s 404us/step - loss: 0.5344 - auc: 0.8403\n",
      "Epoch 17/175\n",
      "1350/1350 [==============================] - 1s 400us/step - loss: 0.5260 - auc: 0.8438\n",
      "Epoch 18/175\n",
      "1350/1350 [==============================] - 1s 429us/step - loss: 0.5183 - auc: 0.8472\n",
      "Epoch 19/175\n",
      "1350/1350 [==============================] - 1s 400us/step - loss: 0.5111 - auc: 0.8504\n",
      "Epoch 20/175\n",
      "1350/1350 [==============================] - 1s 402us/step - loss: 0.5046 - auc: 0.8531\n",
      "Epoch 21/175\n",
      "1350/1350 [==============================] - 1s 400us/step - loss: 0.4985 - auc: 0.8556\n",
      "Epoch 22/175\n",
      "1350/1350 [==============================] - 1s 403us/step - loss: 0.4928 - auc: 0.8578\n",
      "Epoch 23/175\n",
      "1350/1350 [==============================] - 1s 400us/step - loss: 0.4876 - auc: 0.8600\n",
      "Epoch 24/175\n",
      "1350/1350 [==============================] - 1s 412us/step - loss: 0.4827 - auc: 0.8618\n",
      "Epoch 25/175\n",
      "1350/1350 [==============================] - 1s 407us/step - loss: 0.4782 - auc: 0.8636\n",
      "Epoch 26/175\n",
      "1350/1350 [==============================] - 1s 412us/step - loss: 0.4739 - auc: 0.8651\n",
      "Epoch 27/175\n",
      "1350/1350 [==============================] - 1s 400us/step - loss: 0.4699 - auc: 0.8666\n",
      "Epoch 28/175\n",
      "1350/1350 [==============================] - 1s 431us/step - loss: 0.4661 - auc: 0.8680\n",
      "Epoch 29/175\n",
      "1350/1350 [==============================] - 1s 418us/step - loss: 0.4626 - auc: 0.8692\n",
      "Epoch 30/175\n",
      "1350/1350 [==============================] - 1s 428us/step - loss: 0.4593 - auc: 0.8703\n",
      "Epoch 31/175\n",
      "1350/1350 [==============================] - 1s 407us/step - loss: 0.4561 - auc: 0.8715\n",
      "Epoch 32/175\n",
      "1350/1350 [==============================] - 1s 401us/step - loss: 0.4530 - auc: 0.8725\n",
      "Epoch 33/175\n",
      "1350/1350 [==============================] - 1s 397us/step - loss: 0.4501 - auc: 0.8734\n",
      "Epoch 34/175\n",
      "1350/1350 [==============================] - 1s 401us/step - loss: 0.4473 - auc: 0.8743\n",
      "Epoch 35/175\n",
      "1350/1350 [==============================] - 1s 414us/step - loss: 0.4447 - auc: 0.8753\n",
      "Epoch 36/175\n",
      "1350/1350 [==============================] - 1s 397us/step - loss: 0.4423 - auc: 0.8761\n",
      "Epoch 37/175\n",
      "1350/1350 [==============================] - 1s 395us/step - loss: 0.4399 - auc: 0.8769\n",
      "Epoch 38/175\n",
      "1350/1350 [==============================] - 1s 395us/step - loss: 0.4375 - auc: 0.8778\n",
      "Epoch 39/175\n",
      "1350/1350 [==============================] - 1s 397us/step - loss: 0.4354 - auc: 0.8785\n",
      "Epoch 40/175\n",
      "1350/1350 [==============================] - 1s 397us/step - loss: 0.4333 - auc: 0.8792\n",
      "Epoch 41/175\n",
      "1350/1350 [==============================] - 1s 395us/step - loss: 0.4313 - auc: 0.8800\n",
      "Epoch 42/175\n",
      "1350/1350 [==============================] - 1s 396us/step - loss: 0.4293 - auc: 0.8807\n",
      "Epoch 43/175\n",
      "1350/1350 [==============================] - 1s 400us/step - loss: 0.4275 - auc: 0.8813\n",
      "Epoch 44/175\n",
      "1350/1350 [==============================] - 1s 395us/step - loss: 0.4257 - auc: 0.8820\n",
      "Epoch 45/175\n",
      "1350/1350 [==============================] - 1s 395us/step - loss: 0.4240 - auc: 0.8827\n",
      "Epoch 46/175\n",
      "1350/1350 [==============================] - 1s 395us/step - loss: 0.4223 - auc: 0.8833\n",
      "Epoch 47/175\n",
      "1350/1350 [==============================] - 1s 414us/step - loss: 0.4207 - auc: 0.8840\n",
      "Epoch 48/175\n",
      "1350/1350 [==============================] - 1s 396us/step - loss: 0.4192 - auc: 0.8846\n",
      "Epoch 49/175\n",
      "1350/1350 [==============================] - 1s 396us/step - loss: 0.4177 - auc: 0.8853\n",
      "Epoch 50/175\n",
      "1350/1350 [==============================] - 1s 397us/step - loss: 0.4163 - auc: 0.8859\n",
      "Epoch 51/175\n",
      "1350/1350 [==============================] - 1s 394us/step - loss: 0.4150 - auc: 0.8865\n",
      "Epoch 52/175\n",
      "1350/1350 [==============================] - 1s 395us/step - loss: 0.4136 - auc: 0.8871\n",
      "Epoch 53/175\n",
      "1350/1350 [==============================] - 1s 396us/step - loss: 0.4124 - auc: 0.8877\n",
      "Epoch 54/175\n",
      "1350/1350 [==============================] - 1s 394us/step - loss: 0.4112 - auc: 0.8884\n",
      "Epoch 55/175\n",
      "1350/1350 [==============================] - 1s 394us/step - loss: 0.4099 - auc: 0.8889\n",
      "Epoch 56/175\n",
      "1350/1350 [==============================] - 1s 400us/step - loss: 0.4089 - auc: 0.8895\n",
      "Epoch 57/175\n",
      "1350/1350 [==============================] - 1s 395us/step - loss: 0.4077 - auc: 0.8901\n",
      "Epoch 58/175\n",
      "1350/1350 [==============================] - 1s 415us/step - loss: 0.4068 - auc: 0.8906\n",
      "Epoch 59/175\n",
      "1350/1350 [==============================] - 1s 407us/step - loss: 0.4058 - auc: 0.8911\n",
      "Epoch 60/175\n",
      "1350/1350 [==============================] - 1s 407us/step - loss: 0.4047 - auc: 0.8917\n",
      "Epoch 61/175\n",
      "1350/1350 [==============================] - 1s 404us/step - loss: 0.4038 - auc: 0.8922\n",
      "Epoch 62/175\n",
      "1350/1350 [==============================] - 1s 408us/step - loss: 0.4029 - auc: 0.8927\n",
      "Epoch 63/175\n",
      "1350/1350 [==============================] - 1s 412us/step - loss: 0.4019 - auc: 0.8933\n",
      "Epoch 64/175\n",
      "1350/1350 [==============================] - 1s 416us/step - loss: 0.4010 - auc: 0.8938\n",
      "Epoch 65/175\n",
      "1350/1350 [==============================] - 1s 419us/step - loss: 0.4002 - auc: 0.8943\n",
      "Epoch 66/175\n",
      "1350/1350 [==============================] - 1s 417us/step - loss: 0.3993 - auc: 0.8948\n",
      "Epoch 67/175\n",
      "1350/1350 [==============================] - 1s 421us/step - loss: 0.3984 - auc: 0.8953\n",
      "Epoch 68/175\n",
      "1350/1350 [==============================] - 1s 401us/step - loss: 0.3976 - auc: 0.8959\n",
      "Epoch 69/175\n",
      "1350/1350 [==============================] - 1s 397us/step - loss: 0.3968 - auc: 0.8963\n",
      "Epoch 70/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3960 - auc: 0.8969\n",
      "Epoch 71/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3951 - auc: 0.8974\n",
      "Epoch 72/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3944 - auc: 0.8978\n",
      "Epoch 73/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3935 - auc: 0.8984\n",
      "Epoch 74/175\n",
      "1350/1350 [==============================] - 1s 397us/step - loss: 0.3928 - auc: 0.8988\n",
      "Epoch 75/175\n",
      "1350/1350 [==============================] - 1s 400us/step - loss: 0.3920 - auc: 0.8993\n",
      "Epoch 76/175\n",
      "1350/1350 [==============================] - 1s 400us/step - loss: 0.3913 - auc: 0.8998\n",
      "Epoch 77/175\n",
      "1350/1350 [==============================] - 1s 415us/step - loss: 0.3906 - auc: 0.9002\n",
      "Epoch 78/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3899 - auc: 0.9006\n",
      "Epoch 79/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3891 - auc: 0.9011\n",
      "Epoch 80/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3885 - auc: 0.9015\n",
      "Epoch 81/175\n",
      "1350/1350 [==============================] - 1s 404us/step - loss: 0.3877 - auc: 0.9020\n",
      "Epoch 82/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3871 - auc: 0.9023\n",
      "Epoch 83/175\n",
      "1350/1350 [==============================] - 1s 400us/step - loss: 0.3864 - auc: 0.9027\n",
      "Epoch 84/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3857 - auc: 0.9032\n",
      "Epoch 85/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3851 - auc: 0.9034\n",
      "Epoch 86/175\n",
      "1350/1350 [==============================] - 1s 417us/step - loss: 0.3844 - auc: 0.9039\n",
      "Epoch 87/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3838 - auc: 0.9042\n",
      "Epoch 88/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3833 - auc: 0.9045\n",
      "Epoch 89/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3826 - auc: 0.9049\n",
      "Epoch 90/175\n",
      "1350/1350 [==============================] - 1s 397us/step - loss: 0.3821 - auc: 0.9052\n",
      "Epoch 91/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3814 - auc: 0.9056\n",
      "Epoch 92/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3809 - auc: 0.9058\n",
      "Epoch 93/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3804 - auc: 0.9062\n",
      "Epoch 94/175\n",
      "1350/1350 [==============================] - 1s 401us/step - loss: 0.3798 - auc: 0.9065\n",
      "Epoch 95/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3792 - auc: 0.9068\n",
      "Epoch 96/175\n",
      "1350/1350 [==============================] - 1s 418us/step - loss: 0.3788 - auc: 0.9070\n",
      "Epoch 97/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3782 - auc: 0.9073\n",
      "Epoch 98/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3777 - auc: 0.9076\n",
      "Epoch 99/175\n",
      "1350/1350 [==============================] - 1s 397us/step - loss: 0.3772 - auc: 0.9078\n",
      "Epoch 100/175\n",
      "1350/1350 [==============================] - 1s 396us/step - loss: 0.3767 - auc: 0.9081\n",
      "Epoch 101/175\n",
      "1350/1350 [==============================] - 1s 397us/step - loss: 0.3763 - auc: 0.9083\n",
      "Epoch 102/175\n",
      "1350/1350 [==============================] - 1s 397us/step - loss: 0.3757 - auc: 0.9086\n",
      "Epoch 103/175\n",
      "1350/1350 [==============================] - 1s 400us/step - loss: 0.3753 - auc: 0.9088\n",
      "Epoch 104/175\n",
      "1350/1350 [==============================] - 1s 415us/step - loss: 0.3748 - auc: 0.9091\n",
      "Epoch 105/175\n",
      "1350/1350 [==============================] - 1s 400us/step - loss: 0.3744 - auc: 0.9093\n",
      "Epoch 106/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3740 - auc: 0.9095\n",
      "Epoch 107/175\n",
      "1350/1350 [==============================] - 1s 401us/step - loss: 0.3735 - auc: 0.9098\n",
      "Epoch 108/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3731 - auc: 0.9100\n",
      "Epoch 109/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3727 - auc: 0.9102\n",
      "Epoch 110/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3722 - auc: 0.9104\n",
      "Epoch 111/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3718 - auc: 0.9106\n",
      "Epoch 112/175\n",
      "1350/1350 [==============================] - 1s 416us/step - loss: 0.3714 - auc: 0.9109\n",
      "Epoch 113/175\n",
      "1350/1350 [==============================] - 1s 397us/step - loss: 0.3710 - auc: 0.9111\n",
      "Epoch 114/175\n",
      "1350/1350 [==============================] - 1s 400us/step - loss: 0.3706 - auc: 0.9112\n",
      "Epoch 115/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3702 - auc: 0.9114\n",
      "Epoch 116/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3697 - auc: 0.9117\n",
      "Epoch 117/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3693 - auc: 0.9119\n",
      "Epoch 118/175\n",
      "1350/1350 [==============================] - 1s 397us/step - loss: 0.3691 - auc: 0.9120\n",
      "Epoch 119/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3686 - auc: 0.9122\n",
      "Epoch 120/175\n",
      "1350/1350 [==============================] - 1s 403us/step - loss: 0.3682 - auc: 0.9125\n",
      "Epoch 121/175\n",
      "1350/1350 [==============================] - 1s 417us/step - loss: 0.3678 - auc: 0.9127\n",
      "Epoch 122/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3676 - auc: 0.9128\n",
      "Epoch 123/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3672 - auc: 0.9130\n",
      "Epoch 124/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3667 - auc: 0.9132\n",
      "Epoch 125/175\n",
      "1350/1350 [==============================] - 1s 397us/step - loss: 0.3664 - auc: 0.9134\n",
      "Epoch 126/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3661 - auc: 0.9135\n",
      "Epoch 127/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3658 - auc: 0.9137\n",
      "Epoch 128/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3654 - auc: 0.9140\n",
      "Epoch 129/175\n",
      "1350/1350 [==============================] - 1s 417us/step - loss: 0.3651 - auc: 0.9141\n",
      "Epoch 130/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3647 - auc: 0.9143\n",
      "Epoch 131/175\n",
      "1350/1350 [==============================] - 1s 400us/step - loss: 0.3644 - auc: 0.9145\n",
      "Epoch 132/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3641 - auc: 0.9146\n",
      "Epoch 133/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3636 - auc: 0.9148\n",
      "Epoch 134/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3633 - auc: 0.9150\n",
      "Epoch 135/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3630 - auc: 0.9151\n",
      "Epoch 136/175\n",
      "1350/1350 [==============================] - 1s 396us/step - loss: 0.3627 - auc: 0.9153\n",
      "Epoch 137/175\n",
      "1350/1350 [==============================] - 1s 415us/step - loss: 0.3623 - auc: 0.9155\n",
      "Epoch 138/175\n",
      "1350/1350 [==============================] - 1s 400us/step - loss: 0.3621 - auc: 0.9156\n",
      "Epoch 139/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3616 - auc: 0.9158\n",
      "Epoch 140/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3614 - auc: 0.9159\n",
      "Epoch 141/175\n",
      "1350/1350 [==============================] - 1s 403us/step - loss: 0.3611 - auc: 0.9161\n",
      "Epoch 142/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3608 - auc: 0.9163\n",
      "Epoch 143/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3606 - auc: 0.9164\n",
      "Epoch 144/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3602 - auc: 0.9165\n",
      "Epoch 145/175\n",
      "1350/1350 [==============================] - 1s 416us/step - loss: 0.3599 - auc: 0.9167\n",
      "Epoch 146/175\n",
      "1350/1350 [==============================] - 1s 435us/step - loss: 0.3596 - auc: 0.9168\n",
      "Epoch 147/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3593 - auc: 0.9170\n",
      "Epoch 148/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3590 - auc: 0.9171\n",
      "Epoch 149/175\n",
      "1350/1350 [==============================] - 1s 397us/step - loss: 0.3586 - auc: 0.9173\n",
      "Epoch 150/175\n",
      "1350/1350 [==============================] - 1s 400us/step - loss: 0.3584 - auc: 0.9174\n",
      "Epoch 151/175\n",
      "1350/1350 [==============================] - 1s 403us/step - loss: 0.3582 - auc: 0.9175\n",
      "Epoch 152/175\n",
      "1350/1350 [==============================] - 1s 418us/step - loss: 0.3579 - auc: 0.9177\n",
      "Epoch 153/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3576 - auc: 0.9178\n",
      "Epoch 154/175\n",
      "1350/1350 [==============================] - 1s 400us/step - loss: 0.3574 - auc: 0.9180\n",
      "Epoch 155/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3571 - auc: 0.9180\n",
      "Epoch 156/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3569 - auc: 0.9182\n",
      "Epoch 157/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3565 - auc: 0.9184\n",
      "Epoch 158/175\n",
      "1350/1350 [==============================] - 1s 397us/step - loss: 0.3563 - auc: 0.9184\n",
      "Epoch 159/175\n",
      "1350/1350 [==============================] - 1s 420us/step - loss: 0.3561 - auc: 0.9185\n",
      "Epoch 160/175\n",
      "1350/1350 [==============================] - 1s 403us/step - loss: 0.3558 - auc: 0.9187\n",
      "Epoch 161/175\n",
      "1350/1350 [==============================] - 1s 400us/step - loss: 0.3555 - auc: 0.9188\n",
      "Epoch 162/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3553 - auc: 0.9189\n",
      "Epoch 163/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3551 - auc: 0.9190\n",
      "Epoch 164/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3550 - auc: 0.9191\n",
      "Epoch 165/175\n",
      "1350/1350 [==============================] - 1s 401us/step - loss: 0.3547 - auc: 0.9192\n",
      "Epoch 166/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3544 - auc: 0.9193\n",
      "Epoch 167/175\n",
      "1350/1350 [==============================] - 1s 421us/step - loss: 0.3542 - auc: 0.9194\n",
      "Epoch 168/175\n",
      "1350/1350 [==============================] - 1s 400us/step - loss: 0.3540 - auc: 0.9195\n",
      "Epoch 169/175\n",
      "1350/1350 [==============================] - 1s 402us/step - loss: 0.3538 - auc: 0.9196\n",
      "Epoch 170/175\n",
      "1350/1350 [==============================] - 1s 398us/step - loss: 0.3536 - auc: 0.9197\n",
      "Epoch 171/175\n",
      "1350/1350 [==============================] - 1s 397us/step - loss: 0.3533 - auc: 0.9198\n",
      "Epoch 172/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3531 - auc: 0.9200\n",
      "Epoch 173/175\n",
      "1350/1350 [==============================] - 1s 399us/step - loss: 0.3529 - auc: 0.9201\n",
      "Epoch 174/175\n",
      "1350/1350 [==============================] - 1s 416us/step - loss: 0.3528 - auc: 0.9200\n",
      "Epoch 175/175\n",
      "1350/1350 [==============================] - 1s 401us/step - loss: 0.3526 - auc: 0.9202\n",
      "579/579 [==============================] - 0s 270us/step\n",
      "Parámetros: {'epochs': 175, 'optimizer': 'nadam'} \n",
      "F1 score:  0.826\n"
     ]
    }
   ],
   "source": [
    "modelo_cv = KerasClassifier(model=create_model)\n",
    "\n",
    "params_grid = {\n",
    "    'epochs': [175, 200],\n",
    "    #'neurons_per_layer': [3, 6, 9],\n",
    "    #'activation': ['relu', 'sigmoid'],\n",
    "    #'batch_size': [5],\n",
    "    'optimizer': ['nadam']\n",
    "}\n",
    "\n",
    "scorer_fn = make_scorer(f1_score)\n",
    "kfoldcv = StratifiedKFold(n_splits=2)\n",
    "\n",
    "gridcv = GridSearchCV(estimator=modelo_cv,\n",
    "                      param_grid = params_grid,\n",
    "                      scoring=scorer_fn,\n",
    "                      cv=kfoldcv,\n",
    "                      )\n",
    "\n",
    "model = gridcv.fit(x_train,y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "score = f1_score(y_test, y_pred)\n",
    "print(\"Parámetros:\", gridcv.best_params_, \"\\nF1 score: \", round(score, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/175\n",
      "540/540 [==============================] - 1s 682us/step - loss: 0.9240 - auc: 0.4661 - val_loss: 0.9189 - val_auc: 0.4969\n",
      "Epoch 2/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.9151 - auc: 0.5105 - val_loss: 0.9115 - val_auc: 0.5317\n",
      "Epoch 3/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.9082 - auc: 0.5473 - val_loss: 0.9050 - val_auc: 0.5674\n",
      "Epoch 4/175\n",
      "540/540 [==============================] - 0s 500us/step - loss: 0.9018 - auc: 0.5793 - val_loss: 0.8987 - val_auc: 0.5946\n",
      "Epoch 5/175\n",
      "540/540 [==============================] - 0s 549us/step - loss: 0.8954 - auc: 0.6077 - val_loss: 0.8923 - val_auc: 0.6225\n",
      "Epoch 6/175\n",
      "540/540 [==============================] - 0s 508us/step - loss: 0.8889 - auc: 0.6339 - val_loss: 0.8857 - val_auc: 0.6469\n",
      "Epoch 7/175\n",
      "540/540 [==============================] - 0s 507us/step - loss: 0.8820 - auc: 0.6577 - val_loss: 0.8788 - val_auc: 0.6699\n",
      "Epoch 8/175\n",
      "540/540 [==============================] - 0s 502us/step - loss: 0.8749 - auc: 0.6811 - val_loss: 0.8717 - val_auc: 0.6898\n",
      "Epoch 9/175\n",
      "540/540 [==============================] - 0s 548us/step - loss: 0.8673 - auc: 0.7038 - val_loss: 0.8639 - val_auc: 0.7110\n",
      "Epoch 10/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.8590 - auc: 0.7272 - val_loss: 0.8553 - val_auc: 0.7312\n",
      "Epoch 11/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.8495 - auc: 0.7502 - val_loss: 0.8454 - val_auc: 0.7545\n",
      "Epoch 12/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.8392 - auc: 0.7725 - val_loss: 0.8348 - val_auc: 0.7748\n",
      "Epoch 13/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.8284 - auc: 0.7910 - val_loss: 0.8239 - val_auc: 0.7919\n",
      "Epoch 14/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.8173 - auc: 0.8061 - val_loss: 0.8127 - val_auc: 0.8056\n",
      "Epoch 15/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.8060 - auc: 0.8166 - val_loss: 0.8012 - val_auc: 0.8149\n",
      "Epoch 16/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.7942 - auc: 0.8240 - val_loss: 0.7893 - val_auc: 0.8211\n",
      "Epoch 17/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.7821 - auc: 0.8299 - val_loss: 0.7771 - val_auc: 0.8257\n",
      "Epoch 18/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.7697 - auc: 0.8335 - val_loss: 0.7647 - val_auc: 0.8299\n",
      "Epoch 19/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.7572 - auc: 0.8369 - val_loss: 0.7520 - val_auc: 0.8325\n",
      "Epoch 20/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.7444 - auc: 0.8390 - val_loss: 0.7393 - val_auc: 0.8342\n",
      "Epoch 21/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.7317 - auc: 0.8407 - val_loss: 0.7265 - val_auc: 0.8364\n",
      "Epoch 22/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.7192 - auc: 0.8423 - val_loss: 0.7142 - val_auc: 0.8387\n",
      "Epoch 23/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.7072 - auc: 0.8440 - val_loss: 0.7025 - val_auc: 0.8406\n",
      "Epoch 24/175\n",
      "540/540 [==============================] - 0s 512us/step - loss: 0.6959 - auc: 0.8455 - val_loss: 0.6916 - val_auc: 0.8425\n",
      "Epoch 25/175\n",
      "540/540 [==============================] - 0s 502us/step - loss: 0.6853 - auc: 0.8473 - val_loss: 0.6815 - val_auc: 0.8443\n",
      "Epoch 26/175\n",
      "540/540 [==============================] - 0s 508us/step - loss: 0.6757 - auc: 0.8486 - val_loss: 0.6724 - val_auc: 0.8458\n",
      "Epoch 27/175\n",
      "540/540 [==============================] - 0s 520us/step - loss: 0.6669 - auc: 0.8500 - val_loss: 0.6641 - val_auc: 0.8474\n",
      "Epoch 28/175\n",
      "540/540 [==============================] - 0s 503us/step - loss: 0.6589 - auc: 0.8513 - val_loss: 0.6566 - val_auc: 0.8488\n",
      "Epoch 29/175\n",
      "540/540 [==============================] - 0s 519us/step - loss: 0.6517 - auc: 0.8525 - val_loss: 0.6495 - val_auc: 0.8502\n",
      "Epoch 30/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.6450 - auc: 0.8537 - val_loss: 0.6431 - val_auc: 0.8516\n",
      "Epoch 31/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.6389 - auc: 0.8549 - val_loss: 0.6373 - val_auc: 0.8528\n",
      "Epoch 32/175\n",
      "540/540 [==============================] - 0s 540us/step - loss: 0.6332 - auc: 0.8560 - val_loss: 0.6319 - val_auc: 0.8539\n",
      "Epoch 33/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.6279 - auc: 0.8571 - val_loss: 0.6269 - val_auc: 0.8551\n",
      "Epoch 34/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.6230 - auc: 0.8581 - val_loss: 0.6222 - val_auc: 0.8561\n",
      "Epoch 35/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.6185 - auc: 0.8590 - val_loss: 0.6177 - val_auc: 0.8572\n",
      "Epoch 36/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.6142 - auc: 0.8601 - val_loss: 0.6136 - val_auc: 0.8582\n",
      "Epoch 37/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.6101 - auc: 0.8610 - val_loss: 0.6098 - val_auc: 0.8592\n",
      "Epoch 38/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.6062 - auc: 0.8619 - val_loss: 0.6059 - val_auc: 0.8600\n",
      "Epoch 39/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.6025 - auc: 0.8628 - val_loss: 0.6024 - val_auc: 0.8609\n",
      "Epoch 40/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.5990 - auc: 0.8635 - val_loss: 0.5990 - val_auc: 0.8616\n",
      "Epoch 41/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.5956 - auc: 0.8644 - val_loss: 0.5957 - val_auc: 0.8623\n",
      "Epoch 42/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.5924 - auc: 0.8652 - val_loss: 0.5927 - val_auc: 0.8632\n",
      "Epoch 43/175\n",
      "540/540 [==============================] - 0s 507us/step - loss: 0.5892 - auc: 0.8658 - val_loss: 0.5896 - val_auc: 0.8638\n",
      "Epoch 44/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.5863 - auc: 0.8665 - val_loss: 0.5867 - val_auc: 0.8645\n",
      "Epoch 45/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.5834 - auc: 0.8672 - val_loss: 0.5838 - val_auc: 0.8651\n",
      "Epoch 46/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.5806 - auc: 0.8678 - val_loss: 0.5811 - val_auc: 0.8658\n",
      "Epoch 47/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.5779 - auc: 0.8683 - val_loss: 0.5785 - val_auc: 0.8663\n",
      "Epoch 48/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.5752 - auc: 0.8690 - val_loss: 0.5760 - val_auc: 0.8670\n",
      "Epoch 49/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.5727 - auc: 0.8695 - val_loss: 0.5735 - val_auc: 0.8675\n",
      "Epoch 50/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.5702 - auc: 0.8701 - val_loss: 0.5710 - val_auc: 0.8678\n",
      "Epoch 51/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.5677 - auc: 0.8706 - val_loss: 0.5687 - val_auc: 0.8684\n",
      "Epoch 52/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.5654 - auc: 0.8711 - val_loss: 0.5664 - val_auc: 0.8688\n",
      "Epoch 53/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.5631 - auc: 0.8715 - val_loss: 0.5642 - val_auc: 0.8693\n",
      "Epoch 54/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.5609 - auc: 0.8719 - val_loss: 0.5621 - val_auc: 0.8698\n",
      "Epoch 55/175\n",
      "540/540 [==============================] - 0s 537us/step - loss: 0.5587 - auc: 0.8725 - val_loss: 0.5601 - val_auc: 0.8700\n",
      "Epoch 56/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.5567 - auc: 0.8727 - val_loss: 0.5579 - val_auc: 0.8706\n",
      "Epoch 57/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.5546 - auc: 0.8731 - val_loss: 0.5559 - val_auc: 0.8710\n",
      "Epoch 58/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.5526 - auc: 0.8736 - val_loss: 0.5541 - val_auc: 0.8712\n",
      "Epoch 59/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.5507 - auc: 0.8739 - val_loss: 0.5522 - val_auc: 0.8716\n",
      "Epoch 60/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.5488 - auc: 0.8742 - val_loss: 0.5503 - val_auc: 0.8720\n",
      "Epoch 61/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.5469 - auc: 0.8747 - val_loss: 0.5485 - val_auc: 0.8722\n",
      "Epoch 62/175\n",
      "540/540 [==============================] - 0s 490us/step - loss: 0.5451 - auc: 0.8749 - val_loss: 0.5467 - val_auc: 0.8726\n",
      "Epoch 63/175\n",
      "540/540 [==============================] - 0s 488us/step - loss: 0.5433 - auc: 0.8753 - val_loss: 0.5450 - val_auc: 0.8729\n",
      "Epoch 64/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.5416 - auc: 0.8757 - val_loss: 0.5433 - val_auc: 0.8732\n",
      "Epoch 65/175\n",
      "540/540 [==============================] - 0s 489us/step - loss: 0.5399 - auc: 0.8758 - val_loss: 0.5416 - val_auc: 0.8736\n",
      "Epoch 66/175\n",
      "540/540 [==============================] - 0s 500us/step - loss: 0.5382 - auc: 0.8763 - val_loss: 0.5404 - val_auc: 0.8738\n",
      "Epoch 67/175\n",
      "540/540 [==============================] - 0s 488us/step - loss: 0.5367 - auc: 0.8764 - val_loss: 0.5384 - val_auc: 0.8741\n",
      "Epoch 68/175\n",
      "540/540 [==============================] - 0s 489us/step - loss: 0.5351 - auc: 0.8768 - val_loss: 0.5368 - val_auc: 0.8744\n",
      "Epoch 69/175\n",
      "540/540 [==============================] - 0s 488us/step - loss: 0.5335 - auc: 0.8770 - val_loss: 0.5354 - val_auc: 0.8746\n",
      "Epoch 70/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.5320 - auc: 0.8774 - val_loss: 0.5339 - val_auc: 0.8749\n",
      "Epoch 71/175\n",
      "540/540 [==============================] - 0s 489us/step - loss: 0.5305 - auc: 0.8776 - val_loss: 0.5325 - val_auc: 0.8751\n",
      "Epoch 72/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.5291 - auc: 0.8778 - val_loss: 0.5311 - val_auc: 0.8753\n",
      "Epoch 73/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.5277 - auc: 0.8780 - val_loss: 0.5297 - val_auc: 0.8757\n",
      "Epoch 74/175\n",
      "540/540 [==============================] - 0s 531us/step - loss: 0.5263 - auc: 0.8784 - val_loss: 0.5283 - val_auc: 0.8758\n",
      "Epoch 75/175\n",
      "540/540 [==============================] - 0s 482us/step - loss: 0.5250 - auc: 0.8785 - val_loss: 0.5270 - val_auc: 0.8761\n",
      "Epoch 76/175\n",
      "540/540 [==============================] - 0s 480us/step - loss: 0.5236 - auc: 0.8787 - val_loss: 0.5256 - val_auc: 0.8763\n",
      "Epoch 77/175\n",
      "540/540 [==============================] - 0s 481us/step - loss: 0.5223 - auc: 0.8791 - val_loss: 0.5244 - val_auc: 0.8765\n",
      "Epoch 78/175\n",
      "540/540 [==============================] - 0s 481us/step - loss: 0.5210 - auc: 0.8793 - val_loss: 0.5231 - val_auc: 0.8767\n",
      "Epoch 79/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.5197 - auc: 0.8795 - val_loss: 0.5219 - val_auc: 0.8769\n",
      "Epoch 80/175\n",
      "540/540 [==============================] - 0s 480us/step - loss: 0.5185 - auc: 0.8798 - val_loss: 0.5208 - val_auc: 0.8772\n",
      "Epoch 81/175\n",
      "540/540 [==============================] - 0s 481us/step - loss: 0.5172 - auc: 0.8800 - val_loss: 0.5195 - val_auc: 0.8773\n",
      "Epoch 82/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.5159 - auc: 0.8803 - val_loss: 0.5184 - val_auc: 0.8775\n",
      "Epoch 83/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.5149 - auc: 0.8803 - val_loss: 0.5171 - val_auc: 0.8778\n",
      "Epoch 84/175\n",
      "540/540 [==============================] - 0s 479us/step - loss: 0.5137 - auc: 0.8807 - val_loss: 0.5160 - val_auc: 0.8780\n",
      "Epoch 85/175\n",
      "540/540 [==============================] - 0s 481us/step - loss: 0.5126 - auc: 0.8808 - val_loss: 0.5149 - val_auc: 0.8782\n",
      "Epoch 86/175\n",
      "540/540 [==============================] - 0s 478us/step - loss: 0.5114 - auc: 0.8811 - val_loss: 0.5138 - val_auc: 0.8785\n",
      "Epoch 87/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.5103 - auc: 0.8813 - val_loss: 0.5127 - val_auc: 0.8787\n",
      "Epoch 88/175\n",
      "540/540 [==============================] - 0s 481us/step - loss: 0.5092 - auc: 0.8815 - val_loss: 0.5116 - val_auc: 0.8791\n",
      "Epoch 89/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.5082 - auc: 0.8817 - val_loss: 0.5105 - val_auc: 0.8794\n",
      "Epoch 90/175\n",
      "540/540 [==============================] - 0s 482us/step - loss: 0.5071 - auc: 0.8820 - val_loss: 0.5095 - val_auc: 0.8794\n",
      "Epoch 91/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.5060 - auc: 0.8823 - val_loss: 0.5086 - val_auc: 0.8796\n",
      "Epoch 92/175\n",
      "540/540 [==============================] - 0s 479us/step - loss: 0.5051 - auc: 0.8824 - val_loss: 0.5075 - val_auc: 0.8799\n",
      "Epoch 93/175\n",
      "540/540 [==============================] - 0s 537us/step - loss: 0.5041 - auc: 0.8827 - val_loss: 0.5065 - val_auc: 0.8801\n",
      "Epoch 94/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.5030 - auc: 0.8830 - val_loss: 0.5057 - val_auc: 0.8804\n",
      "Epoch 95/175\n",
      "540/540 [==============================] - 0s 489us/step - loss: 0.5022 - auc: 0.8830 - val_loss: 0.5046 - val_auc: 0.8806\n",
      "Epoch 96/175\n",
      "540/540 [==============================] - 0s 488us/step - loss: 0.5011 - auc: 0.8834 - val_loss: 0.5037 - val_auc: 0.8808\n",
      "Epoch 97/175\n",
      "540/540 [==============================] - 0s 487us/step - loss: 0.5003 - auc: 0.8835 - val_loss: 0.5030 - val_auc: 0.8809\n",
      "Epoch 98/175\n",
      "540/540 [==============================] - 0s 482us/step - loss: 0.4994 - auc: 0.8837 - val_loss: 0.5022 - val_auc: 0.8811\n",
      "Epoch 99/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.4986 - auc: 0.8837 - val_loss: 0.5011 - val_auc: 0.8814\n",
      "Epoch 100/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.4977 - auc: 0.8840 - val_loss: 0.5003 - val_auc: 0.8815\n",
      "Epoch 101/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4968 - auc: 0.8843 - val_loss: 0.4995 - val_auc: 0.8817\n",
      "Epoch 102/175\n",
      "540/540 [==============================] - 0s 487us/step - loss: 0.4960 - auc: 0.8844 - val_loss: 0.4987 - val_auc: 0.8818\n",
      "Epoch 103/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.4952 - auc: 0.8845 - val_loss: 0.4979 - val_auc: 0.8820\n",
      "Epoch 104/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.4943 - auc: 0.8847 - val_loss: 0.4972 - val_auc: 0.8822\n",
      "Epoch 105/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.4936 - auc: 0.8849 - val_loss: 0.4963 - val_auc: 0.8823\n",
      "Epoch 106/175\n",
      "540/540 [==============================] - 0s 486us/step - loss: 0.4928 - auc: 0.8850 - val_loss: 0.4955 - val_auc: 0.8825\n",
      "Epoch 107/175\n",
      "540/540 [==============================] - 0s 504us/step - loss: 0.4921 - auc: 0.8852 - val_loss: 0.4948 - val_auc: 0.8826\n",
      "Epoch 108/175\n",
      "540/540 [==============================] - 0s 490us/step - loss: 0.4913 - auc: 0.8854 - val_loss: 0.4940 - val_auc: 0.8829\n",
      "Epoch 109/175\n",
      "540/540 [==============================] - 0s 535us/step - loss: 0.4905 - auc: 0.8856 - val_loss: 0.4934 - val_auc: 0.8830\n",
      "Epoch 110/175\n",
      "540/540 [==============================] - 0s 487us/step - loss: 0.4899 - auc: 0.8857 - val_loss: 0.4926 - val_auc: 0.8832\n",
      "Epoch 111/175\n",
      "540/540 [==============================] - 0s 488us/step - loss: 0.4891 - auc: 0.8859 - val_loss: 0.4920 - val_auc: 0.8833\n",
      "Epoch 112/175\n",
      "540/540 [==============================] - 0s 489us/step - loss: 0.4884 - auc: 0.8860 - val_loss: 0.4913 - val_auc: 0.8836\n",
      "Epoch 113/175\n",
      "540/540 [==============================] - 0s 487us/step - loss: 0.4877 - auc: 0.8863 - val_loss: 0.4906 - val_auc: 0.8836\n",
      "Epoch 114/175\n",
      "540/540 [==============================] - 0s 489us/step - loss: 0.4870 - auc: 0.8863 - val_loss: 0.4898 - val_auc: 0.8839\n",
      "Epoch 115/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4863 - auc: 0.8867 - val_loss: 0.4893 - val_auc: 0.8840\n",
      "Epoch 116/175\n",
      "540/540 [==============================] - 0s 490us/step - loss: 0.4857 - auc: 0.8866 - val_loss: 0.4885 - val_auc: 0.8843\n",
      "Epoch 117/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4850 - auc: 0.8870 - val_loss: 0.4879 - val_auc: 0.8844\n",
      "Epoch 118/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4844 - auc: 0.8871 - val_loss: 0.4873 - val_auc: 0.8844\n",
      "Epoch 119/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4837 - auc: 0.8872 - val_loss: 0.4867 - val_auc: 0.8846\n",
      "Epoch 120/175\n",
      "540/540 [==============================] - 0s 488us/step - loss: 0.4831 - auc: 0.8874 - val_loss: 0.4861 - val_auc: 0.8847\n",
      "Epoch 121/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4825 - auc: 0.8875 - val_loss: 0.4855 - val_auc: 0.8849\n",
      "Epoch 122/175\n",
      "540/540 [==============================] - 0s 539us/step - loss: 0.4819 - auc: 0.8877 - val_loss: 0.4849 - val_auc: 0.8850\n",
      "Epoch 123/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4814 - auc: 0.8878 - val_loss: 0.4843 - val_auc: 0.8852\n",
      "Epoch 124/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4807 - auc: 0.8880 - val_loss: 0.4838 - val_auc: 0.8854\n",
      "Epoch 125/175\n",
      "540/540 [==============================] - 0s 489us/step - loss: 0.4802 - auc: 0.8882 - val_loss: 0.4831 - val_auc: 0.8855\n",
      "Epoch 126/175\n",
      "540/540 [==============================] - 0s 501us/step - loss: 0.4796 - auc: 0.8883 - val_loss: 0.4826 - val_auc: 0.8856\n",
      "Epoch 127/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4790 - auc: 0.8884 - val_loss: 0.4819 - val_auc: 0.8859\n",
      "Epoch 128/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4785 - auc: 0.8887 - val_loss: 0.4814 - val_auc: 0.8861\n",
      "Epoch 129/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4779 - auc: 0.8888 - val_loss: 0.4810 - val_auc: 0.8862\n",
      "Epoch 130/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.4774 - auc: 0.8889 - val_loss: 0.4803 - val_auc: 0.8864\n",
      "Epoch 131/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4768 - auc: 0.8891 - val_loss: 0.4798 - val_auc: 0.8865\n",
      "Epoch 132/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4762 - auc: 0.8893 - val_loss: 0.4793 - val_auc: 0.8866\n",
      "Epoch 133/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4757 - auc: 0.8895 - val_loss: 0.4788 - val_auc: 0.8868\n",
      "Epoch 134/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4752 - auc: 0.8895 - val_loss: 0.4782 - val_auc: 0.8870\n",
      "Epoch 135/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4747 - auc: 0.8897 - val_loss: 0.4777 - val_auc: 0.8871\n",
      "Epoch 136/175\n",
      "540/540 [==============================] - 0s 540us/step - loss: 0.4741 - auc: 0.8899 - val_loss: 0.4772 - val_auc: 0.8873\n",
      "Epoch 137/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4737 - auc: 0.8900 - val_loss: 0.4768 - val_auc: 0.8875\n",
      "Epoch 138/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4732 - auc: 0.8901 - val_loss: 0.4762 - val_auc: 0.8876\n",
      "Epoch 139/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4727 - auc: 0.8903 - val_loss: 0.4759 - val_auc: 0.8877\n",
      "Epoch 140/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4723 - auc: 0.8904 - val_loss: 0.4753 - val_auc: 0.8879\n",
      "Epoch 141/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4717 - auc: 0.8906 - val_loss: 0.4750 - val_auc: 0.8879\n",
      "Epoch 142/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4713 - auc: 0.8907 - val_loss: 0.4744 - val_auc: 0.8881\n",
      "Epoch 143/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4708 - auc: 0.8909 - val_loss: 0.4740 - val_auc: 0.8883\n",
      "Epoch 144/175\n",
      "540/540 [==============================] - 0s 507us/step - loss: 0.4704 - auc: 0.8909 - val_loss: 0.4735 - val_auc: 0.8884\n",
      "Epoch 145/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4699 - auc: 0.8912 - val_loss: 0.4730 - val_auc: 0.8886\n",
      "Epoch 146/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4695 - auc: 0.8914 - val_loss: 0.4726 - val_auc: 0.8886\n",
      "Epoch 147/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4690 - auc: 0.8914 - val_loss: 0.4721 - val_auc: 0.8889\n",
      "Epoch 148/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4686 - auc: 0.8916 - val_loss: 0.4717 - val_auc: 0.8890\n",
      "Epoch 149/175\n",
      "540/540 [==============================] - 0s 535us/step - loss: 0.4682 - auc: 0.8917 - val_loss: 0.4713 - val_auc: 0.8891\n",
      "Epoch 150/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4678 - auc: 0.8918 - val_loss: 0.4708 - val_auc: 0.8894\n",
      "Epoch 151/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4674 - auc: 0.8920 - val_loss: 0.4704 - val_auc: 0.8895\n",
      "Epoch 152/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4670 - auc: 0.8922 - val_loss: 0.4702 - val_auc: 0.8896\n",
      "Epoch 153/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4666 - auc: 0.8922 - val_loss: 0.4697 - val_auc: 0.8896\n",
      "Epoch 154/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4662 - auc: 0.8923 - val_loss: 0.4693 - val_auc: 0.8899\n",
      "Epoch 155/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4658 - auc: 0.8926 - val_loss: 0.4690 - val_auc: 0.8899\n",
      "Epoch 156/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4654 - auc: 0.8926 - val_loss: 0.4685 - val_auc: 0.8901\n",
      "Epoch 157/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4649 - auc: 0.8929 - val_loss: 0.4683 - val_auc: 0.8902\n",
      "Epoch 158/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4646 - auc: 0.8929 - val_loss: 0.4679 - val_auc: 0.8903\n",
      "Epoch 159/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4642 - auc: 0.8931 - val_loss: 0.4674 - val_auc: 0.8904\n",
      "Epoch 160/175\n",
      "540/540 [==============================] - 0s 501us/step - loss: 0.4639 - auc: 0.8931 - val_loss: 0.4671 - val_auc: 0.8907\n",
      "Epoch 161/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4635 - auc: 0.8933 - val_loss: 0.4667 - val_auc: 0.8908\n",
      "Epoch 162/175\n",
      "540/540 [==============================] - 0s 534us/step - loss: 0.4631 - auc: 0.8935 - val_loss: 0.4663 - val_auc: 0.8909\n",
      "Epoch 163/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4628 - auc: 0.8936 - val_loss: 0.4662 - val_auc: 0.8910\n",
      "Epoch 164/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4624 - auc: 0.8937 - val_loss: 0.4658 - val_auc: 0.8912\n",
      "Epoch 165/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4622 - auc: 0.8938 - val_loss: 0.4653 - val_auc: 0.8912\n",
      "Epoch 166/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4618 - auc: 0.8940 - val_loss: 0.4650 - val_auc: 0.8913\n",
      "Epoch 167/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4614 - auc: 0.8941 - val_loss: 0.4648 - val_auc: 0.8915\n",
      "Epoch 168/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4611 - auc: 0.8941 - val_loss: 0.4645 - val_auc: 0.8916\n",
      "Epoch 169/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4608 - auc: 0.8942 - val_loss: 0.4640 - val_auc: 0.8918\n",
      "Epoch 170/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4605 - auc: 0.8944 - val_loss: 0.4636 - val_auc: 0.8920\n",
      "Epoch 171/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4601 - auc: 0.8946 - val_loss: 0.4634 - val_auc: 0.8920\n",
      "Epoch 172/175\n",
      "540/540 [==============================] - 0s 540us/step - loss: 0.4597 - auc: 0.8946 - val_loss: 0.4631 - val_auc: 0.8922\n",
      "Epoch 173/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4595 - auc: 0.8948 - val_loss: 0.4628 - val_auc: 0.8922\n",
      "Epoch 174/175\n",
      "540/540 [==============================] - 0s 500us/step - loss: 0.4591 - auc: 0.8950 - val_loss: 0.4624 - val_auc: 0.8924\n",
      "Epoch 175/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4589 - auc: 0.8951 - val_loss: 0.4621 - val_auc: 0.8925\n",
      "675/675 [==============================] - 0s 267us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/175\n",
      "540/540 [==============================] - 1s 686us/step - loss: 0.9195 - auc: 0.5252 - val_loss: 0.9152 - val_auc: 0.5652\n",
      "Epoch 2/175\n",
      "540/540 [==============================] - 0s 499us/step - loss: 0.9116 - auc: 0.5852 - val_loss: 0.9068 - val_auc: 0.6256\n",
      "Epoch 3/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.9029 - auc: 0.6396 - val_loss: 0.8974 - val_auc: 0.6777\n",
      "Epoch 4/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.8934 - auc: 0.6834 - val_loss: 0.8877 - val_auc: 0.7125\n",
      "Epoch 5/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.8839 - auc: 0.7111 - val_loss: 0.8781 - val_auc: 0.7339\n",
      "Epoch 6/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.8741 - auc: 0.7319 - val_loss: 0.8681 - val_auc: 0.7508\n",
      "Epoch 7/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.8638 - auc: 0.7481 - val_loss: 0.8575 - val_auc: 0.7630\n",
      "Epoch 8/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.8529 - auc: 0.7617 - val_loss: 0.8466 - val_auc: 0.7747\n",
      "Epoch 9/175\n",
      "540/540 [==============================] - 0s 500us/step - loss: 0.8416 - auc: 0.7730 - val_loss: 0.8351 - val_auc: 0.7839\n",
      "Epoch 10/175\n",
      "540/540 [==============================] - 0s 501us/step - loss: 0.8299 - auc: 0.7826 - val_loss: 0.8235 - val_auc: 0.7915\n",
      "Epoch 11/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.8180 - auc: 0.7902 - val_loss: 0.8115 - val_auc: 0.7979\n",
      "Epoch 12/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.8058 - auc: 0.7968 - val_loss: 0.7995 - val_auc: 0.8036\n",
      "Epoch 13/175\n",
      "540/540 [==============================] - 0s 552us/step - loss: 0.7937 - auc: 0.8022 - val_loss: 0.7875 - val_auc: 0.8080\n",
      "Epoch 14/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.7817 - auc: 0.8072 - val_loss: 0.7756 - val_auc: 0.8123\n",
      "Epoch 15/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.7699 - auc: 0.8116 - val_loss: 0.7640 - val_auc: 0.8160\n",
      "Epoch 16/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.7584 - auc: 0.8153 - val_loss: 0.7528 - val_auc: 0.8191\n",
      "Epoch 17/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.7472 - auc: 0.8185 - val_loss: 0.7420 - val_auc: 0.8221\n",
      "Epoch 18/175\n",
      "540/540 [==============================] - 0s 502us/step - loss: 0.7365 - auc: 0.8215 - val_loss: 0.7316 - val_auc: 0.8246\n",
      "Epoch 19/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.7263 - auc: 0.8242 - val_loss: 0.7216 - val_auc: 0.8273\n",
      "Epoch 20/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.7165 - auc: 0.8269 - val_loss: 0.7122 - val_auc: 0.8294\n",
      "Epoch 21/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.7072 - auc: 0.8293 - val_loss: 0.7033 - val_auc: 0.8317\n",
      "Epoch 22/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.6985 - auc: 0.8317 - val_loss: 0.6949 - val_auc: 0.8337\n",
      "Epoch 23/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.6902 - auc: 0.8336 - val_loss: 0.6869 - val_auc: 0.8357\n",
      "Epoch 24/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.6825 - auc: 0.8355 - val_loss: 0.6795 - val_auc: 0.8375\n",
      "Epoch 25/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.6752 - auc: 0.8372 - val_loss: 0.6725 - val_auc: 0.8392\n",
      "Epoch 26/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.6683 - auc: 0.8388 - val_loss: 0.6660 - val_auc: 0.8406\n",
      "Epoch 27/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.6618 - auc: 0.8404 - val_loss: 0.6596 - val_auc: 0.8422\n",
      "Epoch 28/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.6557 - auc: 0.8419 - val_loss: 0.6538 - val_auc: 0.8434\n",
      "Epoch 29/175\n",
      "540/540 [==============================] - 0s 504us/step - loss: 0.6498 - auc: 0.8433 - val_loss: 0.6483 - val_auc: 0.8450\n",
      "Epoch 30/175\n",
      "540/540 [==============================] - 0s 499us/step - loss: 0.6443 - auc: 0.8449 - val_loss: 0.6428 - val_auc: 0.8463\n",
      "Epoch 31/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.6391 - auc: 0.8461 - val_loss: 0.6378 - val_auc: 0.8477\n",
      "Epoch 32/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.6340 - auc: 0.8474 - val_loss: 0.6329 - val_auc: 0.8489\n",
      "Epoch 33/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.6292 - auc: 0.8486 - val_loss: 0.6283 - val_auc: 0.8501\n",
      "Epoch 34/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.6246 - auc: 0.8500 - val_loss: 0.6239 - val_auc: 0.8513\n",
      "Epoch 35/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.6203 - auc: 0.8509 - val_loss: 0.6197 - val_auc: 0.8525\n",
      "Epoch 36/175\n",
      "540/540 [==============================] - 0s 501us/step - loss: 0.6160 - auc: 0.8521 - val_loss: 0.6156 - val_auc: 0.8536\n",
      "Epoch 37/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.6119 - auc: 0.8533 - val_loss: 0.6117 - val_auc: 0.8547\n",
      "Epoch 38/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.6080 - auc: 0.8544 - val_loss: 0.6078 - val_auc: 0.8558\n",
      "Epoch 39/175\n",
      "540/540 [==============================] - 0s 546us/step - loss: 0.6043 - auc: 0.8554 - val_loss: 0.6042 - val_auc: 0.8569\n",
      "Epoch 40/175\n",
      "540/540 [==============================] - 0s 505us/step - loss: 0.6006 - auc: 0.8565 - val_loss: 0.6008 - val_auc: 0.8577\n",
      "Epoch 41/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.5971 - auc: 0.8575 - val_loss: 0.5977 - val_auc: 0.8587\n",
      "Epoch 42/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.5938 - auc: 0.8583 - val_loss: 0.5942 - val_auc: 0.8595\n",
      "Epoch 43/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.5905 - auc: 0.8593 - val_loss: 0.5913 - val_auc: 0.8604\n",
      "Epoch 44/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.5873 - auc: 0.8603 - val_loss: 0.5878 - val_auc: 0.8612\n",
      "Epoch 45/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.5843 - auc: 0.8609 - val_loss: 0.5848 - val_auc: 0.8620\n",
      "Epoch 46/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.5814 - auc: 0.8618 - val_loss: 0.5822 - val_auc: 0.8628\n",
      "Epoch 47/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.5785 - auc: 0.8626 - val_loss: 0.5794 - val_auc: 0.8634\n",
      "Epoch 48/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.5757 - auc: 0.8634 - val_loss: 0.5768 - val_auc: 0.8643\n",
      "Epoch 49/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.5730 - auc: 0.8641 - val_loss: 0.5740 - val_auc: 0.8647\n",
      "Epoch 50/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.5704 - auc: 0.8649 - val_loss: 0.5718 - val_auc: 0.8655\n",
      "Epoch 51/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.5678 - auc: 0.8657 - val_loss: 0.5693 - val_auc: 0.8661\n",
      "Epoch 52/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.5653 - auc: 0.8663 - val_loss: 0.5667 - val_auc: 0.8667\n",
      "Epoch 53/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.5629 - auc: 0.8671 - val_loss: 0.5643 - val_auc: 0.8672\n",
      "Epoch 54/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.5606 - auc: 0.8676 - val_loss: 0.5621 - val_auc: 0.8678\n",
      "Epoch 55/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.5583 - auc: 0.8682 - val_loss: 0.5600 - val_auc: 0.8683\n",
      "Epoch 56/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.5560 - auc: 0.8688 - val_loss: 0.5578 - val_auc: 0.8688\n",
      "Epoch 57/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.5539 - auc: 0.8694 - val_loss: 0.5558 - val_auc: 0.8695\n",
      "Epoch 58/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.5517 - auc: 0.8701 - val_loss: 0.5537 - val_auc: 0.8699\n",
      "Epoch 59/175\n",
      "540/540 [==============================] - 0s 511us/step - loss: 0.5496 - auc: 0.8706 - val_loss: 0.5517 - val_auc: 0.8704\n",
      "Epoch 60/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.5476 - auc: 0.8711 - val_loss: 0.5498 - val_auc: 0.8708\n",
      "Epoch 61/175\n",
      "540/540 [==============================] - 0s 547us/step - loss: 0.5456 - auc: 0.8717 - val_loss: 0.5482 - val_auc: 0.8712\n",
      "Epoch 62/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.5437 - auc: 0.8722 - val_loss: 0.5461 - val_auc: 0.8717\n",
      "Epoch 63/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.5418 - auc: 0.8728 - val_loss: 0.5442 - val_auc: 0.8722\n",
      "Epoch 64/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.5400 - auc: 0.8733 - val_loss: 0.5426 - val_auc: 0.8725\n",
      "Epoch 65/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.5381 - auc: 0.8739 - val_loss: 0.5408 - val_auc: 0.8728\n",
      "Epoch 66/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.5364 - auc: 0.8742 - val_loss: 0.5394 - val_auc: 0.8734\n",
      "Epoch 67/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.5346 - auc: 0.8748 - val_loss: 0.5376 - val_auc: 0.8737\n",
      "Epoch 68/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.5329 - auc: 0.8753 - val_loss: 0.5363 - val_auc: 0.8742\n",
      "Epoch 69/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.5313 - auc: 0.8759 - val_loss: 0.5347 - val_auc: 0.8746\n",
      "Epoch 70/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.5297 - auc: 0.8762 - val_loss: 0.5329 - val_auc: 0.8748\n",
      "Epoch 71/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.5280 - auc: 0.8768 - val_loss: 0.5316 - val_auc: 0.8752\n",
      "Epoch 72/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.5265 - auc: 0.8773 - val_loss: 0.5300 - val_auc: 0.8755\n",
      "Epoch 73/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.5249 - auc: 0.8778 - val_loss: 0.5284 - val_auc: 0.8759\n",
      "Epoch 74/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.5234 - auc: 0.8782 - val_loss: 0.5271 - val_auc: 0.8762\n",
      "Epoch 75/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.5220 - auc: 0.8786 - val_loss: 0.5256 - val_auc: 0.8767\n",
      "Epoch 76/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.5205 - auc: 0.8791 - val_loss: 0.5246 - val_auc: 0.8770\n",
      "Epoch 77/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.5191 - auc: 0.8795 - val_loss: 0.5230 - val_auc: 0.8774\n",
      "Epoch 78/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.5177 - auc: 0.8800 - val_loss: 0.5216 - val_auc: 0.8779\n",
      "Epoch 79/175\n",
      "540/540 [==============================] - 0s 544us/step - loss: 0.5163 - auc: 0.8805 - val_loss: 0.5204 - val_auc: 0.8782\n",
      "Epoch 80/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.5150 - auc: 0.8809 - val_loss: 0.5190 - val_auc: 0.8786\n",
      "Epoch 81/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.5137 - auc: 0.8814 - val_loss: 0.5177 - val_auc: 0.8790\n",
      "Epoch 82/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.5124 - auc: 0.8817 - val_loss: 0.5165 - val_auc: 0.8793\n",
      "Epoch 83/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.5112 - auc: 0.8822 - val_loss: 0.5153 - val_auc: 0.8798\n",
      "Epoch 84/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.5099 - auc: 0.8827 - val_loss: 0.5145 - val_auc: 0.8800\n",
      "Epoch 85/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.5087 - auc: 0.8830 - val_loss: 0.5133 - val_auc: 0.8804\n",
      "Epoch 86/175\n",
      "540/540 [==============================] - 0s 500us/step - loss: 0.5075 - auc: 0.8835 - val_loss: 0.5118 - val_auc: 0.8809\n",
      "Epoch 87/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.5064 - auc: 0.8839 - val_loss: 0.5107 - val_auc: 0.8812\n",
      "Epoch 88/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.5052 - auc: 0.8843 - val_loss: 0.5098 - val_auc: 0.8813\n",
      "Epoch 89/175\n",
      "540/540 [==============================] - 0s 507us/step - loss: 0.5041 - auc: 0.8847 - val_loss: 0.5091 - val_auc: 0.8816\n",
      "Epoch 90/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.5030 - auc: 0.8851 - val_loss: 0.5077 - val_auc: 0.8821\n",
      "Epoch 91/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.5019 - auc: 0.8853 - val_loss: 0.5069 - val_auc: 0.8824\n",
      "Epoch 92/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.5009 - auc: 0.8858 - val_loss: 0.5061 - val_auc: 0.8826\n",
      "Epoch 93/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.4998 - auc: 0.8861 - val_loss: 0.5046 - val_auc: 0.8831\n",
      "Epoch 94/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4989 - auc: 0.8866 - val_loss: 0.5041 - val_auc: 0.8831\n",
      "Epoch 95/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4978 - auc: 0.8868 - val_loss: 0.5033 - val_auc: 0.8834\n",
      "Epoch 96/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.4969 - auc: 0.8871 - val_loss: 0.5019 - val_auc: 0.8839\n",
      "Epoch 97/175\n",
      "540/540 [==============================] - 0s 544us/step - loss: 0.4959 - auc: 0.8875 - val_loss: 0.5010 - val_auc: 0.8842\n",
      "Epoch 98/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.4950 - auc: 0.8879 - val_loss: 0.5004 - val_auc: 0.8844\n",
      "Epoch 99/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.4941 - auc: 0.8882 - val_loss: 0.4996 - val_auc: 0.8845\n",
      "Epoch 100/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.4932 - auc: 0.8885 - val_loss: 0.4993 - val_auc: 0.8847\n",
      "Epoch 101/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.4923 - auc: 0.8886 - val_loss: 0.4981 - val_auc: 0.8852\n",
      "Epoch 102/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.4914 - auc: 0.8891 - val_loss: 0.4970 - val_auc: 0.8856\n",
      "Epoch 103/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.4906 - auc: 0.8895 - val_loss: 0.4960 - val_auc: 0.8858\n",
      "Epoch 104/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4898 - auc: 0.8896 - val_loss: 0.4953 - val_auc: 0.8861\n",
      "Epoch 105/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4890 - auc: 0.8901 - val_loss: 0.4945 - val_auc: 0.8863\n",
      "Epoch 106/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.4881 - auc: 0.8902 - val_loss: 0.4937 - val_auc: 0.8866\n",
      "Epoch 107/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.4873 - auc: 0.8905 - val_loss: 0.4928 - val_auc: 0.8870\n",
      "Epoch 108/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4866 - auc: 0.8908 - val_loss: 0.4923 - val_auc: 0.8871\n",
      "Epoch 109/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.4858 - auc: 0.8912 - val_loss: 0.4913 - val_auc: 0.8874\n",
      "Epoch 110/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4850 - auc: 0.8913 - val_loss: 0.4911 - val_auc: 0.8875\n",
      "Epoch 111/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4843 - auc: 0.8916 - val_loss: 0.4902 - val_auc: 0.8879\n",
      "Epoch 112/175\n",
      "540/540 [==============================] - 0s 507us/step - loss: 0.4835 - auc: 0.8920 - val_loss: 0.4893 - val_auc: 0.8882\n",
      "Epoch 113/175\n",
      "540/540 [==============================] - 0s 542us/step - loss: 0.4828 - auc: 0.8922 - val_loss: 0.4889 - val_auc: 0.8883\n",
      "Epoch 114/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4820 - auc: 0.8927 - val_loss: 0.4880 - val_auc: 0.8886\n",
      "Epoch 115/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.4813 - auc: 0.8929 - val_loss: 0.4874 - val_auc: 0.8887\n",
      "Epoch 116/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.4806 - auc: 0.8932 - val_loss: 0.4870 - val_auc: 0.8888\n",
      "Epoch 117/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4799 - auc: 0.8933 - val_loss: 0.4861 - val_auc: 0.8894\n",
      "Epoch 118/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.4792 - auc: 0.8937 - val_loss: 0.4853 - val_auc: 0.8897\n",
      "Epoch 119/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.4786 - auc: 0.8939 - val_loss: 0.4850 - val_auc: 0.8898\n",
      "Epoch 120/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.4778 - auc: 0.8943 - val_loss: 0.4841 - val_auc: 0.8900\n",
      "Epoch 121/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.4772 - auc: 0.8945 - val_loss: 0.4840 - val_auc: 0.8901\n",
      "Epoch 122/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4766 - auc: 0.8947 - val_loss: 0.4831 - val_auc: 0.8905\n",
      "Epoch 123/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4759 - auc: 0.8950 - val_loss: 0.4822 - val_auc: 0.8908\n",
      "Epoch 124/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4752 - auc: 0.8952 - val_loss: 0.4826 - val_auc: 0.8907\n",
      "Epoch 125/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.4747 - auc: 0.8953 - val_loss: 0.4811 - val_auc: 0.8909\n",
      "Epoch 126/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4742 - auc: 0.8955 - val_loss: 0.4805 - val_auc: 0.8911\n",
      "Epoch 127/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.4736 - auc: 0.8954 - val_loss: 0.4801 - val_auc: 0.8914\n",
      "Epoch 128/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4730 - auc: 0.8958 - val_loss: 0.4796 - val_auc: 0.8914\n",
      "Epoch 129/175\n",
      "540/540 [==============================] - 0s 541us/step - loss: 0.4724 - auc: 0.8959 - val_loss: 0.4789 - val_auc: 0.8918\n",
      "Epoch 130/175\n",
      "540/540 [==============================] - 0s 499us/step - loss: 0.4719 - auc: 0.8961 - val_loss: 0.4785 - val_auc: 0.8917\n",
      "Epoch 131/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.4714 - auc: 0.8961 - val_loss: 0.4780 - val_auc: 0.8919\n",
      "Epoch 132/175\n",
      "540/540 [==============================] - 0s 511us/step - loss: 0.4709 - auc: 0.8964 - val_loss: 0.4776 - val_auc: 0.8921\n",
      "Epoch 133/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.4703 - auc: 0.8967 - val_loss: 0.4781 - val_auc: 0.8920\n",
      "Epoch 134/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.4700 - auc: 0.8965 - val_loss: 0.4764 - val_auc: 0.8923\n",
      "Epoch 135/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.4694 - auc: 0.8969 - val_loss: 0.4767 - val_auc: 0.8923\n",
      "Epoch 136/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.4689 - auc: 0.8970 - val_loss: 0.4758 - val_auc: 0.8924\n",
      "Epoch 137/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.4684 - auc: 0.8970 - val_loss: 0.4751 - val_auc: 0.8925\n",
      "Epoch 138/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4679 - auc: 0.8972 - val_loss: 0.4747 - val_auc: 0.8927\n",
      "Epoch 139/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.4675 - auc: 0.8973 - val_loss: 0.4746 - val_auc: 0.8928\n",
      "Epoch 140/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4671 - auc: 0.8973 - val_loss: 0.4737 - val_auc: 0.8930\n",
      "Epoch 141/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4666 - auc: 0.8976 - val_loss: 0.4734 - val_auc: 0.8931\n",
      "Epoch 142/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4662 - auc: 0.8976 - val_loss: 0.4731 - val_auc: 0.8932\n",
      "Epoch 143/175\n",
      "540/540 [==============================] - 0s 544us/step - loss: 0.4658 - auc: 0.8978 - val_loss: 0.4724 - val_auc: 0.8933\n",
      "Epoch 144/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.4654 - auc: 0.8977 - val_loss: 0.4722 - val_auc: 0.8934\n",
      "Epoch 145/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.4650 - auc: 0.8979 - val_loss: 0.4720 - val_auc: 0.8936\n",
      "Epoch 146/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.4646 - auc: 0.8980 - val_loss: 0.4715 - val_auc: 0.8937\n",
      "Epoch 147/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4641 - auc: 0.8982 - val_loss: 0.4712 - val_auc: 0.8937\n",
      "Epoch 148/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.4638 - auc: 0.8984 - val_loss: 0.4707 - val_auc: 0.8938\n",
      "Epoch 149/175\n",
      "540/540 [==============================] - 0s 507us/step - loss: 0.4634 - auc: 0.8984 - val_loss: 0.4704 - val_auc: 0.8939\n",
      "Epoch 150/175\n",
      "540/540 [==============================] - 0s 541us/step - loss: 0.4631 - auc: 0.8985 - val_loss: 0.4701 - val_auc: 0.8940\n",
      "Epoch 151/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4626 - auc: 0.8986 - val_loss: 0.4699 - val_auc: 0.8940\n",
      "Epoch 152/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.4623 - auc: 0.8988 - val_loss: 0.4693 - val_auc: 0.8941\n",
      "Epoch 153/175\n",
      "540/540 [==============================] - 0s 499us/step - loss: 0.4620 - auc: 0.8989 - val_loss: 0.4688 - val_auc: 0.8943\n",
      "Epoch 154/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.4615 - auc: 0.8991 - val_loss: 0.4684 - val_auc: 0.8943\n",
      "Epoch 155/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.4612 - auc: 0.8989 - val_loss: 0.4680 - val_auc: 0.8945\n",
      "Epoch 156/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4609 - auc: 0.8992 - val_loss: 0.4679 - val_auc: 0.8945\n",
      "Epoch 157/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4605 - auc: 0.8992 - val_loss: 0.4675 - val_auc: 0.8946\n",
      "Epoch 158/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4602 - auc: 0.8993 - val_loss: 0.4674 - val_auc: 0.8948\n",
      "Epoch 159/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.4598 - auc: 0.8994 - val_loss: 0.4671 - val_auc: 0.8948\n",
      "Epoch 160/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.4595 - auc: 0.8995 - val_loss: 0.4668 - val_auc: 0.8950\n",
      "Epoch 161/175\n",
      "540/540 [==============================] - 0s 499us/step - loss: 0.4591 - auc: 0.8997 - val_loss: 0.4661 - val_auc: 0.8952\n",
      "Epoch 162/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.4589 - auc: 0.8998 - val_loss: 0.4657 - val_auc: 0.8953\n",
      "Epoch 163/175\n",
      "540/540 [==============================] - 0s 534us/step - loss: 0.4585 - auc: 0.8998 - val_loss: 0.4662 - val_auc: 0.8952\n",
      "Epoch 164/175\n",
      "540/540 [==============================] - 0s 499us/step - loss: 0.4582 - auc: 0.8999 - val_loss: 0.4652 - val_auc: 0.8955\n",
      "Epoch 165/175\n",
      "540/540 [==============================] - 0s 507us/step - loss: 0.4578 - auc: 0.9001 - val_loss: 0.4649 - val_auc: 0.8956\n",
      "Epoch 166/175\n",
      "540/540 [==============================] - 0s 499us/step - loss: 0.4576 - auc: 0.9002 - val_loss: 0.4651 - val_auc: 0.8956\n",
      "Epoch 167/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.4571 - auc: 0.9004 - val_loss: 0.4646 - val_auc: 0.8955\n",
      "Epoch 168/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.4570 - auc: 0.9004 - val_loss: 0.4640 - val_auc: 0.8956\n",
      "Epoch 169/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4567 - auc: 0.9005 - val_loss: 0.4638 - val_auc: 0.8956\n",
      "Epoch 170/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4564 - auc: 0.9005 - val_loss: 0.4647 - val_auc: 0.8955\n",
      "Epoch 171/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.4561 - auc: 0.9005 - val_loss: 0.4632 - val_auc: 0.8958\n",
      "Epoch 172/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4559 - auc: 0.9006 - val_loss: 0.4636 - val_auc: 0.8959\n",
      "Epoch 173/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4556 - auc: 0.9008 - val_loss: 0.4627 - val_auc: 0.8960\n",
      "Epoch 174/175\n",
      "540/540 [==============================] - 0s 502us/step - loss: 0.4553 - auc: 0.9008 - val_loss: 0.4625 - val_auc: 0.8961\n",
      "Epoch 175/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4551 - auc: 0.9009 - val_loss: 0.4627 - val_auc: 0.8960\n",
      "675/675 [==============================] - 0s 317us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/175\n",
      "540/540 [==============================] - 1s 679us/step - loss: 0.9170 - auc: 0.6732 - val_loss: 0.9118 - val_auc: 0.7045\n",
      "Epoch 2/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.9079 - auc: 0.7126 - val_loss: 0.9026 - val_auc: 0.7287\n",
      "Epoch 3/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.8991 - auc: 0.7354 - val_loss: 0.8938 - val_auc: 0.7473\n",
      "Epoch 4/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.8905 - auc: 0.7490 - val_loss: 0.8850 - val_auc: 0.7575\n",
      "Epoch 5/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.8818 - auc: 0.7577 - val_loss: 0.8760 - val_auc: 0.7653\n",
      "Epoch 6/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.8728 - auc: 0.7644 - val_loss: 0.8666 - val_auc: 0.7724\n",
      "Epoch 7/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.8634 - auc: 0.7703 - val_loss: 0.8569 - val_auc: 0.7765\n",
      "Epoch 8/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.8536 - auc: 0.7753 - val_loss: 0.8469 - val_auc: 0.7817\n",
      "Epoch 9/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.8436 - auc: 0.7796 - val_loss: 0.8366 - val_auc: 0.7855\n",
      "Epoch 10/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.8333 - auc: 0.7849 - val_loss: 0.8260 - val_auc: 0.7900\n",
      "Epoch 11/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.8226 - auc: 0.7889 - val_loss: 0.8151 - val_auc: 0.7944\n",
      "Epoch 12/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.8118 - auc: 0.7931 - val_loss: 0.8041 - val_auc: 0.7983\n",
      "Epoch 13/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.8008 - auc: 0.7971 - val_loss: 0.7929 - val_auc: 0.8020\n",
      "Epoch 14/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.7898 - auc: 0.8011 - val_loss: 0.7818 - val_auc: 0.8049\n",
      "Epoch 15/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.7787 - auc: 0.8047 - val_loss: 0.7707 - val_auc: 0.8085\n",
      "Epoch 16/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.7678 - auc: 0.8084 - val_loss: 0.7598 - val_auc: 0.8119\n",
      "Epoch 17/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.7570 - auc: 0.8118 - val_loss: 0.7492 - val_auc: 0.8154\n",
      "Epoch 18/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.7465 - auc: 0.8152 - val_loss: 0.7389 - val_auc: 0.8187\n",
      "Epoch 19/175\n",
      "540/540 [==============================] - 0s 507us/step - loss: 0.7364 - auc: 0.8187 - val_loss: 0.7290 - val_auc: 0.8215\n",
      "Epoch 20/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.7266 - auc: 0.8218 - val_loss: 0.7196 - val_auc: 0.8239\n",
      "Epoch 21/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.7173 - auc: 0.8245 - val_loss: 0.7106 - val_auc: 0.8267\n",
      "Epoch 22/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.7084 - auc: 0.8271 - val_loss: 0.7020 - val_auc: 0.8290\n",
      "Epoch 23/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.7000 - auc: 0.8296 - val_loss: 0.6939 - val_auc: 0.8314\n",
      "Epoch 24/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.6921 - auc: 0.8320 - val_loss: 0.6862 - val_auc: 0.8337\n",
      "Epoch 25/175\n",
      "540/540 [==============================] - 0s 537us/step - loss: 0.6845 - auc: 0.8343 - val_loss: 0.6790 - val_auc: 0.8359\n",
      "Epoch 26/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.6774 - auc: 0.8364 - val_loss: 0.6723 - val_auc: 0.8377\n",
      "Epoch 27/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.6706 - auc: 0.8385 - val_loss: 0.6660 - val_auc: 0.8394\n",
      "Epoch 28/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.6643 - auc: 0.8403 - val_loss: 0.6600 - val_auc: 0.8410\n",
      "Epoch 29/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.6583 - auc: 0.8419 - val_loss: 0.6541 - val_auc: 0.8429\n",
      "Epoch 30/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.6526 - auc: 0.8436 - val_loss: 0.6486 - val_auc: 0.8444\n",
      "Epoch 31/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.6472 - auc: 0.8452 - val_loss: 0.6434 - val_auc: 0.8461\n",
      "Epoch 32/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.6421 - auc: 0.8467 - val_loss: 0.6385 - val_auc: 0.8474\n",
      "Epoch 33/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.6371 - auc: 0.8481 - val_loss: 0.6339 - val_auc: 0.8488\n",
      "Epoch 34/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.6324 - auc: 0.8494 - val_loss: 0.6293 - val_auc: 0.8502\n",
      "Epoch 35/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.6280 - auc: 0.8507 - val_loss: 0.6250 - val_auc: 0.8513\n",
      "Epoch 36/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.6237 - auc: 0.8520 - val_loss: 0.6209 - val_auc: 0.8524\n",
      "Epoch 37/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.6195 - auc: 0.8531 - val_loss: 0.6170 - val_auc: 0.8535\n",
      "Epoch 38/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.6155 - auc: 0.8543 - val_loss: 0.6131 - val_auc: 0.8546\n",
      "Epoch 39/175\n",
      "540/540 [==============================] - 0s 490us/step - loss: 0.6117 - auc: 0.8554 - val_loss: 0.6094 - val_auc: 0.8556\n",
      "Epoch 40/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.6080 - auc: 0.8563 - val_loss: 0.6059 - val_auc: 0.8567\n",
      "Epoch 41/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.6044 - auc: 0.8574 - val_loss: 0.6025 - val_auc: 0.8576\n",
      "Epoch 42/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.6009 - auc: 0.8584 - val_loss: 0.5992 - val_auc: 0.8585\n",
      "Epoch 43/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.5976 - auc: 0.8593 - val_loss: 0.5960 - val_auc: 0.8594\n",
      "Epoch 44/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.5944 - auc: 0.8602 - val_loss: 0.5928 - val_auc: 0.8602\n",
      "Epoch 45/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.5913 - auc: 0.8611 - val_loss: 0.5898 - val_auc: 0.8611\n",
      "Epoch 46/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.5882 - auc: 0.8619 - val_loss: 0.5869 - val_auc: 0.8619\n",
      "Epoch 47/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.5853 - auc: 0.8626 - val_loss: 0.5840 - val_auc: 0.8626\n",
      "Epoch 48/175\n",
      "540/540 [==============================] - 0s 501us/step - loss: 0.5824 - auc: 0.8635 - val_loss: 0.5813 - val_auc: 0.8633\n",
      "Epoch 49/175\n",
      "540/540 [==============================] - 0s 536us/step - loss: 0.5796 - auc: 0.8642 - val_loss: 0.5785 - val_auc: 0.8640\n",
      "Epoch 50/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.5769 - auc: 0.8649 - val_loss: 0.5759 - val_auc: 0.8647\n",
      "Epoch 51/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.5742 - auc: 0.8657 - val_loss: 0.5733 - val_auc: 0.8655\n",
      "Epoch 52/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.5716 - auc: 0.8663 - val_loss: 0.5708 - val_auc: 0.8661\n",
      "Epoch 53/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.5691 - auc: 0.8669 - val_loss: 0.5684 - val_auc: 0.8667\n",
      "Epoch 54/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.5666 - auc: 0.8676 - val_loss: 0.5661 - val_auc: 0.8673\n",
      "Epoch 55/175\n",
      "540/540 [==============================] - 0s 488us/step - loss: 0.5642 - auc: 0.8684 - val_loss: 0.5638 - val_auc: 0.8677\n",
      "Epoch 56/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.5620 - auc: 0.8688 - val_loss: 0.5615 - val_auc: 0.8684\n",
      "Epoch 57/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.5597 - auc: 0.8693 - val_loss: 0.5593 - val_auc: 0.8690\n",
      "Epoch 58/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.5574 - auc: 0.8701 - val_loss: 0.5572 - val_auc: 0.8694\n",
      "Epoch 59/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.5553 - auc: 0.8705 - val_loss: 0.5552 - val_auc: 0.8699\n",
      "Epoch 60/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.5532 - auc: 0.8711 - val_loss: 0.5530 - val_auc: 0.8706\n",
      "Epoch 61/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.5511 - auc: 0.8716 - val_loss: 0.5511 - val_auc: 0.8710\n",
      "Epoch 62/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.5491 - auc: 0.8720 - val_loss: 0.5491 - val_auc: 0.8715\n",
      "Epoch 63/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.5471 - auc: 0.8725 - val_loss: 0.5472 - val_auc: 0.8720\n",
      "Epoch 64/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.5452 - auc: 0.8731 - val_loss: 0.5453 - val_auc: 0.8724\n",
      "Epoch 65/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.5433 - auc: 0.8735 - val_loss: 0.5434 - val_auc: 0.8729\n",
      "Epoch 66/175\n",
      "540/540 [==============================] - 0s 489us/step - loss: 0.5413 - auc: 0.8741 - val_loss: 0.5420 - val_auc: 0.8733\n",
      "Epoch 67/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.5396 - auc: 0.8744 - val_loss: 0.5399 - val_auc: 0.8737\n",
      "Epoch 68/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.5379 - auc: 0.8749 - val_loss: 0.5381 - val_auc: 0.8741\n",
      "Epoch 69/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.5361 - auc: 0.8754 - val_loss: 0.5365 - val_auc: 0.8745\n",
      "Epoch 70/175\n",
      "540/540 [==============================] - 0s 537us/step - loss: 0.5344 - auc: 0.8759 - val_loss: 0.5348 - val_auc: 0.8750\n",
      "Epoch 71/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.5327 - auc: 0.8762 - val_loss: 0.5332 - val_auc: 0.8754\n",
      "Epoch 72/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.5310 - auc: 0.8767 - val_loss: 0.5316 - val_auc: 0.8758\n",
      "Epoch 73/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.5294 - auc: 0.8771 - val_loss: 0.5300 - val_auc: 0.8762\n",
      "Epoch 74/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.5278 - auc: 0.8777 - val_loss: 0.5284 - val_auc: 0.8767\n",
      "Epoch 75/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.5263 - auc: 0.8779 - val_loss: 0.5268 - val_auc: 0.8770\n",
      "Epoch 76/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.5247 - auc: 0.8784 - val_loss: 0.5253 - val_auc: 0.8774\n",
      "Epoch 77/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.5232 - auc: 0.8789 - val_loss: 0.5239 - val_auc: 0.8778\n",
      "Epoch 78/175\n",
      "540/540 [==============================] - 0s 502us/step - loss: 0.5217 - auc: 0.8792 - val_loss: 0.5225 - val_auc: 0.8781\n",
      "Epoch 79/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.5203 - auc: 0.8797 - val_loss: 0.5211 - val_auc: 0.8784\n",
      "Epoch 80/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.5189 - auc: 0.8800 - val_loss: 0.5199 - val_auc: 0.8788\n",
      "Epoch 81/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.5176 - auc: 0.8804 - val_loss: 0.5185 - val_auc: 0.8791\n",
      "Epoch 82/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.5161 - auc: 0.8808 - val_loss: 0.5172 - val_auc: 0.8794\n",
      "Epoch 83/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.5150 - auc: 0.8810 - val_loss: 0.5159 - val_auc: 0.8798\n",
      "Epoch 84/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.5136 - auc: 0.8814 - val_loss: 0.5147 - val_auc: 0.8802\n",
      "Epoch 85/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.5124 - auc: 0.8817 - val_loss: 0.5135 - val_auc: 0.8804\n",
      "Epoch 86/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.5112 - auc: 0.8820 - val_loss: 0.5123 - val_auc: 0.8807\n",
      "Epoch 87/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.5100 - auc: 0.8823 - val_loss: 0.5111 - val_auc: 0.8810\n",
      "Epoch 88/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.5088 - auc: 0.8827 - val_loss: 0.5100 - val_auc: 0.8813\n",
      "Epoch 89/175\n",
      "540/540 [==============================] - 0s 539us/step - loss: 0.5077 - auc: 0.8829 - val_loss: 0.5088 - val_auc: 0.8817\n",
      "Epoch 90/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.5065 - auc: 0.8832 - val_loss: 0.5078 - val_auc: 0.8819\n",
      "Epoch 91/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.5054 - auc: 0.8836 - val_loss: 0.5068 - val_auc: 0.8821\n",
      "Epoch 92/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.5044 - auc: 0.8838 - val_loss: 0.5057 - val_auc: 0.8825\n",
      "Epoch 93/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.5033 - auc: 0.8841 - val_loss: 0.5046 - val_auc: 0.8828\n",
      "Epoch 94/175\n",
      "540/540 [==============================] - 0s 500us/step - loss: 0.5022 - auc: 0.8845 - val_loss: 0.5038 - val_auc: 0.8829\n",
      "Epoch 95/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.5013 - auc: 0.8846 - val_loss: 0.5027 - val_auc: 0.8833\n",
      "Epoch 96/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.5001 - auc: 0.8850 - val_loss: 0.5017 - val_auc: 0.8835\n",
      "Epoch 97/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4993 - auc: 0.8853 - val_loss: 0.5009 - val_auc: 0.8838\n",
      "Epoch 98/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4982 - auc: 0.8856 - val_loss: 0.5001 - val_auc: 0.8839\n",
      "Epoch 99/175\n",
      "540/540 [==============================] - 0s 490us/step - loss: 0.4974 - auc: 0.8857 - val_loss: 0.4990 - val_auc: 0.8842\n",
      "Epoch 100/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4964 - auc: 0.8860 - val_loss: 0.4981 - val_auc: 0.8845\n",
      "Epoch 101/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4955 - auc: 0.8863 - val_loss: 0.4972 - val_auc: 0.8846\n",
      "Epoch 102/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4946 - auc: 0.8865 - val_loss: 0.4964 - val_auc: 0.8847\n",
      "Epoch 103/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4937 - auc: 0.8866 - val_loss: 0.4955 - val_auc: 0.8850\n",
      "Epoch 104/175\n",
      "540/540 [==============================] - 0s 503us/step - loss: 0.4928 - auc: 0.8870 - val_loss: 0.4948 - val_auc: 0.8852\n",
      "Epoch 105/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4920 - auc: 0.8872 - val_loss: 0.4939 - val_auc: 0.8855\n",
      "Epoch 106/175\n",
      "540/540 [==============================] - 0s 538us/step - loss: 0.4912 - auc: 0.8873 - val_loss: 0.4931 - val_auc: 0.8857\n",
      "Epoch 107/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4904 - auc: 0.8877 - val_loss: 0.4923 - val_auc: 0.8859\n",
      "Epoch 108/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4896 - auc: 0.8878 - val_loss: 0.4915 - val_auc: 0.8861\n",
      "Epoch 109/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.4887 - auc: 0.8880 - val_loss: 0.4909 - val_auc: 0.8862\n",
      "Epoch 110/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4880 - auc: 0.8882 - val_loss: 0.4901 - val_auc: 0.8864\n",
      "Epoch 111/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4873 - auc: 0.8884 - val_loss: 0.4894 - val_auc: 0.8865\n",
      "Epoch 112/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4865 - auc: 0.8886 - val_loss: 0.4887 - val_auc: 0.8868\n",
      "Epoch 113/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4858 - auc: 0.8888 - val_loss: 0.4879 - val_auc: 0.8869\n",
      "Epoch 114/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4851 - auc: 0.8889 - val_loss: 0.4872 - val_auc: 0.8871\n",
      "Epoch 115/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4843 - auc: 0.8892 - val_loss: 0.4866 - val_auc: 0.8871\n",
      "Epoch 116/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4837 - auc: 0.8892 - val_loss: 0.4859 - val_auc: 0.8874\n",
      "Epoch 117/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4829 - auc: 0.8896 - val_loss: 0.4852 - val_auc: 0.8876\n",
      "Epoch 118/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4823 - auc: 0.8897 - val_loss: 0.4846 - val_auc: 0.8877\n",
      "Epoch 119/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.4816 - auc: 0.8898 - val_loss: 0.4840 - val_auc: 0.8878\n",
      "Epoch 120/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4810 - auc: 0.8900 - val_loss: 0.4834 - val_auc: 0.8880\n",
      "Epoch 121/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4804 - auc: 0.8902 - val_loss: 0.4827 - val_auc: 0.8882\n",
      "Epoch 122/175\n",
      "540/540 [==============================] - 0s 542us/step - loss: 0.4798 - auc: 0.8904 - val_loss: 0.4822 - val_auc: 0.8883\n",
      "Epoch 123/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.4792 - auc: 0.8905 - val_loss: 0.4816 - val_auc: 0.8885\n",
      "Epoch 124/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4785 - auc: 0.8908 - val_loss: 0.4811 - val_auc: 0.8887\n",
      "Epoch 125/175\n",
      "540/540 [==============================] - 0s 508us/step - loss: 0.4779 - auc: 0.8910 - val_loss: 0.4804 - val_auc: 0.8887\n",
      "Epoch 126/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4774 - auc: 0.8910 - val_loss: 0.4799 - val_auc: 0.8889\n",
      "Epoch 127/175\n",
      "540/540 [==============================] - 0s 490us/step - loss: 0.4768 - auc: 0.8911 - val_loss: 0.4793 - val_auc: 0.8891\n",
      "Epoch 128/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4762 - auc: 0.8913 - val_loss: 0.4788 - val_auc: 0.8891\n",
      "Epoch 129/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4757 - auc: 0.8915 - val_loss: 0.4783 - val_auc: 0.8892\n",
      "Epoch 130/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4751 - auc: 0.8916 - val_loss: 0.4778 - val_auc: 0.8894\n",
      "Epoch 131/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4746 - auc: 0.8918 - val_loss: 0.4772 - val_auc: 0.8896\n",
      "Epoch 132/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4740 - auc: 0.8920 - val_loss: 0.4767 - val_auc: 0.8897\n",
      "Epoch 133/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4734 - auc: 0.8922 - val_loss: 0.4762 - val_auc: 0.8898\n",
      "Epoch 134/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4730 - auc: 0.8922 - val_loss: 0.4757 - val_auc: 0.8900\n",
      "Epoch 135/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4725 - auc: 0.8924 - val_loss: 0.4752 - val_auc: 0.8901\n",
      "Epoch 136/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4718 - auc: 0.8926 - val_loss: 0.4747 - val_auc: 0.8903\n",
      "Epoch 137/175\n",
      "540/540 [==============================] - 0s 540us/step - loss: 0.4714 - auc: 0.8927 - val_loss: 0.4743 - val_auc: 0.8904\n",
      "Epoch 138/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4710 - auc: 0.8928 - val_loss: 0.4737 - val_auc: 0.8906\n",
      "Epoch 139/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4705 - auc: 0.8930 - val_loss: 0.4735 - val_auc: 0.8907\n",
      "Epoch 140/175\n",
      "540/540 [==============================] - 0s 490us/step - loss: 0.4701 - auc: 0.8930 - val_loss: 0.4729 - val_auc: 0.8908\n",
      "Epoch 141/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4695 - auc: 0.8933 - val_loss: 0.4726 - val_auc: 0.8908\n",
      "Epoch 142/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4691 - auc: 0.8933 - val_loss: 0.4720 - val_auc: 0.8910\n",
      "Epoch 143/175\n",
      "540/540 [==============================] - 0s 505us/step - loss: 0.4687 - auc: 0.8935 - val_loss: 0.4717 - val_auc: 0.8910\n",
      "Epoch 144/175\n",
      "540/540 [==============================] - 0s 501us/step - loss: 0.4682 - auc: 0.8935 - val_loss: 0.4712 - val_auc: 0.8913\n",
      "Epoch 145/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4678 - auc: 0.8937 - val_loss: 0.4707 - val_auc: 0.8913\n",
      "Epoch 146/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.4673 - auc: 0.8939 - val_loss: 0.4703 - val_auc: 0.8913\n",
      "Epoch 147/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4669 - auc: 0.8939 - val_loss: 0.4699 - val_auc: 0.8916\n",
      "Epoch 148/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4665 - auc: 0.8941 - val_loss: 0.4695 - val_auc: 0.8917\n",
      "Epoch 149/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.4661 - auc: 0.8942 - val_loss: 0.4692 - val_auc: 0.8918\n",
      "Epoch 150/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4657 - auc: 0.8943 - val_loss: 0.4687 - val_auc: 0.8919\n",
      "Epoch 151/175\n",
      "540/540 [==============================] - 0s 541us/step - loss: 0.4653 - auc: 0.8945 - val_loss: 0.4683 - val_auc: 0.8920\n",
      "Epoch 152/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4649 - auc: 0.8946 - val_loss: 0.4681 - val_auc: 0.8920\n",
      "Epoch 153/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4645 - auc: 0.8947 - val_loss: 0.4676 - val_auc: 0.8921\n",
      "Epoch 154/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4641 - auc: 0.8947 - val_loss: 0.4673 - val_auc: 0.8924\n",
      "Epoch 155/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4638 - auc: 0.8949 - val_loss: 0.4669 - val_auc: 0.8924\n",
      "Epoch 156/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4634 - auc: 0.8949 - val_loss: 0.4666 - val_auc: 0.8925\n",
      "Epoch 157/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4629 - auc: 0.8952 - val_loss: 0.4663 - val_auc: 0.8926\n",
      "Epoch 158/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4627 - auc: 0.8953 - val_loss: 0.4659 - val_auc: 0.8927\n",
      "Epoch 159/175\n",
      "540/540 [==============================] - 0s 507us/step - loss: 0.4622 - auc: 0.8955 - val_loss: 0.4655 - val_auc: 0.8927\n",
      "Epoch 160/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4620 - auc: 0.8954 - val_loss: 0.4651 - val_auc: 0.8929\n",
      "Epoch 161/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4616 - auc: 0.8956 - val_loss: 0.4649 - val_auc: 0.8930\n",
      "Epoch 162/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.4612 - auc: 0.8958 - val_loss: 0.4645 - val_auc: 0.8931\n",
      "Epoch 163/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4609 - auc: 0.8958 - val_loss: 0.4644 - val_auc: 0.8933\n",
      "Epoch 164/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.4606 - auc: 0.8960 - val_loss: 0.4640 - val_auc: 0.8933\n",
      "Epoch 165/175\n",
      "540/540 [==============================] - 0s 540us/step - loss: 0.4603 - auc: 0.8960 - val_loss: 0.4635 - val_auc: 0.8934\n",
      "Epoch 166/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4599 - auc: 0.8962 - val_loss: 0.4633 - val_auc: 0.8934\n",
      "Epoch 167/175\n",
      "540/540 [==============================] - 0s 492us/step - loss: 0.4596 - auc: 0.8963 - val_loss: 0.4632 - val_auc: 0.8935\n",
      "Epoch 168/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.4593 - auc: 0.8963 - val_loss: 0.4628 - val_auc: 0.8935\n",
      "Epoch 169/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4590 - auc: 0.8964 - val_loss: 0.4624 - val_auc: 0.8938\n",
      "Epoch 170/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.4587 - auc: 0.8966 - val_loss: 0.4620 - val_auc: 0.8939\n",
      "Epoch 171/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4584 - auc: 0.8967 - val_loss: 0.4618 - val_auc: 0.8940\n",
      "Epoch 172/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4580 - auc: 0.8968 - val_loss: 0.4616 - val_auc: 0.8940\n",
      "Epoch 173/175\n",
      "540/540 [==============================] - 0s 506us/step - loss: 0.4577 - auc: 0.8969 - val_loss: 0.4613 - val_auc: 0.8942\n",
      "Epoch 174/175\n",
      "540/540 [==============================] - 0s 494us/step - loss: 0.4573 - auc: 0.8971 - val_loss: 0.4608 - val_auc: 0.8944\n",
      "Epoch 175/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.4572 - auc: 0.8972 - val_loss: 0.4606 - val_auc: 0.8944\n",
      "675/675 [==============================] - 0s 266us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/175\n",
      "540/540 [==============================] - 1s 673us/step - loss: 0.9318 - auc: 0.4633 - val_loss: 0.9239 - val_auc: 0.5111\n",
      "Epoch 2/175\n",
      "540/540 [==============================] - 0s 502us/step - loss: 0.9201 - auc: 0.5240 - val_loss: 0.9156 - val_auc: 0.5616\n",
      "Epoch 3/175\n",
      "540/540 [==============================] - 1s 1ms/step - loss: 0.9128 - auc: 0.5649 - val_loss: 0.9089 - val_auc: 0.5949\n",
      "Epoch 4/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.9064 - auc: 0.5995 - val_loss: 0.9028 - val_auc: 0.6244\n",
      "Epoch 5/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.9004 - auc: 0.6275 - val_loss: 0.8967 - val_auc: 0.6509\n",
      "Epoch 6/175\n",
      "540/540 [==============================] - 0s 487us/step - loss: 0.8944 - auc: 0.6505 - val_loss: 0.8906 - val_auc: 0.6705\n",
      "Epoch 7/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.8883 - auc: 0.6707 - val_loss: 0.8845 - val_auc: 0.6886\n",
      "Epoch 8/175\n",
      "540/540 [==============================] - 0s 487us/step - loss: 0.8821 - auc: 0.6892 - val_loss: 0.8783 - val_auc: 0.7052\n",
      "Epoch 9/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.8758 - auc: 0.7048 - val_loss: 0.8719 - val_auc: 0.7200\n",
      "Epoch 10/175\n",
      "540/540 [==============================] - 0s 486us/step - loss: 0.8694 - auc: 0.7206 - val_loss: 0.8654 - val_auc: 0.7340\n",
      "Epoch 11/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.8627 - auc: 0.7346 - val_loss: 0.8586 - val_auc: 0.7486\n",
      "Epoch 12/175\n",
      "540/540 [==============================] - 0s 487us/step - loss: 0.8559 - auc: 0.7473 - val_loss: 0.8516 - val_auc: 0.7597\n",
      "Epoch 13/175\n",
      "540/540 [==============================] - 0s 487us/step - loss: 0.8487 - auc: 0.7594 - val_loss: 0.8442 - val_auc: 0.7703\n",
      "Epoch 14/175\n",
      "540/540 [==============================] - 0s 486us/step - loss: 0.8412 - auc: 0.7710 - val_loss: 0.8366 - val_auc: 0.7809\n",
      "Epoch 15/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.8334 - auc: 0.7808 - val_loss: 0.8287 - val_auc: 0.7886\n",
      "Epoch 16/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.8254 - auc: 0.7895 - val_loss: 0.8205 - val_auc: 0.7958\n",
      "Epoch 17/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.8171 - auc: 0.7963 - val_loss: 0.8122 - val_auc: 0.8013\n",
      "Epoch 18/175\n",
      "540/540 [==============================] - 0s 486us/step - loss: 0.8086 - auc: 0.8024 - val_loss: 0.8037 - val_auc: 0.8060\n",
      "Epoch 19/175\n",
      "540/540 [==============================] - 0s 482us/step - loss: 0.8000 - auc: 0.8075 - val_loss: 0.7952 - val_auc: 0.8097\n",
      "Epoch 20/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.7914 - auc: 0.8115 - val_loss: 0.7865 - val_auc: 0.8132\n",
      "Epoch 21/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.7827 - auc: 0.8147 - val_loss: 0.7778 - val_auc: 0.8155\n",
      "Epoch 22/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.7739 - auc: 0.8171 - val_loss: 0.7691 - val_auc: 0.8182\n",
      "Epoch 23/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.7651 - auc: 0.8196 - val_loss: 0.7603 - val_auc: 0.8203\n",
      "Epoch 24/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.7564 - auc: 0.8215 - val_loss: 0.7517 - val_auc: 0.8219\n",
      "Epoch 25/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.7477 - auc: 0.8235 - val_loss: 0.7431 - val_auc: 0.8235\n",
      "Epoch 26/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.7391 - auc: 0.8250 - val_loss: 0.7347 - val_auc: 0.8251\n",
      "Epoch 27/175\n",
      "540/540 [==============================] - 0s 482us/step - loss: 0.7307 - auc: 0.8269 - val_loss: 0.7265 - val_auc: 0.8266\n",
      "Epoch 28/175\n",
      "540/540 [==============================] - 0s 530us/step - loss: 0.7224 - auc: 0.8282 - val_loss: 0.7185 - val_auc: 0.8280\n",
      "Epoch 29/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.7144 - auc: 0.8296 - val_loss: 0.7107 - val_auc: 0.8294\n",
      "Epoch 30/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.7066 - auc: 0.8312 - val_loss: 0.7032 - val_auc: 0.8306\n",
      "Epoch 31/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.6992 - auc: 0.8325 - val_loss: 0.6961 - val_auc: 0.8321\n",
      "Epoch 32/175\n",
      "540/540 [==============================] - 0s 501us/step - loss: 0.6920 - auc: 0.8339 - val_loss: 0.6891 - val_auc: 0.8335\n",
      "Epoch 33/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.6852 - auc: 0.8353 - val_loss: 0.6826 - val_auc: 0.8348\n",
      "Epoch 34/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.6787 - auc: 0.8369 - val_loss: 0.6764 - val_auc: 0.8359\n",
      "Epoch 35/175\n",
      "540/540 [==============================] - 0s 482us/step - loss: 0.6725 - auc: 0.8378 - val_loss: 0.6704 - val_auc: 0.8372\n",
      "Epoch 36/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.6665 - auc: 0.8393 - val_loss: 0.6646 - val_auc: 0.8384\n",
      "Epoch 37/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.6608 - auc: 0.8405 - val_loss: 0.6592 - val_auc: 0.8395\n",
      "Epoch 38/175\n",
      "540/540 [==============================] - 0s 482us/step - loss: 0.6553 - auc: 0.8416 - val_loss: 0.6538 - val_auc: 0.8408\n",
      "Epoch 39/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.6499 - auc: 0.8428 - val_loss: 0.6487 - val_auc: 0.8419\n",
      "Epoch 40/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.6448 - auc: 0.8441 - val_loss: 0.6438 - val_auc: 0.8430\n",
      "Epoch 41/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.6398 - auc: 0.8452 - val_loss: 0.6393 - val_auc: 0.8441\n",
      "Epoch 42/175\n",
      "540/540 [==============================] - 0s 480us/step - loss: 0.6352 - auc: 0.8462 - val_loss: 0.6345 - val_auc: 0.8452\n",
      "Epoch 43/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.6305 - auc: 0.8474 - val_loss: 0.6302 - val_auc: 0.8464\n",
      "Epoch 44/175\n",
      "540/540 [==============================] - 0s 534us/step - loss: 0.6261 - auc: 0.8485 - val_loss: 0.6256 - val_auc: 0.8475\n",
      "Epoch 45/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.6218 - auc: 0.8495 - val_loss: 0.6214 - val_auc: 0.8486\n",
      "Epoch 46/175\n",
      "540/540 [==============================] - 0s 487us/step - loss: 0.6176 - auc: 0.8507 - val_loss: 0.6176 - val_auc: 0.8495\n",
      "Epoch 47/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.6135 - auc: 0.8516 - val_loss: 0.6136 - val_auc: 0.8506\n",
      "Epoch 48/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.6096 - auc: 0.8527 - val_loss: 0.6099 - val_auc: 0.8516\n",
      "Epoch 49/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.6057 - auc: 0.8537 - val_loss: 0.6060 - val_auc: 0.8526\n",
      "Epoch 50/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.6020 - auc: 0.8547 - val_loss: 0.6027 - val_auc: 0.8535\n",
      "Epoch 51/175\n",
      "540/540 [==============================] - 0s 509us/step - loss: 0.5983 - auc: 0.8559 - val_loss: 0.5992 - val_auc: 0.8543\n",
      "Epoch 52/175\n",
      "540/540 [==============================] - 0s 572us/step - loss: 0.5948 - auc: 0.8566 - val_loss: 0.5957 - val_auc: 0.8552\n",
      "Epoch 53/175\n",
      "540/540 [==============================] - 0s 490us/step - loss: 0.5913 - auc: 0.8576 - val_loss: 0.5924 - val_auc: 0.8560\n",
      "Epoch 54/175\n",
      "540/540 [==============================] - 0s 482us/step - loss: 0.5880 - auc: 0.8583 - val_loss: 0.5892 - val_auc: 0.8568\n",
      "Epoch 55/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.5848 - auc: 0.8592 - val_loss: 0.5863 - val_auc: 0.8576\n",
      "Epoch 56/175\n",
      "540/540 [==============================] - 0s 482us/step - loss: 0.5817 - auc: 0.8599 - val_loss: 0.5832 - val_auc: 0.8583\n",
      "Epoch 57/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.5787 - auc: 0.8606 - val_loss: 0.5804 - val_auc: 0.8590\n",
      "Epoch 58/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.5757 - auc: 0.8614 - val_loss: 0.5776 - val_auc: 0.8598\n",
      "Epoch 59/175\n",
      "540/540 [==============================] - 0s 490us/step - loss: 0.5729 - auc: 0.8620 - val_loss: 0.5750 - val_auc: 0.8604\n",
      "Epoch 60/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.5701 - auc: 0.8627 - val_loss: 0.5724 - val_auc: 0.8610\n",
      "Epoch 61/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.5675 - auc: 0.8634 - val_loss: 0.5701 - val_auc: 0.8616\n",
      "Epoch 62/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.5649 - auc: 0.8641 - val_loss: 0.5674 - val_auc: 0.8620\n",
      "Epoch 63/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.5624 - auc: 0.8646 - val_loss: 0.5650 - val_auc: 0.8626\n",
      "Epoch 64/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.5600 - auc: 0.8653 - val_loss: 0.5628 - val_auc: 0.8632\n",
      "Epoch 65/175\n",
      "540/540 [==============================] - 0s 481us/step - loss: 0.5575 - auc: 0.8660 - val_loss: 0.5606 - val_auc: 0.8636\n",
      "Epoch 66/175\n",
      "540/540 [==============================] - 0s 481us/step - loss: 0.5553 - auc: 0.8663 - val_loss: 0.5587 - val_auc: 0.8641\n",
      "Epoch 67/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.5531 - auc: 0.8668 - val_loss: 0.5564 - val_auc: 0.8646\n",
      "Epoch 68/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.5509 - auc: 0.8674 - val_loss: 0.5546 - val_auc: 0.8651\n",
      "Epoch 69/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.5489 - auc: 0.8680 - val_loss: 0.5527 - val_auc: 0.8655\n",
      "Epoch 70/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.5469 - auc: 0.8683 - val_loss: 0.5505 - val_auc: 0.8658\n",
      "Epoch 71/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.5449 - auc: 0.8689 - val_loss: 0.5489 - val_auc: 0.8664\n",
      "Epoch 72/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.5430 - auc: 0.8694 - val_loss: 0.5470 - val_auc: 0.8667\n",
      "Epoch 73/175\n",
      "540/540 [==============================] - 0s 529us/step - loss: 0.5411 - auc: 0.8698 - val_loss: 0.5450 - val_auc: 0.8670\n",
      "Epoch 74/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.5393 - auc: 0.8703 - val_loss: 0.5433 - val_auc: 0.8674\n",
      "Epoch 75/175\n",
      "540/540 [==============================] - 0s 482us/step - loss: 0.5375 - auc: 0.8707 - val_loss: 0.5417 - val_auc: 0.8679\n",
      "Epoch 76/175\n",
      "540/540 [==============================] - 0s 486us/step - loss: 0.5358 - auc: 0.8711 - val_loss: 0.5404 - val_auc: 0.8682\n",
      "Epoch 77/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.5342 - auc: 0.8716 - val_loss: 0.5386 - val_auc: 0.8687\n",
      "Epoch 78/175\n",
      "540/540 [==============================] - 0s 491us/step - loss: 0.5324 - auc: 0.8720 - val_loss: 0.5369 - val_auc: 0.8691\n",
      "Epoch 79/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.5309 - auc: 0.8725 - val_loss: 0.5356 - val_auc: 0.8695\n",
      "Epoch 80/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.5293 - auc: 0.8730 - val_loss: 0.5338 - val_auc: 0.8699\n",
      "Epoch 81/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.5278 - auc: 0.8734 - val_loss: 0.5324 - val_auc: 0.8702\n",
      "Epoch 82/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.5263 - auc: 0.8738 - val_loss: 0.5309 - val_auc: 0.8706\n",
      "Epoch 83/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.5248 - auc: 0.8741 - val_loss: 0.5296 - val_auc: 0.8711\n",
      "Epoch 84/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.5234 - auc: 0.8747 - val_loss: 0.5286 - val_auc: 0.8713\n",
      "Epoch 85/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.5220 - auc: 0.8751 - val_loss: 0.5272 - val_auc: 0.8718\n",
      "Epoch 86/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.5206 - auc: 0.8755 - val_loss: 0.5256 - val_auc: 0.8722\n",
      "Epoch 87/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.5193 - auc: 0.8759 - val_loss: 0.5243 - val_auc: 0.8725\n",
      "Epoch 88/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.5180 - auc: 0.8763 - val_loss: 0.5232 - val_auc: 0.8728\n",
      "Epoch 89/175\n",
      "540/540 [==============================] - 0s 481us/step - loss: 0.5167 - auc: 0.8766 - val_loss: 0.5223 - val_auc: 0.8731\n",
      "Epoch 90/175\n",
      "540/540 [==============================] - 0s 495us/step - loss: 0.5155 - auc: 0.8771 - val_loss: 0.5208 - val_auc: 0.8735\n",
      "Epoch 91/175\n",
      "540/540 [==============================] - 0s 528us/step - loss: 0.5142 - auc: 0.8773 - val_loss: 0.5199 - val_auc: 0.8738\n",
      "Epoch 92/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.5131 - auc: 0.8777 - val_loss: 0.5189 - val_auc: 0.8740\n",
      "Epoch 93/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.5119 - auc: 0.8780 - val_loss: 0.5173 - val_auc: 0.8745\n",
      "Epoch 94/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.5109 - auc: 0.8784 - val_loss: 0.5166 - val_auc: 0.8747\n",
      "Epoch 95/175\n",
      "540/540 [==============================] - 0s 482us/step - loss: 0.5096 - auc: 0.8787 - val_loss: 0.5158 - val_auc: 0.8750\n",
      "Epoch 96/175\n",
      "540/540 [==============================] - 0s 482us/step - loss: 0.5086 - auc: 0.8790 - val_loss: 0.5142 - val_auc: 0.8753\n",
      "Epoch 97/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.5075 - auc: 0.8793 - val_loss: 0.5132 - val_auc: 0.8756\n",
      "Epoch 98/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.5065 - auc: 0.8797 - val_loss: 0.5124 - val_auc: 0.8758\n",
      "Epoch 99/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.5056 - auc: 0.8799 - val_loss: 0.5116 - val_auc: 0.8761\n",
      "Epoch 100/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.5045 - auc: 0.8802 - val_loss: 0.5113 - val_auc: 0.8762\n",
      "Epoch 101/175\n",
      "540/540 [==============================] - 0s 482us/step - loss: 0.5036 - auc: 0.8804 - val_loss: 0.5098 - val_auc: 0.8766\n",
      "Epoch 102/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.5026 - auc: 0.8807 - val_loss: 0.5088 - val_auc: 0.8770\n",
      "Epoch 103/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.5018 - auc: 0.8811 - val_loss: 0.5077 - val_auc: 0.8771\n",
      "Epoch 104/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.5008 - auc: 0.8812 - val_loss: 0.5068 - val_auc: 0.8774\n",
      "Epoch 105/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.5000 - auc: 0.8815 - val_loss: 0.5061 - val_auc: 0.8775\n",
      "Epoch 106/175\n",
      "540/540 [==============================] - 0s 498us/step - loss: 0.4991 - auc: 0.8817 - val_loss: 0.5052 - val_auc: 0.8777\n",
      "Epoch 107/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.4983 - auc: 0.8819 - val_loss: 0.5042 - val_auc: 0.8781\n",
      "Epoch 108/175\n",
      "540/540 [==============================] - 0s 531us/step - loss: 0.4975 - auc: 0.8821 - val_loss: 0.5036 - val_auc: 0.8782\n",
      "Epoch 109/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.4966 - auc: 0.8824 - val_loss: 0.5027 - val_auc: 0.8784\n",
      "Epoch 110/175\n",
      "540/540 [==============================] - 0s 481us/step - loss: 0.4959 - auc: 0.8825 - val_loss: 0.5024 - val_auc: 0.8785\n",
      "Epoch 111/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.4951 - auc: 0.8827 - val_loss: 0.5014 - val_auc: 0.8788\n",
      "Epoch 112/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.4943 - auc: 0.8830 - val_loss: 0.5005 - val_auc: 0.8791\n",
      "Epoch 113/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.4936 - auc: 0.8832 - val_loss: 0.5000 - val_auc: 0.8791\n",
      "Epoch 114/175\n",
      "540/540 [==============================] - 0s 496us/step - loss: 0.4928 - auc: 0.8835 - val_loss: 0.4992 - val_auc: 0.8792\n",
      "Epoch 115/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.4921 - auc: 0.8837 - val_loss: 0.4987 - val_auc: 0.8793\n",
      "Epoch 116/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.4914 - auc: 0.8838 - val_loss: 0.4983 - val_auc: 0.8794\n",
      "Epoch 117/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.4908 - auc: 0.8839 - val_loss: 0.4972 - val_auc: 0.8798\n",
      "Epoch 118/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.4901 - auc: 0.8842 - val_loss: 0.4964 - val_auc: 0.8800\n",
      "Epoch 119/175\n",
      "540/540 [==============================] - 0s 599us/step - loss: 0.4894 - auc: 0.8843 - val_loss: 0.4961 - val_auc: 0.8800\n",
      "Epoch 120/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.4887 - auc: 0.8845 - val_loss: 0.4953 - val_auc: 0.8802\n",
      "Epoch 121/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.4881 - auc: 0.8846 - val_loss: 0.4952 - val_auc: 0.8803\n",
      "Epoch 122/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.4875 - auc: 0.8848 - val_loss: 0.4944 - val_auc: 0.8806\n",
      "Epoch 123/175\n",
      "540/540 [==============================] - 0s 548us/step - loss: 0.4868 - auc: 0.8851 - val_loss: 0.4933 - val_auc: 0.8809\n",
      "Epoch 124/175\n",
      "540/540 [==============================] - 0s 486us/step - loss: 0.4862 - auc: 0.8853 - val_loss: 0.4936 - val_auc: 0.8808\n",
      "Epoch 125/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.4857 - auc: 0.8854 - val_loss: 0.4922 - val_auc: 0.8812\n",
      "Epoch 126/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.4851 - auc: 0.8855 - val_loss: 0.4916 - val_auc: 0.8812\n",
      "Epoch 127/175\n",
      "540/540 [==============================] - 0s 486us/step - loss: 0.4846 - auc: 0.8856 - val_loss: 0.4912 - val_auc: 0.8814\n",
      "Epoch 128/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.4840 - auc: 0.8859 - val_loss: 0.4908 - val_auc: 0.8816\n",
      "Epoch 129/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.4834 - auc: 0.8860 - val_loss: 0.4899 - val_auc: 0.8817\n",
      "Epoch 130/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.4829 - auc: 0.8861 - val_loss: 0.4895 - val_auc: 0.8818\n",
      "Epoch 131/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.4823 - auc: 0.8862 - val_loss: 0.4890 - val_auc: 0.8820\n",
      "Epoch 132/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.4819 - auc: 0.8864 - val_loss: 0.4887 - val_auc: 0.8821\n",
      "Epoch 133/175\n",
      "540/540 [==============================] - 0s 493us/step - loss: 0.4812 - auc: 0.8867 - val_loss: 0.4896 - val_auc: 0.8820\n",
      "Epoch 134/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.4809 - auc: 0.8866 - val_loss: 0.4874 - val_auc: 0.8824\n",
      "Epoch 135/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.4803 - auc: 0.8869 - val_loss: 0.4878 - val_auc: 0.8824\n",
      "Epoch 136/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.4798 - auc: 0.8871 - val_loss: 0.4867 - val_auc: 0.8825\n",
      "Epoch 137/175\n",
      "540/540 [==============================] - 0s 482us/step - loss: 0.4792 - auc: 0.8871 - val_loss: 0.4860 - val_auc: 0.8827\n",
      "Epoch 138/175\n",
      "540/540 [==============================] - 0s 523us/step - loss: 0.4788 - auc: 0.8873 - val_loss: 0.4856 - val_auc: 0.8828\n",
      "Epoch 139/175\n",
      "540/540 [==============================] - 0s 478us/step - loss: 0.4783 - auc: 0.8874 - val_loss: 0.4853 - val_auc: 0.8830\n",
      "Epoch 140/175\n",
      "540/540 [==============================] - 0s 477us/step - loss: 0.4779 - auc: 0.8874 - val_loss: 0.4845 - val_auc: 0.8833\n",
      "Epoch 141/175\n",
      "540/540 [==============================] - 0s 479us/step - loss: 0.4774 - auc: 0.8877 - val_loss: 0.4841 - val_auc: 0.8835\n",
      "Epoch 142/175\n",
      "540/540 [==============================] - 0s 478us/step - loss: 0.4770 - auc: 0.8878 - val_loss: 0.4838 - val_auc: 0.8836\n",
      "Epoch 143/175\n",
      "540/540 [==============================] - 0s 482us/step - loss: 0.4765 - auc: 0.8880 - val_loss: 0.4831 - val_auc: 0.8837\n",
      "Epoch 144/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.4761 - auc: 0.8881 - val_loss: 0.4829 - val_auc: 0.8838\n",
      "Epoch 145/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.4756 - auc: 0.8883 - val_loss: 0.4825 - val_auc: 0.8840\n",
      "Epoch 146/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.4752 - auc: 0.8883 - val_loss: 0.4821 - val_auc: 0.8840\n",
      "Epoch 147/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.4746 - auc: 0.8886 - val_loss: 0.4817 - val_auc: 0.8841\n",
      "Epoch 148/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.4743 - auc: 0.8887 - val_loss: 0.4812 - val_auc: 0.8843\n",
      "Epoch 149/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.4739 - auc: 0.8888 - val_loss: 0.4808 - val_auc: 0.8844\n",
      "Epoch 150/175\n",
      "540/540 [==============================] - 0s 497us/step - loss: 0.4735 - auc: 0.8889 - val_loss: 0.4806 - val_auc: 0.8846\n",
      "Epoch 151/175\n",
      "540/540 [==============================] - 0s 531us/step - loss: 0.4730 - auc: 0.8891 - val_loss: 0.4801 - val_auc: 0.8846\n",
      "Epoch 152/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.4726 - auc: 0.8892 - val_loss: 0.4796 - val_auc: 0.8848\n",
      "Epoch 153/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.4723 - auc: 0.8893 - val_loss: 0.4790 - val_auc: 0.8850\n",
      "Epoch 154/175\n",
      "540/540 [==============================] - 0s 487us/step - loss: 0.4718 - auc: 0.8896 - val_loss: 0.4786 - val_auc: 0.8850\n",
      "Epoch 155/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.4715 - auc: 0.8895 - val_loss: 0.4781 - val_auc: 0.8852\n",
      "Epoch 156/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.4711 - auc: 0.8897 - val_loss: 0.4780 - val_auc: 0.8852\n",
      "Epoch 157/175\n",
      "540/540 [==============================] - 0s 482us/step - loss: 0.4707 - auc: 0.8898 - val_loss: 0.4776 - val_auc: 0.8855\n",
      "Epoch 158/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.4703 - auc: 0.8899 - val_loss: 0.4774 - val_auc: 0.8856\n",
      "Epoch 159/175\n",
      "540/540 [==============================] - 0s 486us/step - loss: 0.4700 - auc: 0.8901 - val_loss: 0.4770 - val_auc: 0.8858\n",
      "Epoch 160/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.4696 - auc: 0.8902 - val_loss: 0.4768 - val_auc: 0.8858\n",
      "Epoch 161/175\n",
      "540/540 [==============================] - 0s 488us/step - loss: 0.4692 - auc: 0.8904 - val_loss: 0.4759 - val_auc: 0.8862\n",
      "Epoch 162/175\n",
      "540/540 [==============================] - 0s 487us/step - loss: 0.4689 - auc: 0.8905 - val_loss: 0.4755 - val_auc: 0.8862\n",
      "Epoch 163/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.4685 - auc: 0.8905 - val_loss: 0.4761 - val_auc: 0.8861\n",
      "Epoch 164/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.4681 - auc: 0.8907 - val_loss: 0.4749 - val_auc: 0.8863\n",
      "Epoch 165/175\n",
      "540/540 [==============================] - 0s 545us/step - loss: 0.4677 - auc: 0.8909 - val_loss: 0.4746 - val_auc: 0.8866\n",
      "Epoch 166/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.4675 - auc: 0.8909 - val_loss: 0.4748 - val_auc: 0.8865\n",
      "Epoch 167/175\n",
      "540/540 [==============================] - 0s 485us/step - loss: 0.4670 - auc: 0.8912 - val_loss: 0.4743 - val_auc: 0.8866\n",
      "Epoch 168/175\n",
      "540/540 [==============================] - 0s 482us/step - loss: 0.4668 - auc: 0.8911 - val_loss: 0.4736 - val_auc: 0.8867\n",
      "Epoch 169/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.4665 - auc: 0.8914 - val_loss: 0.4733 - val_auc: 0.8869\n",
      "Epoch 170/175\n",
      "540/540 [==============================] - 0s 484us/step - loss: 0.4661 - auc: 0.8914 - val_loss: 0.4745 - val_auc: 0.8868\n",
      "Epoch 171/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.4658 - auc: 0.8915 - val_loss: 0.4727 - val_auc: 0.8871\n",
      "Epoch 172/175\n",
      "540/540 [==============================] - 0s 483us/step - loss: 0.4655 - auc: 0.8916 - val_loss: 0.4731 - val_auc: 0.8871\n",
      "Epoch 173/175\n",
      "540/540 [==============================] - 0s 486us/step - loss: 0.4652 - auc: 0.8917 - val_loss: 0.4720 - val_auc: 0.8875\n",
      "Epoch 174/175\n",
      "540/540 [==============================] - 0s 482us/step - loss: 0.4649 - auc: 0.8918 - val_loss: 0.4717 - val_auc: 0.8876\n",
      "Epoch 175/175\n",
      "540/540 [==============================] - 0s 482us/step - loss: 0.4646 - auc: 0.8920 - val_loss: 0.4721 - val_auc: 0.8875\n",
      "675/675 [==============================] - 0s 261us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/175\n",
      "1080/1080 [==============================] - 1s 572us/step - loss: 0.9252 - auc: 0.5499 - val_loss: 0.9160 - val_auc: 0.6111\n",
      "Epoch 2/175\n",
      "1080/1080 [==============================] - 1s 478us/step - loss: 0.9082 - auc: 0.6524 - val_loss: 0.9018 - val_auc: 0.6714\n",
      "Epoch 3/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.8943 - auc: 0.6969 - val_loss: 0.8881 - val_auc: 0.7055\n",
      "Epoch 4/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.8803 - auc: 0.7271 - val_loss: 0.8743 - val_auc: 0.7327\n",
      "Epoch 5/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.8659 - auc: 0.7524 - val_loss: 0.8597 - val_auc: 0.7555\n",
      "Epoch 6/175\n",
      "1080/1080 [==============================] - 1s 478us/step - loss: 0.8504 - auc: 0.7741 - val_loss: 0.8439 - val_auc: 0.7732\n",
      "Epoch 7/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.8343 - auc: 0.7871 - val_loss: 0.8280 - val_auc: 0.7845\n",
      "Epoch 8/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.8184 - auc: 0.7953 - val_loss: 0.8122 - val_auc: 0.7929\n",
      "Epoch 9/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.8025 - auc: 0.8017 - val_loss: 0.7964 - val_auc: 0.7996\n",
      "Epoch 10/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.7868 - auc: 0.8069 - val_loss: 0.7810 - val_auc: 0.8050\n",
      "Epoch 11/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.7713 - auc: 0.8114 - val_loss: 0.7656 - val_auc: 0.8094\n",
      "Epoch 12/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.7561 - auc: 0.8149 - val_loss: 0.7509 - val_auc: 0.8133\n",
      "Epoch 13/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.7414 - auc: 0.8181 - val_loss: 0.7364 - val_auc: 0.8167\n",
      "Epoch 14/175\n",
      "1080/1080 [==============================] - 1s 474us/step - loss: 0.7271 - auc: 0.8210 - val_loss: 0.7223 - val_auc: 0.8205\n",
      "Epoch 15/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.7135 - auc: 0.8244 - val_loss: 0.7090 - val_auc: 0.8239\n",
      "Epoch 16/175\n",
      "1080/1080 [==============================] - 1s 500us/step - loss: 0.7006 - auc: 0.8275 - val_loss: 0.6964 - val_auc: 0.8273\n",
      "Epoch 17/175\n",
      "1080/1080 [==============================] - 1s 485us/step - loss: 0.6884 - auc: 0.8305 - val_loss: 0.6847 - val_auc: 0.8303\n",
      "Epoch 18/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.6770 - auc: 0.8334 - val_loss: 0.6734 - val_auc: 0.8336\n",
      "Epoch 19/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.6662 - auc: 0.8360 - val_loss: 0.6628 - val_auc: 0.8366\n",
      "Epoch 20/175\n",
      "1080/1080 [==============================] - 1s 474us/step - loss: 0.6561 - auc: 0.8386 - val_loss: 0.6531 - val_auc: 0.8390\n",
      "Epoch 21/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.6466 - auc: 0.8414 - val_loss: 0.6436 - val_auc: 0.8422\n",
      "Epoch 22/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.6377 - auc: 0.8439 - val_loss: 0.6350 - val_auc: 0.8448\n",
      "Epoch 23/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.6293 - auc: 0.8466 - val_loss: 0.6267 - val_auc: 0.8476\n",
      "Epoch 24/175\n",
      "1080/1080 [==============================] - 1s 513us/step - loss: 0.6214 - auc: 0.8492 - val_loss: 0.6190 - val_auc: 0.8502\n",
      "Epoch 25/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.6138 - auc: 0.8516 - val_loss: 0.6116 - val_auc: 0.8525\n",
      "Epoch 26/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.6067 - auc: 0.8540 - val_loss: 0.6044 - val_auc: 0.8547\n",
      "Epoch 27/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.5998 - auc: 0.8563 - val_loss: 0.5977 - val_auc: 0.8571\n",
      "Epoch 28/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.5932 - auc: 0.8585 - val_loss: 0.5912 - val_auc: 0.8592\n",
      "Epoch 29/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.5868 - auc: 0.8607 - val_loss: 0.5853 - val_auc: 0.8613\n",
      "Epoch 30/175\n",
      "1080/1080 [==============================] - 1s 480us/step - loss: 0.5808 - auc: 0.8626 - val_loss: 0.5789 - val_auc: 0.8632\n",
      "Epoch 31/175\n",
      "1080/1080 [==============================] - 1s 492us/step - loss: 0.5750 - auc: 0.8643 - val_loss: 0.5735 - val_auc: 0.8649\n",
      "Epoch 32/175\n",
      "1080/1080 [==============================] - 1s 474us/step - loss: 0.5694 - auc: 0.8661 - val_loss: 0.5678 - val_auc: 0.8664\n",
      "Epoch 33/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.5641 - auc: 0.8676 - val_loss: 0.5628 - val_auc: 0.8677\n",
      "Epoch 34/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.5591 - auc: 0.8690 - val_loss: 0.5581 - val_auc: 0.8691\n",
      "Epoch 35/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.5542 - auc: 0.8703 - val_loss: 0.5531 - val_auc: 0.8703\n",
      "Epoch 36/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.5496 - auc: 0.8715 - val_loss: 0.5489 - val_auc: 0.8715\n",
      "Epoch 37/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.5453 - auc: 0.8727 - val_loss: 0.5451 - val_auc: 0.8725\n",
      "Epoch 38/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.5412 - auc: 0.8738 - val_loss: 0.5412 - val_auc: 0.8734\n",
      "Epoch 39/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.5373 - auc: 0.8747 - val_loss: 0.5374 - val_auc: 0.8744\n",
      "Epoch 40/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.5336 - auc: 0.8758 - val_loss: 0.5336 - val_auc: 0.8751\n",
      "Epoch 41/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.5301 - auc: 0.8766 - val_loss: 0.5304 - val_auc: 0.8760\n",
      "Epoch 42/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.5268 - auc: 0.8774 - val_loss: 0.5268 - val_auc: 0.8767\n",
      "Epoch 43/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.5235 - auc: 0.8783 - val_loss: 0.5243 - val_auc: 0.8775\n",
      "Epoch 44/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.5205 - auc: 0.8790 - val_loss: 0.5209 - val_auc: 0.8782\n",
      "Epoch 45/175\n",
      "1080/1080 [==============================] - 1s 509us/step - loss: 0.5176 - auc: 0.8799 - val_loss: 0.5181 - val_auc: 0.8788\n",
      "Epoch 46/175\n",
      "1080/1080 [==============================] - 1s 479us/step - loss: 0.5148 - auc: 0.8805 - val_loss: 0.5156 - val_auc: 0.8794\n",
      "Epoch 47/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.5121 - auc: 0.8811 - val_loss: 0.5130 - val_auc: 0.8800\n",
      "Epoch 48/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.5096 - auc: 0.8819 - val_loss: 0.5110 - val_auc: 0.8805\n",
      "Epoch 49/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.5072 - auc: 0.8824 - val_loss: 0.5086 - val_auc: 0.8811\n",
      "Epoch 50/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.5048 - auc: 0.8830 - val_loss: 0.5060 - val_auc: 0.8817\n",
      "Epoch 51/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.5026 - auc: 0.8836 - val_loss: 0.5039 - val_auc: 0.8823\n",
      "Epoch 52/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.5004 - auc: 0.8842 - val_loss: 0.5023 - val_auc: 0.8828\n",
      "Epoch 53/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4984 - auc: 0.8848 - val_loss: 0.5002 - val_auc: 0.8833\n",
      "Epoch 54/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4963 - auc: 0.8854 - val_loss: 0.4981 - val_auc: 0.8838\n",
      "Epoch 55/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4945 - auc: 0.8858 - val_loss: 0.4962 - val_auc: 0.8843\n",
      "Epoch 56/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.4927 - auc: 0.8864 - val_loss: 0.4946 - val_auc: 0.8847\n",
      "Epoch 57/175\n",
      "1080/1080 [==============================] - 1s 493us/step - loss: 0.4909 - auc: 0.8869 - val_loss: 0.4929 - val_auc: 0.8851\n",
      "Epoch 58/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.4893 - auc: 0.8872 - val_loss: 0.4914 - val_auc: 0.8856\n",
      "Epoch 59/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4876 - auc: 0.8879 - val_loss: 0.4902 - val_auc: 0.8860\n",
      "Epoch 60/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4860 - auc: 0.8883 - val_loss: 0.4891 - val_auc: 0.8863\n",
      "Epoch 61/175\n",
      "1080/1080 [==============================] - 1s 482us/step - loss: 0.4845 - auc: 0.8887 - val_loss: 0.4868 - val_auc: 0.8868\n",
      "Epoch 62/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4831 - auc: 0.8891 - val_loss: 0.4856 - val_auc: 0.8870\n",
      "Epoch 63/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4817 - auc: 0.8895 - val_loss: 0.4843 - val_auc: 0.8875\n",
      "Epoch 64/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4804 - auc: 0.8899 - val_loss: 0.4829 - val_auc: 0.8879\n",
      "Epoch 65/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4791 - auc: 0.8904 - val_loss: 0.4821 - val_auc: 0.8881\n",
      "Epoch 66/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4778 - auc: 0.8907 - val_loss: 0.4805 - val_auc: 0.8885\n",
      "Epoch 67/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.4767 - auc: 0.8910 - val_loss: 0.4795 - val_auc: 0.8889\n",
      "Epoch 68/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4755 - auc: 0.8914 - val_loss: 0.4783 - val_auc: 0.8892\n",
      "Epoch 69/175\n",
      "1080/1080 [==============================] - 1s 498us/step - loss: 0.4743 - auc: 0.8918 - val_loss: 0.4777 - val_auc: 0.8894\n",
      "Epoch 70/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4733 - auc: 0.8921 - val_loss: 0.4760 - val_auc: 0.8898\n",
      "Epoch 71/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4722 - auc: 0.8925 - val_loss: 0.4752 - val_auc: 0.8901\n",
      "Epoch 72/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4712 - auc: 0.8929 - val_loss: 0.4743 - val_auc: 0.8903\n",
      "Epoch 73/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4702 - auc: 0.8931 - val_loss: 0.4741 - val_auc: 0.8906\n",
      "Epoch 74/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4693 - auc: 0.8934 - val_loss: 0.4724 - val_auc: 0.8910\n",
      "Epoch 75/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4685 - auc: 0.8937 - val_loss: 0.4717 - val_auc: 0.8913\n",
      "Epoch 76/175\n",
      "1080/1080 [==============================] - 1s 482us/step - loss: 0.4675 - auc: 0.8941 - val_loss: 0.4707 - val_auc: 0.8915\n",
      "Epoch 77/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4667 - auc: 0.8944 - val_loss: 0.4702 - val_auc: 0.8917\n",
      "Epoch 78/175\n",
      "1080/1080 [==============================] - 1s 474us/step - loss: 0.4659 - auc: 0.8946 - val_loss: 0.4694 - val_auc: 0.8920\n",
      "Epoch 79/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4650 - auc: 0.8949 - val_loss: 0.4683 - val_auc: 0.8922\n",
      "Epoch 80/175\n",
      "1080/1080 [==============================] - 1s 497us/step - loss: 0.4643 - auc: 0.8951 - val_loss: 0.4676 - val_auc: 0.8926\n",
      "Epoch 81/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4636 - auc: 0.8954 - val_loss: 0.4670 - val_auc: 0.8927\n",
      "Epoch 82/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4628 - auc: 0.8957 - val_loss: 0.4669 - val_auc: 0.8929\n",
      "Epoch 83/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4621 - auc: 0.8959 - val_loss: 0.4655 - val_auc: 0.8931\n",
      "Epoch 84/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.4615 - auc: 0.8960 - val_loss: 0.4651 - val_auc: 0.8934\n",
      "Epoch 85/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4608 - auc: 0.8964 - val_loss: 0.4642 - val_auc: 0.8936\n",
      "Epoch 86/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4601 - auc: 0.8967 - val_loss: 0.4637 - val_auc: 0.8937\n",
      "Epoch 87/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4594 - auc: 0.8969 - val_loss: 0.4641 - val_auc: 0.8938\n",
      "Epoch 88/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4588 - auc: 0.8970 - val_loss: 0.4629 - val_auc: 0.8942\n",
      "Epoch 89/175\n",
      "1080/1080 [==============================] - 1s 479us/step - loss: 0.4583 - auc: 0.8972 - val_loss: 0.4620 - val_auc: 0.8945\n",
      "Epoch 90/175\n",
      "1080/1080 [==============================] - 1s 500us/step - loss: 0.4577 - auc: 0.8976 - val_loss: 0.4616 - val_auc: 0.8946\n",
      "Epoch 91/175\n",
      "1080/1080 [==============================] - 1s 483us/step - loss: 0.4572 - auc: 0.8977 - val_loss: 0.4609 - val_auc: 0.8949\n",
      "Epoch 92/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4566 - auc: 0.8980 - val_loss: 0.4608 - val_auc: 0.8949\n",
      "Epoch 93/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.4561 - auc: 0.8982 - val_loss: 0.4600 - val_auc: 0.8950\n",
      "Epoch 94/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.4555 - auc: 0.8983 - val_loss: 0.4596 - val_auc: 0.8951\n",
      "Epoch 95/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4551 - auc: 0.8985 - val_loss: 0.4590 - val_auc: 0.8954\n",
      "Epoch 96/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4545 - auc: 0.8987 - val_loss: 0.4588 - val_auc: 0.8956\n",
      "Epoch 97/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4540 - auc: 0.8989 - val_loss: 0.4580 - val_auc: 0.8959\n",
      "Epoch 98/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4536 - auc: 0.8991 - val_loss: 0.4576 - val_auc: 0.8961\n",
      "Epoch 99/175\n",
      "1080/1080 [==============================] - 1s 493us/step - loss: 0.4531 - auc: 0.8993 - val_loss: 0.4573 - val_auc: 0.8961\n",
      "Epoch 100/175\n",
      "1080/1080 [==============================] - 1s 479us/step - loss: 0.4527 - auc: 0.8996 - val_loss: 0.4567 - val_auc: 0.8962\n",
      "Epoch 101/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4523 - auc: 0.8995 - val_loss: 0.4565 - val_auc: 0.8963\n",
      "Epoch 102/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.4520 - auc: 0.8997 - val_loss: 0.4564 - val_auc: 0.8964\n",
      "Epoch 103/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4514 - auc: 0.9000 - val_loss: 0.4555 - val_auc: 0.8967\n",
      "Epoch 104/175\n",
      "1080/1080 [==============================] - 1s 482us/step - loss: 0.4511 - auc: 0.9000 - val_loss: 0.4553 - val_auc: 0.8968\n",
      "Epoch 105/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.4507 - auc: 0.9002 - val_loss: 0.4555 - val_auc: 0.8969\n",
      "Epoch 106/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.4503 - auc: 0.9003 - val_loss: 0.4546 - val_auc: 0.8971\n",
      "Epoch 107/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4499 - auc: 0.9005 - val_loss: 0.4550 - val_auc: 0.8972\n",
      "Epoch 108/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4496 - auc: 0.9006 - val_loss: 0.4538 - val_auc: 0.8975\n",
      "Epoch 109/175\n",
      "1080/1080 [==============================] - 1s 498us/step - loss: 0.4492 - auc: 0.9009 - val_loss: 0.4547 - val_auc: 0.8973\n",
      "Epoch 110/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4490 - auc: 0.9008 - val_loss: 0.4536 - val_auc: 0.8975\n",
      "Epoch 111/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4485 - auc: 0.9010 - val_loss: 0.4528 - val_auc: 0.8977\n",
      "Epoch 112/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4482 - auc: 0.9013 - val_loss: 0.4523 - val_auc: 0.8978\n",
      "Epoch 113/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4478 - auc: 0.9014 - val_loss: 0.4534 - val_auc: 0.8977\n",
      "Epoch 114/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.4475 - auc: 0.9014 - val_loss: 0.4517 - val_auc: 0.8980\n",
      "Epoch 115/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4472 - auc: 0.9016 - val_loss: 0.4517 - val_auc: 0.8980\n",
      "Epoch 116/175\n",
      "1080/1080 [==============================] - 1s 483us/step - loss: 0.4468 - auc: 0.9018 - val_loss: 0.4512 - val_auc: 0.8982\n",
      "Epoch 117/175\n",
      "1080/1080 [==============================] - 1s 478us/step - loss: 0.4467 - auc: 0.9017 - val_loss: 0.4511 - val_auc: 0.8982\n",
      "Epoch 118/175\n",
      "1080/1080 [==============================] - 1s 499us/step - loss: 0.4463 - auc: 0.9019 - val_loss: 0.4511 - val_auc: 0.8984\n",
      "Epoch 119/175\n",
      "1080/1080 [==============================] - 1s 478us/step - loss: 0.4460 - auc: 0.9020 - val_loss: 0.4506 - val_auc: 0.8984\n",
      "Epoch 120/175\n",
      "1080/1080 [==============================] - 1s 478us/step - loss: 0.4457 - auc: 0.9021 - val_loss: 0.4501 - val_auc: 0.8987\n",
      "Epoch 121/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4454 - auc: 0.9022 - val_loss: 0.4496 - val_auc: 0.8986\n",
      "Epoch 122/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.4451 - auc: 0.9023 - val_loss: 0.4492 - val_auc: 0.8989\n",
      "Epoch 123/175\n",
      "1080/1080 [==============================] - 1s 480us/step - loss: 0.4447 - auc: 0.9026 - val_loss: 0.4491 - val_auc: 0.8987\n",
      "Epoch 124/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4444 - auc: 0.9026 - val_loss: 0.4492 - val_auc: 0.8987\n",
      "Epoch 125/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4442 - auc: 0.9026 - val_loss: 0.4491 - val_auc: 0.8990\n",
      "Epoch 126/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4438 - auc: 0.9029 - val_loss: 0.4486 - val_auc: 0.8990\n",
      "Epoch 127/175\n",
      "1080/1080 [==============================] - 1s 506us/step - loss: 0.4436 - auc: 0.9029 - val_loss: 0.4489 - val_auc: 0.8991\n",
      "Epoch 128/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4433 - auc: 0.9030 - val_loss: 0.4480 - val_auc: 0.8991\n",
      "Epoch 129/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.4432 - auc: 0.9030 - val_loss: 0.4479 - val_auc: 0.8994\n",
      "Epoch 130/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4427 - auc: 0.9032 - val_loss: 0.4475 - val_auc: 0.8996\n",
      "Epoch 131/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4427 - auc: 0.9033 - val_loss: 0.4475 - val_auc: 0.8995\n",
      "Epoch 132/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.4423 - auc: 0.9034 - val_loss: 0.4468 - val_auc: 0.8996\n",
      "Epoch 133/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4421 - auc: 0.9035 - val_loss: 0.4467 - val_auc: 0.8998\n",
      "Epoch 134/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4419 - auc: 0.9036 - val_loss: 0.4469 - val_auc: 0.8997\n",
      "Epoch 135/175\n",
      "1080/1080 [==============================] - 1s 501us/step - loss: 0.4417 - auc: 0.9037 - val_loss: 0.4484 - val_auc: 0.8996\n",
      "Epoch 136/175\n",
      "1080/1080 [==============================] - 1s 474us/step - loss: 0.4415 - auc: 0.9036 - val_loss: 0.4471 - val_auc: 0.9001\n",
      "Epoch 137/175\n",
      "1080/1080 [==============================] - 1s 481us/step - loss: 0.4412 - auc: 0.9038 - val_loss: 0.4468 - val_auc: 0.9001\n",
      "Epoch 138/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4410 - auc: 0.9041 - val_loss: 0.4457 - val_auc: 0.9001\n",
      "Epoch 139/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.4408 - auc: 0.9039 - val_loss: 0.4456 - val_auc: 0.9003\n",
      "Epoch 140/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4406 - auc: 0.9042 - val_loss: 0.4452 - val_auc: 0.9002\n",
      "Epoch 141/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.4405 - auc: 0.9041 - val_loss: 0.4449 - val_auc: 0.9004\n",
      "Epoch 142/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4401 - auc: 0.9043 - val_loss: 0.4473 - val_auc: 0.9000\n",
      "Epoch 143/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4400 - auc: 0.9043 - val_loss: 0.4446 - val_auc: 0.9005\n",
      "Epoch 144/175\n",
      "1080/1080 [==============================] - 1s 499us/step - loss: 0.4397 - auc: 0.9045 - val_loss: 0.4447 - val_auc: 0.9006\n",
      "Epoch 145/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.4397 - auc: 0.9044 - val_loss: 0.4461 - val_auc: 0.9003\n",
      "Epoch 146/175\n",
      "1080/1080 [==============================] - 1s 494us/step - loss: 0.4395 - auc: 0.9044 - val_loss: 0.4442 - val_auc: 0.9009\n",
      "Epoch 147/175\n",
      "1080/1080 [==============================] - 1s 474us/step - loss: 0.4392 - auc: 0.9047 - val_loss: 0.4438 - val_auc: 0.9009\n",
      "Epoch 148/175\n",
      "1080/1080 [==============================] - 1s 473us/step - loss: 0.4391 - auc: 0.9047 - val_loss: 0.4436 - val_auc: 0.9010\n",
      "Epoch 149/175\n",
      "1080/1080 [==============================] - 1s 474us/step - loss: 0.4389 - auc: 0.9048 - val_loss: 0.4439 - val_auc: 0.9009\n",
      "Epoch 150/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4387 - auc: 0.9049 - val_loss: 0.4435 - val_auc: 0.9010\n",
      "Epoch 151/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4385 - auc: 0.9050 - val_loss: 0.4432 - val_auc: 0.9009\n",
      "Epoch 152/175\n",
      "1080/1080 [==============================] - 1s 497us/step - loss: 0.4383 - auc: 0.9049 - val_loss: 0.4432 - val_auc: 0.9012\n",
      "Epoch 153/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4382 - auc: 0.9051 - val_loss: 0.4428 - val_auc: 0.9013\n",
      "Epoch 154/175\n",
      "1080/1080 [==============================] - 1s 474us/step - loss: 0.4380 - auc: 0.9051 - val_loss: 0.4432 - val_auc: 0.9013\n",
      "Epoch 155/175\n",
      "1080/1080 [==============================] - 1s 480us/step - loss: 0.4378 - auc: 0.9052 - val_loss: 0.4425 - val_auc: 0.9014\n",
      "Epoch 156/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4377 - auc: 0.9052 - val_loss: 0.4423 - val_auc: 0.9014\n",
      "Epoch 157/175\n",
      "1080/1080 [==============================] - 1s 474us/step - loss: 0.4376 - auc: 0.9053 - val_loss: 0.4422 - val_auc: 0.9014\n",
      "Epoch 158/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4374 - auc: 0.9054 - val_loss: 0.4420 - val_auc: 0.9014\n",
      "Epoch 159/175\n",
      "1080/1080 [==============================] - 1s 497us/step - loss: 0.4372 - auc: 0.9053 - val_loss: 0.4426 - val_auc: 0.9016\n",
      "Epoch 160/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4371 - auc: 0.9055 - val_loss: 0.4417 - val_auc: 0.9017\n",
      "Epoch 161/175\n",
      "1080/1080 [==============================] - 1s 478us/step - loss: 0.4369 - auc: 0.9056 - val_loss: 0.4422 - val_auc: 0.9016\n",
      "Epoch 162/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4367 - auc: 0.9057 - val_loss: 0.4417 - val_auc: 0.9016\n",
      "Epoch 163/175\n",
      "1080/1080 [==============================] - 1s 480us/step - loss: 0.4366 - auc: 0.9057 - val_loss: 0.4413 - val_auc: 0.9018\n",
      "Epoch 164/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4364 - auc: 0.9058 - val_loss: 0.4412 - val_auc: 0.9017\n",
      "Epoch 165/175\n",
      "1080/1080 [==============================] - 1s 474us/step - loss: 0.4363 - auc: 0.9057 - val_loss: 0.4424 - val_auc: 0.9020\n",
      "Epoch 166/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4362 - auc: 0.9059 - val_loss: 0.4410 - val_auc: 0.9021\n",
      "Epoch 167/175\n",
      "1080/1080 [==============================] - 1s 499us/step - loss: 0.4360 - auc: 0.9059 - val_loss: 0.4414 - val_auc: 0.9018\n",
      "Epoch 168/175\n",
      "1080/1080 [==============================] - 1s 476us/step - loss: 0.4359 - auc: 0.9060 - val_loss: 0.4407 - val_auc: 0.9019\n",
      "Epoch 169/175\n",
      "1080/1080 [==============================] - 1s 477us/step - loss: 0.4357 - auc: 0.9059 - val_loss: 0.4406 - val_auc: 0.9023\n",
      "Epoch 170/175\n",
      "1080/1080 [==============================] - 1s 474us/step - loss: 0.4356 - auc: 0.9062 - val_loss: 0.4423 - val_auc: 0.9019\n",
      "Epoch 171/175\n",
      "1080/1080 [==============================] - 1s 479us/step - loss: 0.4355 - auc: 0.9062 - val_loss: 0.4402 - val_auc: 0.9021\n",
      "Epoch 172/175\n",
      "1080/1080 [==============================] - 1s 479us/step - loss: 0.4353 - auc: 0.9062 - val_loss: 0.4405 - val_auc: 0.9022\n",
      "Epoch 173/175\n",
      "1080/1080 [==============================] - 1s 475us/step - loss: 0.4352 - auc: 0.9062 - val_loss: 0.4399 - val_auc: 0.9023\n",
      "Epoch 174/175\n",
      "1080/1080 [==============================] - 1s 499us/step - loss: 0.4351 - auc: 0.9062 - val_loss: 0.4399 - val_auc: 0.9023\n",
      "Epoch 175/175\n",
      "1080/1080 [==============================] - 1s 473us/step - loss: 0.4349 - auc: 0.9063 - val_loss: 0.4399 - val_auc: 0.9024\n",
      "579/579 [==============================] - 0s 267us/step\n",
      "Parámetros: {'epochs': 175, 'optimizer': 'nadam'} \n",
      "F1 score:  0.809\n"
     ]
    }
   ],
   "source": [
    "modelo_cv = KerasClassifier(model=create_model)\n",
    "\n",
    "params_grid = {\n",
    "    'epochs': [175],\n",
    "    #'batch_size': [5],\n",
    "    'optimizer': ['nadam', 'adam']\n",
    "}\n",
    "\n",
    "scorer_fn = make_scorer(f1_score)\n",
    "kfoldcv = StratifiedKFold(n_splits=2)\n",
    "\n",
    "gridcv = GridSearchCV(estimator=modelo_cv,\n",
    "                      param_grid = params_grid,\n",
    "                      scoring=scorer_fn,\n",
    "                      cv=kfoldcv,\n",
    "                      )\n",
    "\n",
    "model = gridcv.fit(x_train,y_train, validation_split=0.2)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "score = f1_score(y_test, y_pred)\n",
    "print(\"Parámetros:\", gridcv.best_params_, \"\\nF1 score: \", round(score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas de mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 264us/step\n",
      "579/579 [==============================] - 0s 259us/step\n",
      "F1-Score sobre el set de entrenamiento: 0.817\n",
      "F1-Score sobre el set de prueba: 0.809\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "binary_predictions_test = (y_test_pred > 0.5).astype(int)\n",
    "binary_predictions_train = (y_train_pred > 0.5).astype(int)\n",
    "\n",
    "train_score = f1_score(y_train, binary_predictions_train)\n",
    "test_score = f1_score(y_test, binary_predictions_test)\n",
    "\n",
    "print(\"F1-Score sobre el set de entrenamiento:\", round(train_score, 3))\n",
    "print(\"F1-Score sobre el set de prueba:\", round(test_score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción de hotels_test\n",
    "\n",
    "Se modifica el dataset de test de manera similar al de train, para que el modelo obtenido pueda ser aplicado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sx/p4bbsw2947l2kjgww8lxntch0000gn/T/ipykernel_12531/3312754589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_mod[col] = False\n",
      "/var/folders/sx/p4bbsw2947l2kjgww8lxntch0000gn/T/ipykernel_12531/3312754589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_mod[col] = False\n",
      "/var/folders/sx/p4bbsw2947l2kjgww8lxntch0000gn/T/ipykernel_12531/3312754589.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_mod[col] = False\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('hotels_test.csv')\n",
    "test_df_mod = test_df.copy()\n",
    "\n",
    "# renombrar columna del dataframe de reserved_room_type a room_type_match\n",
    "test_df_mod = test_df_mod.rename(columns={'reserved_room_type': 'room_type_match'})\n",
    "\n",
    "test_df_mod.loc[test_df_mod['room_type_match'] == test_df_mod['assigned_room_type'], 'room_type_match'] = True\n",
    "test_df_mod.loc[test_df_mod['room_type_match'] != test_df_mod['assigned_room_type'], 'room_type_match'] = False\n",
    "test_df_mod['room_type_match'] = test_df_mod['room_type_match'].astype(bool)\n",
    "\n",
    "test_df_mod['agent'] = test_df_mod['agent'].astype(str)\n",
    "\n",
    "id_backup = test_df_mod[['id']].copy()\n",
    "\n",
    "test_df_mod = test_df_mod.drop(['arrival_date_year', 'arrival_date_day_of_month', 'stays_in_weekend_nights', 'stays_in_week_nights', 'children', 'company', 'adr', 'id'], axis=1)\n",
    "test_df_mod = test_df_mod.drop(['reservation_status_date'], axis='columns') #Esta es la columna que no debería estar en el dataset de test\n",
    "\n",
    "#Se normalizan los valores de las columnas numéricas cuantitativas\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "for col in test_df_mod.select_dtypes(include=[np.number, \"int64\", \"float64\"]).columns:\n",
    "    test_df_mod[col] = scaler.fit_transform(test_df_mod[[col]])\n",
    "\n",
    "#One-hot encoding para las columnas categóricas\n",
    "test_df_mod = pd.get_dummies(test_df_mod, columns=[\"hotel\", \"arrival_date_month\", \"meal\", \"country\", \"market_segment\", \"distribution_channel\", \"assigned_room_type\", \"deposit_type\", \"customer_type\", \"agent\", \"room_type_match\" ], drop_first=True)\n",
    "\n",
    "#Se crean las columnas que están en el df para entrenar pero no en el df a predecir\n",
    "for col in df_x.columns:\n",
    "    if col not in test_df_mod.columns:\n",
    "        test_df_mod[col] = False\n",
    "\n",
    "#Se eliminan las columnas que están en el df para predecir pero no en el df para entrenar\n",
    "for col in test_df_mod.columns:\n",
    "    if col not in df_x.columns:\n",
    "        test_df_mod = test_df_mod.drop(columns=[col])\n",
    "\n",
    "test_df_mod = test_df_mod.reindex(sorted(test_df_mod.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the columns with data type 'bool'\n",
    "bool_columns = test_df_mod.select_dtypes(include=['bool'])\n",
    "\n",
    "# Convert the 'bool' columns to 'float64'\n",
    "for column in bool_columns.columns:\n",
    "    test_df_mod[column] = test_df_mod[column].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830/830 [==============================] - 0s 254us/step\n"
     ]
    }
   ],
   "source": [
    "#Se realiza una predicción sobre test utilizando el modelo\n",
    "y_pred = model.predict(test_df_mod)\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "predictions['id'] = id_backup['id'].values\n",
    "predictions['is_canceled'] = y_pred.astype(int)\n",
    "\n",
    "predictions.to_csv('sample_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
